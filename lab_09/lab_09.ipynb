{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align='center' style='max-width: 1000px' src='banner.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhQBRR0SdqWK"
   },
   "source": [
    "<img align='right' style='max-width: 200px; height: auto' src='hsg_logo.png'>\n",
    "\n",
    "## Lab 09 - Generative Adversarial Networks (GANs)\n",
    "\n",
    "GSERM Summer School 2024, Deep Learning: Fundamentals and Applications, University of St. Gallen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rx6G8npURVV5"
   },
   "source": [
    "The lab environment is based on Jupyter Notebooks (https://jupyter.org), which provide an interactive platform for performing a variety of statistical evaluations and data analyses. In this lab, we will learn how to apply a deep learning technique referred to as **Generative Adversarial Networks (GANs)**. Unlike standard feedforward neural networks, GANs consist of two networks, a generator and a discriminator, which are trained together in a game-theoretic framework to generate realistic synthetic data.\n",
    "\n",
    "GANs were introduced by *Ian Goodfellow* and his colleagues in 2014 and have since revolutionized the field of generative modeling. They are capable of generating high-quality data across various domains, including images, text, and audio. The generator network creates synthetic data, while the discriminator network attempts to distinguish between real and synthetic data. Through this adversarial process, the generator improves its ability to create realistic data over time.\n",
    "\n",
    "In this lab, we will use the `PyTorch` library to implement and train a **Generative Adversarial Networks**. The network will be trained on the Fashion-MNIST dataset, which consists of grayscale images of various fashion items. Once the network is trained, we will evaluate its performance by visually inspecting the quality of the generated images. \n",
    "\n",
    "The figure below illustrates a high-level view of the machine learning process we aim to establish in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4_TCVNIRaXG"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 800px\" src=\"gan_pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlQn8826Rfa9"
   },
   "source": [
    "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our CANVAS (StudyNet) forum (https://learning.unisg.ch), or send us an email (using the course email)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-9fLcw87kxS"
   },
   "source": [
    "## 1. Lab Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqQXBWrb7ni_"
   },
   "source": [
    "After today's lab, you should be able to:\n",
    "\n",
    "> 1. **Understand Generative Adversarial Network (GAN) Design:** Learn the fundamental concepts and architectural design of GANs.\n",
    "> 2. **Implement and Train a GAN Model:** Gain hands-on experience with PyTorch to implement, train, and evaluate GAN models.\n",
    "> 3. **Apply GAN Models to Generate Synthetic Data:** Use GANs to generate realistic fashion images using the Fashion-MNIST dataset.\n",
    "> 4. **Evaluate and Interpret Model Performance:** Evaluate the GAN model's performance using relevant metrics and interpret the generated results.\n",
    "> 5. **Visualize and Interpret Generated Images:** Visualize the images generated to gain insights into the model's ability to capture the underlying data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDA2_HDkRqN0"
   },
   "source": [
    "Before we start let's watch a motivational video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88QlMdTlRsFd"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# AlphaFold: The Making of a Scientific Breakthrough\n",
    "# YouTubeVideo('gg7WjuFs8F4', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpEYm4uu55Vq"
   },
   "source": [
    "## 2. Setup of the Jupyter Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SP-khxJG9OQg"
   },
   "source": [
    "Similar to the previous labs, we need to import several Python libraries that facilitate data analysis and visualization. We will primarily use `PyTorch`, `NumPy`, `Scikit-learn`, `Matplotlib`, `Seaborn`, and a few utility libraries throughout this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CENL_GceN7xO"
   },
   "source": [
    "We start by importing `numpy` and utility libraries. Here, we also import the `pickle` module to save and reuse some Python objects. `pickle` \"serializes\" an object before writing it to a file. An object can be converted to a character stream and then reconstructed either later on in the script, or in another script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxM3wl9G43Uo"
   },
   "outputs": [],
   "source": [
    "# import python data science and utility libraries\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pickle as pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q19jzRk9NDLu"
   },
   "source": [
    "Importing `PyTorch` data download and transform libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5ijcYAQNJaG"
   },
   "outputs": [],
   "source": [
    "# import pytorch datasets and transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqtg-JZJiEPe"
   },
   "source": [
    "Import `Python` machine learning and deep learning libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QUkkYkuIOtwI"
   },
   "outputs": [],
   "source": [
    "# import pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KkUguPrsicvQ"
   },
   "source": [
    "Import the `Matplotlib` data visualization library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzmmJ15cJIM5"
   },
   "outputs": [],
   "source": [
    "# import isualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set global plotting theme and parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seaborn theme\n",
    "sns.set_theme()\n",
    "\n",
    "# set general plotting parameters\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.rcParams['figure.dpi']= 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable inline plotting with `Matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPVR4Ej3_yXL"
   },
   "source": [
    "Create notebook folder structure to store the data as well as the trained neural network models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxJv4unP_yxd"
   },
   "outputs": [],
   "source": [
    "# create the data sub-directory\n",
    "data_directory = './data_gan'\n",
    "if not os.path.exists(data_directory): os.makedirs(data_directory)\n",
    "\n",
    "# create the models sub-directory\n",
    "models_directory = './models_gan'\n",
    "if not os.path.exists(models_directory): os.makedirs(models_directory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDctK9zYjiBj"
   },
   "source": [
    "Set a random `seed` value to obtain reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gHRgJxKDdYec"
   },
   "outputs": [],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 123\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value); # set pytorch seed CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orGguatajo-z"
   },
   "source": [
    "Google Colab provides free GPUs for running notebooks. However, if you execute this notebook as is, it will use your device's CPU. To run the lab on a GPU, go to `Runtime` > `Change runtime type` and set the Runtime type to `GPU` in the drop-down menu. Running this lab on a CPU is fine, but you will find that GPU computing is faster. *CUDA* indicates that the lab is being run on a GPU.\n",
    "\n",
    "Enable GPU computing by setting the device flag and initializing a CUDA seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T2HC2Tj_jqvn"
   },
   "outputs": [],
   "source": [
    "# set cpu or gpu enabled device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu').type\n",
    "\n",
    "# init deterministic GPU seed\n",
    "torch.mps.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "# log type of device enabled\n",
    "print('[LOG] notebook with {} computation enabled'.format(str(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "REp66XG3jttw"
   },
   "source": [
    "Let's determine if we have access to a GPU provided by environments such as `Google Colab`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mmP5ewETjvFU"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9BF1tAz5-Wx"
   },
   "source": [
    "## 3. Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RYEDLlUYQ7W"
   },
   "source": [
    "In this lab, we will use the popular **FashionMNIST** dataset, which you have already seen in lab 04 **\"Artificial Neural Networks (ANNs)\"**. Back then, we used the dataset to train a simple neural network to classify the fashion articles. In this lab, we are going to train a model - consisting of 2 networks - to create its own images, based on the **FashionMNIST** items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VafwcSFZBEl"
   },
   "source": [
    "The **Fashion-MNIST database** is a large dataset of Zalando articles commonly used for training various image processing systems. The database is widely used for training and testing in the field of machine learning. Let's take a brief look at a few sample images from the dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rDq-OARSaQh7"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: 300px\" src=\"FashionMNIST.png\">\n",
    "\n",
    "Source: https://www.kaggle.com/c/insar-fashion-mnist-challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_g7z_CvFaeOJ"
   },
   "source": [
    "Further details on the dataset can be obtained from Zalando Research's [GitHub page](https://github.com/zalandoresearch/fashion-mnist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6SHtN10ai6g"
   },
   "source": [
    "The **Fashion-MNIST database** is an image dataset of Zalando's article images, consisting of **70,000 images** in total. The dataset is divided into **60,000 training examples** and **10,000 evaluation examples**. Each example is a **28x28 grayscale image**, associated with a **label from 10 classes**. Zalando created this dataset to replace the popular **MNIST** handwritten digits dataset. It is a useful addition as it is a bit more complex but still very easy to use. It shares the same image size and train/test split structure as MNIST, making it a drop-in replacement. It requires minimal efforts in preprocessing and formatting the distinct images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xGIXYczdalnp"
   },
   "source": [
    "Let's download, transform and inspect the training images of the dataset. Therefore, let's first define the directory in which we aim to store the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oXylyu3l_7xL"
   },
   "outputs": [],
   "source": [
    "train_path = data_directory + '/train_fmnist'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkvp0Tbbaz2c"
   },
   "source": [
    "Now, let's download the training data accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-82ZaoDQ-toq"
   },
   "outputs": [],
   "source": [
    "# define pytorch transformation into tensor format\n",
    "transf = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# download and transform images\n",
    "fashion_mnist_data = datasets.FashionMNIST(root=train_path, train=True, transform=transf, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEwUQPRLa_XF"
   },
   "source": [
    "Verify the number of training images downloaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBAnNPIVOKIF"
   },
   "outputs": [],
   "source": [
    "# determine the number of training data images\n",
    "len(fashion_mnist_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eQshYLpOL9H"
   },
   "source": [
    "Next, let's inspect a few of the downloaded training images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3xZH2_zZ_ckP"
   },
   "outputs": [],
   "source": [
    "# select and set a (random) image id\n",
    "image_id = 7779\n",
    "\n",
    "# retrieve image exhibiting the image id\n",
    "fashion_mnist_data[image_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5yFDVpPpz8ID"
   },
   "source": [
    "Ok, that doesn't seem right. Let's now separate the image from its label information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y892FD9fADw8"
   },
   "outputs": [],
   "source": [
    "fashion_mnist_image, fashion_mnist_label = fashion_mnist_data[image_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YagB8Q4S0Dyz"
   },
   "source": [
    "We can verify the label of our selected image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WECogUWX0HIH"
   },
   "outputs": [],
   "source": [
    "fashion_mnist_label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_q0XAtz0VRa"
   },
   "source": [
    "Ok, we know that the numerical label is 6. Each image is associated with a label from 0 to 9, representing one of the fashion items. So what does 6 mean? Is it a bag? A pullover? \n",
    "\n",
    "The order of the classes can be found on Zalando Research's [GitHub page](https://github.com/zalandoresearch/fashion-mnist). We need to map each numerical label to its fashion item, which will be useful throughout the lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKt0Mfu-0bVg"
   },
   "outputs": [],
   "source": [
    "fashion_classes = {0: 'T-shirt/top',\n",
    "                    1: 'Trouser',\n",
    "                    2: 'Pullover',\n",
    "                    3: 'Dress',\n",
    "                    4: 'Coat',\n",
    "                    5: 'Sandal',\n",
    "                    6: 'Shirt',\n",
    "                    7: 'Sneaker',\n",
    "                    8: 'Bag',\n",
    "                    9: 'Ankle boot'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zpm2N6g0eIT"
   },
   "source": [
    "So, we can determine the fashion item that the label represents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_classes[fashion_mnist_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's now visually inspect our sample image: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d_pw0YSh0f13"
   },
   "outputs": [],
   "source": [
    "# define tensor to image transformation\n",
    "trans = transforms.ToPILImage()\n",
    "\n",
    "# set image plot title \n",
    "plt.title('Example: {}, Label: {}'.format(str(image_id), fashion_classes[fashion_mnist_label]))\n",
    "\n",
    "# plot mnist handwritten digit sample\n",
    "plt.imshow(trans(fashion_mnist_image), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7A6-LM5oPOC5"
   },
   "source": [
    "That's it! In this lab, we will not use any test dataset. Unlike our previous labs where we built classifiers that required evaluation on test data, this lab does not utilize test data. We will train a model to generate new images, which, by definition, cannot be compared and validated against a test set. To train this lab's model, we will only use the training set of the `FashionMNIST` dataset, as it contains a sufficient number of images for our purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltj04nCn6jyX"
   },
   "source": [
    "## 4. Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BI5AgtTRVbjm"
   },
   "source": [
    "In this section, we implement the architectures of the two **neural networks** that will constitute our **GAN** model. We aim to train our **GAN** to generate new images—artificial images based on the **FashionMNIST** dataset that no human has ever drawn, created, or dreamt of. Before we dive into the theory, let's briefly revisit the process to be established. The following illustration provides a bird's-eye view:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Is5F9yGVXX5"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 800px\" src=\"gan_pipeline.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9KlC-V5nfiN9"
   },
   "source": [
    "We will build the two models that together constitute the **GAN** architecture. We will start with the construction of the `Discriminator` and then proceed to the `Generator`. After this, we will instantiate both models along with the other required components.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qa-k_CmAQdze"
   },
   "source": [
    "### 4.1 Implementing the Discriminator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLQnyuF3Lit2"
   },
   "source": [
    "The discriminative network $D$, which we name `Discriminator`, consists of four **fully-connected layers**. These layers aim to learn **non-linear feature combinations** that enable the detection of patterns. In fully-connected layers, all inputs are connected to all activation units of the next layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z2WBHxJypMOm"
   },
   "source": [
    "Let's implement the `Discriminator`. This is a binary classifier as described above. The input size to the first layer is 28x28 = 784, since our **FashionMNIST** images are 28x28 pixels. The output size of the last layer is 1, which corresponds to the model's classification of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hn8EJi8mOKzy"
   },
   "outputs": [],
   "source": [
    "# implement the Discriminator network architecture\n",
    "class DiscriminatorNet(nn.Module):\n",
    "\n",
    "    # define the class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "        \n",
    "        # specify fc layer 1: in 28*28, out 128\n",
    "        self.fc1 = nn.Linear(28*28, 128, bias=True, device=device) # the linearity W*x+b\n",
    "        self.activation1 = nn.LeakyReLU(0.2, inplace=True) # the non-linearity\n",
    "        \n",
    "        # specify fc layer 2: in 128, out 64\n",
    "        self.fc2 = nn.Linear(128, 64, bias=True, device=device) # the linearity W*x+b\n",
    "        self.activation2 = nn.LeakyReLU(0.2, inplace=True) # the non-linearity\n",
    "\n",
    "        # specify fc layer 3: in 64, out 32\n",
    "        self.fc3 = nn.Linear(64, 32, bias=True, device=device) # the linearity W*x+b\n",
    "        self.activation3 = nn.LeakyReLU(0.2, inplace=True) # the non-linearity\n",
    "        \n",
    "        # specify fc layer 4: in 32, out 1\n",
    "        self.fc4 = nn.Linear(32, 1, bias=True, device=device) # the linearity W*x+b\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    # define network forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # flatten image\n",
    "        x = x.view(-1, 28*28)\n",
    "\n",
    "        # define fc layer 1 forward pass and add dropout\n",
    "        x = self.activation1(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # define fc layer 2 forward pass and add dropout\n",
    "        x = self.activation2(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # define fc layer 3 forward pass and add dropout\n",
    "        x = self.activation3(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # define fc layer 4 forward pass\n",
    "        out = self.fc4(x)\n",
    "\n",
    "        # return forward pass result\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m117tbtTQhmJ"
   },
   "source": [
    "### 4.2 Implementing the Generator Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xF0n4No85V7t"
   },
   "source": [
    "Our generative network $G$, which we name `Generator`, consists of four **fully-connected layers**. These layers aim to learn **non-linear feature combinations** that enable the detection and, later, generation of patterns. In fully-connected layers, all inputs are connected to all activation units of the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hxaITEJEOAi3"
   },
   "outputs": [],
   "source": [
    "# implement the Generator network architecture\n",
    "class GeneratorNet(nn.Module):\n",
    "\n",
    "    # define the class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        \n",
    "        # specify fc layer 1: in 100, out 32\n",
    "        self.fc1 = nn.Linear(100, 32, bias=True, device=device) # the linearity W*x+b\n",
    "        self.activation1 = nn.LeakyReLU(0.2, inplace=True) # the non-linearity\n",
    "\n",
    "        # specify fc layer 2: in 32, out 64\n",
    "        self.fc2 = nn.Linear(32, 64, bias=True, device=device) # the linearity W*x+b\n",
    "        self.activation2 = nn.LeakyReLU(0.2, inplace=True) # the non-linearity\n",
    "\n",
    "        # specify fc layer 3: in 64, out 128\n",
    "        self.fc3 = nn.Linear(64, 128, bias=True, device=device) # the linearity W*x+b\n",
    "        self.activation3 = nn.LeakyReLU(0.2, inplace=True) # the non-linearity\n",
    "        \n",
    "        # specify fc layer 4: in 128, out 28*28\n",
    "        self.fc4 = nn.Linear(128, 28*28, bias=True, device=device) # the linearity W*x+b\n",
    "       \n",
    "        # dropout layer \n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "    # define network forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # define fc layer 1 forward pass and add dropout\n",
    "        x = self.activation1(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # define fc layer 2 forward pass and add dropout\n",
    "        x = self.activation2(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # define fc layer 3 forward pass and add dropout\n",
    "        x = self.activation3(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # define fc layer 4 with tanh applied\n",
    "        out = self.fc4(x).tanh()\n",
    "\n",
    "        # return forward pass result\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that we use the `tanh` function as the last layer of our `Generator`. This is done because the `Discriminator` expects normalized input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PXkJ4n5aQqw4"
   },
   "source": [
    "### 4.3 Generative Adversarial Network Model Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mye5TGu2WNAN"
   },
   "source": [
    "Now that we have implemented the GAN's `Discriminator` and `Generator` networks, we are ready to instantiate models of both for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Bz7J5OFwNiU"
   },
   "outputs": [],
   "source": [
    "discriminator = DiscriminatorNet()\n",
    "generator = GeneratorNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tqsFD-0eWetu"
   },
   "source": [
    "Let's push the initialized `Discriminator` and `Generator` models to the enabled computing device:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fxkshxJiWZUO"
   },
   "outputs": [],
   "source": [
    "discriminator = discriminator.to(device)\n",
    "generator = generator.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i1bM7hHkAZI-"
   },
   "source": [
    "Let's double-check if our model was deployed to the GPU, if available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RXCGbZacAZfM"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRyqI4XuE4cy"
   },
   "source": [
    "Once the models are initialized, we can visualize the model structures and review the implemented network architectures by executing the following cells. We start with the `Discriminator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuefJXmTEuN9"
   },
   "outputs": [],
   "source": [
    "print('[LOG] Discriminator architecture:\\n\\n{}\\n'.format(discriminator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ObW8WqBHE_5j"
   },
   "source": [
    "And now, let's review the `Generator`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DKJXL_nPEyCN"
   },
   "outputs": [],
   "source": [
    "print('[LOG] Generator architecture:\\n\\n{}\\n'.format(generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CGDgWJpgFGz1"
   },
   "source": [
    "Looks like it worked as intended? Brilliant! Finally, let's look into the number of model parameters that we aim to train in the next steps of the notebook. Again, we start with the `Discriminator`. The number of parameters, if everything is defined correctly, should be: \n",
    "$$(784+1) * 128 + (128+1) * 64 + (64+1) * 32 + (32+1) * 1 = 110,849$$\n",
    "\n",
    "Don't hesitate to revisit our **CNN** lab if you are unsure how to count the number of parameters. Let's verify this calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtjKVEidFROJ"
   },
   "outputs": [],
   "source": [
    "# init the number of model parameters\n",
    "num_params_discriminator = 0\n",
    "\n",
    "# iterate over the distinct parameters\n",
    "for param in discriminator.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    num_params_discriminator += param.numel()\n",
    "    \n",
    "# print the number of model paramters\n",
    "print('[LOG] Number of Discriminator model parameters to be trained: {}.'.format(num_params_discriminator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKQAFGp2FR_4"
   },
   "source": [
    "Now, let's look into the number of model parameters of the `Generator`. The number of parameters, if everything is defined correctly, should be: \n",
    "$$(100+1) * 32 + (32+1) * 64 + (64+1) * 128 + (128+1) * 784 = 114,800$$\n",
    "\n",
    "Let's verify this calculation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cUB1YCBINBp"
   },
   "outputs": [],
   "source": [
    "# init the number of model parameters\n",
    "num_params_generator = 0\n",
    "\n",
    "# iterate over the distinct parameters\n",
    "for param in generator.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    num_params_generator += param.numel()\n",
    "\n",
    "# print the number of model paramters\n",
    "print('[LOG] Number of Generator model parameters to be trained: {}.'.format(num_params_generator))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L88OkEBcFd8-"
   },
   "source": [
    "Okay, our 'simple' **GAN** model already encompasses an impressive number of parameters: 110,849 + 114,800 = **225,649** model parameters to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vsIefZBlHw9W"
   },
   "source": [
    "Now that we have implemented the GANs, we are ready to train the network. However, before starting the training, we need to define an appropriate loss function. Remember, we discussed in the theory section above (see 4.1.1) that we want to use **Binary Cross-Entropy (BCE)** loss with logits, so we do not have to manually define a sigmoid function in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c88zFh9NIPZ"
   },
   "source": [
    "Let's instantiate the **BCEWithLogitsLoss** by executing the following PyTorch command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N-U_Zf_HcF5q"
   },
   "outputs": [],
   "source": [
    "# define the optimization criterion / loss function\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jwhrASD3YMY3"
   },
   "source": [
    "Next, let's also push the initialized `criterion` computation to the enabled computing `device`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G44Qf3UbWbHT"
   },
   "outputs": [],
   "source": [
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wXr4dCEpObqE"
   },
   "source": [
    "Based on the loss magnitude of a certain mini-batch, PyTorch automatically computes the gradients. Even better, based on the gradient, the library also helps us in the optimization and update of the network parameters $\\theta$.\n",
    "\n",
    "Based on the loss magnitude of a certain mini-batch, PyTorch automatically computes the gradients. Even better, based on the gradient, the library also helps us in the optimization and update of the network parameters $\\theta$. We also set the learning rate to 0.02 for our **Discriminator** and to 0.002 for our **Generator**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e_n2k787wf7x"
   },
   "outputs": [],
   "source": [
    "# set different learning rates for both networks\n",
    "discriminator_learning_rate = 0.02\n",
    "generator_learning_rate = 0.002"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the advice of [Soumith Chintala](https://github.com/soumith/ganhacks), we use the **Stochastic Gradient Descent** (`SGD`) optimizer for our **Discriminator**, and the `Adam` optimizer for our **Generator**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create optimizers for the discriminator and generator\n",
    "discriminator_optimizer = optim.SGD(params=discriminator.parameters(), lr=discriminator_learning_rate) \n",
    "generator_optimizer = optim.Adam(params=generator.parameters(), lr=generator_learning_rate) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6DABtmWYpJv"
   },
   "source": [
    "That's it! We are finally done with the implementation. Now, let's get down to training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMnnAQsi60Qc"
   },
   "source": [
    "## 5. Neural Network Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKVE_BJJKWXZ"
   },
   "source": [
    "In this section, we will train a **Generative Adversarial Network (GAN)** model (as implemented in the section above) using the **FashionMNIST** images. Specifically, we will take a detailed look at the distinct training steps and how to monitor the training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adwAoKT7PYMB"
   },
   "source": [
    "### 5.1 Preparing the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJQ9fPxzPpkR"
   },
   "source": [
    "So far, we have pre-processed the dataset, implemented the GANs, and defined the loss function. Let's now start training the model for **20 epochs** with a **mini-batch size of 64** FashionMNIST images per batch. This means that the entire dataset will be fed through the network 20 times in chunks of 64 images, yielding **938 mini-batches** (60,000 images / 64 images per mini-batch) per epoch. After processing each mini-batch, the parameters of the network will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BwtpxWLNJ-n3"
   },
   "outputs": [],
   "source": [
    "# specify the training parameters\n",
    "num_epochs = 20 # number of training epochs\n",
    "mini_batch_size = 64 # size of the mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8omxQ6Z_TQ0y"
   },
   "source": [
    "Furthermore, let's specify and instantiate a corresponding PyTorch data loader that feeds the image tensors to our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqKM4MraKTNC"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(fashion_mnist_data, batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pgXLmxUnTlb3"
   },
   "source": [
    "We can verify the length of the training `DataLoader`, which should correspond to **938 mini-batches**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k6suJKlCTmSV"
   },
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-ytj2fZT6HZ"
   },
   "source": [
    "Please, remember that our `Discriminator` will attempt to classify samples as either *real* or *fake*. We therefore have to define what these labels will be.\n",
    "\n",
    "As this is a binary classification task, we define:\n",
    ">- $1$ as the label for real images: $y = 1$\n",
    ">- $0$ as the label for fake images: $y = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvOI2QtEUcED"
   },
   "outputs": [],
   "source": [
    "# establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTqM4-r0UrBX"
   },
   "source": [
    "Lastly, we create a batch of **latent vectors**, exhibiting 100 dimensions each, that we will use later to visualize the progress of the `Generator`. We will call it `fixed_noise`, as it will remain fixed. This will allow us to take 4 images (we define a sample size of 4) and see how the results evolve in the evaluation section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BNO6qImbVS4h"
   },
   "outputs": [],
   "source": [
    "# define size of latent vector\n",
    "z_size = 100\n",
    "\n",
    "# define sample size\n",
    "sample_size = 4\n",
    "\n",
    "# uniformly distribute data of size z_size over an interval of -1; 1\n",
    "fixed_noise = np.random.uniform(-1, 1, size=(sample_size, z_size))\n",
    "\n",
    "# create numpy array into tensor, and convert data to float\n",
    "fixed_noise = torch.from_numpy(fixed_noise).float()\n",
    "\n",
    "# push the fixed vector to the device that's enabled\n",
    "fixed_noise = fixed_noise.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i79EdYdaa7F9"
   },
   "source": [
    "### 5.2 Running the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYospAbhbM7t"
   },
   "source": [
    "Finally, we start training the model according to the following adversarial training protocol:\n",
    "\n",
    ">1. Train the `Discriminator` on the real images.\n",
    ">2. Generate fake images with the `Generator` and train the `Discriminator` on them.\n",
    ">3. Perform a backward pass through the `Discriminator` and update its parameters $θ_{D}$.\n",
    ">4. Train the `Generator` based on the `Discriminator`'s output on the fake data.\n",
    ">5. Perform a backward pass through the `Generator` and update its parameters $θ_{G}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZ3kjkohcZuo"
   },
   "source": [
    "To ensure effective learning while training our **Generative Adversarial Network (GAN)** model, we will monitor whether the loss decreases as training progresses. Therefore, we will obtain and evaluate the performance on the entire training dataset after each iteration. Based on this evaluation, we can assess the training progress and determine whether the loss is converging, indicating that the model might not improve any further. The following elements of the network training code below should be given particular attention:\n",
    "\n",
    ">- `loss.backward()` computes the gradients based on the magnitude of the reconstruction loss.\n",
    ">- `optimizer.step()` updates the network parameters based on the gradients.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqClyFAK4Bxe"
   },
   "outputs": [],
   "source": [
    "# initialize list of the generated (fake) images\n",
    "fake_images = []\n",
    "\n",
    "# initialize collection of epoch losses\n",
    "discriminator_epoch_losses = []\n",
    "generator_epoch_losses = []\n",
    "\n",
    "# set networks to training mode\n",
    "discriminator.train()\n",
    "generator.train()\n",
    "\n",
    "# init and wrap range of training iterations\n",
    "training_epochs = tqdm(range(0, num_epochs), position=0, leave=True)\n",
    "\n",
    "# train the GANs\n",
    "for epoch in training_epochs:\n",
    "\n",
    "    # initialize collection of batch losses\n",
    "    discriminator_batch_losses = []\n",
    "    generator_batch_losses = []\n",
    "\n",
    "    # iterate over mini batches\n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        # determine real images and ignore class labels\n",
    "        real_images = data[0]\n",
    "\n",
    "        # determine batch size as the images' size to ensure the loader is emptied completely\n",
    "        batch_size = real_images.size(0)\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # (1) train Discriminator network\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        #### train with real images\n",
    "\n",
    "        # push real images to compute device\n",
    "        real_images = real_images.to(device)\n",
    "\n",
    "        # create tensor of same size as mini-batch and filled with 1's (real_label)\n",
    "        label = torch.full((batch_size,), real_label, dtype=torch.float, device=device)\n",
    "\n",
    "        # rescaling input images from [0,1) to [-1, 1), which is needed for network\n",
    "        real_images = real_images * 2 - 1\n",
    "\n",
    "        # run forward pass through Discriminator\n",
    "        output = discriminator(real_images).view(-1)\n",
    "\n",
    "        # reset graph gradients\n",
    "        discriminator.zero_grad()\n",
    "\n",
    "        # determine loss on Discriminator\n",
    "        discriminator_loss_real = criterion(output, label)\n",
    "\n",
    "        # run backward pass\n",
    "        discriminator_loss_real.backward()\n",
    "    \n",
    "        #### train with fake images\n",
    "\n",
    "        # generate batch of latent vectors\n",
    "        z = np.random.uniform(-1, 1, size=(batch_size, z_size)) # torch.randn(batch_size, z_size, 1, 1, device=device)\n",
    "\n",
    "        # create numpy array into tensor, and convert data to float\n",
    "        z = torch.from_numpy(z).float()\n",
    "\n",
    "        # push the z vector to the device that's enabled\n",
    "        z = z.to(device)\n",
    "\n",
    "        # generate fake image batch with Generator\n",
    "        fake = generator(z)\n",
    "\n",
    "        # fills label tensor with 0's (fake_label)\n",
    "        label.fill_(fake_label)\n",
    "\n",
    "        # classify all fake batch with Discriminator\n",
    "        output = discriminator(fake.detach()).view(-1)\n",
    "\n",
    "        # get discriminator loss on the fake batch\n",
    "        discriminator_loss_fake = criterion(output, label)\n",
    "\n",
    "        # run backward pass\n",
    "        discriminator_loss_fake.backward()\n",
    "\n",
    "        #### update Discriminator model parameters\n",
    "\n",
    "        # compute error of Discriminator as sum of loss over the fake and the real batches\n",
    "        discriminator_loss = discriminator_loss_fake + discriminator_loss_real\n",
    "\n",
    "        # update Discriminator parameters\n",
    "        discriminator_optimizer.step()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # (2) train Generator network\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # reset graph gradients\n",
    "        generator.zero_grad()\n",
    "\n",
    "        # fake labels are real for generator\n",
    "        label.fill_(real_label)\n",
    "\n",
    "        # since we just updated D, perform another forward pass of fake batch through the Discriminator\n",
    "        output = discriminator(fake).view(-1)\n",
    "\n",
    "        # get Generator loss based on this output\n",
    "        generator_loss = criterion(output, label)\n",
    "\n",
    "        # run backward pass\n",
    "        generator_loss.backward()\n",
    "\n",
    "        #### update Generator model parameters\n",
    "\n",
    "        # update Generator parameters\n",
    "        generator_optimizer.step()\n",
    "\n",
    "        # --------------------------------------------------------------------------\n",
    "        # (3) evaluate generative adversarial network\n",
    "        # --------------------------------------------------------------------------\n",
    "\n",
    "        # collect training losses\n",
    "        discriminator_batch_losses.append(discriminator_loss.item())\n",
    "        generator_batch_losses.append(generator_loss.item())\n",
    "\n",
    "        # set Generator to eval mode for generating samples\n",
    "        generator.eval() \n",
    "\n",
    "        # make Generator generate samples from the fixed noise distribution\n",
    "        samples = generator(fixed_noise.float())\n",
    "\n",
    "        # push samples to computation device\n",
    "        samples = samples.to(device)\n",
    "\n",
    "        # append generated fixed samples to the fake_images list\n",
    "        fake_images.append(samples)\n",
    "\n",
    "        # set Generator back to train mode\n",
    "        generator.train()\n",
    "\n",
    "    # determine mean mini-batch loss of epoch\n",
    "    discriminator_epoch_loss = np.mean(discriminator_batch_losses)\n",
    "    generator_epoch_loss = np.mean(generator_batch_losses)\n",
    "\n",
    "    # collect mean mini-batch loss of epoch\n",
    "    discriminator_epoch_losses.append(discriminator_epoch_loss)\n",
    "    generator_epoch_losses.append(generator_epoch_loss)\n",
    "\n",
    "    # print training progress and training losses\n",
    "    now = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    training_epochs.set_description(\n",
    "        (\n",
    "            '[LOG {}] epoch: {}, disc.-loss: {}, gen.-loss: {}'.format(str(now), str(epoch).zfill(6), str(round(discriminator_epoch_loss, 6)), str(round(generator_epoch_loss, 6)))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # set filename of actual discriminator and generator model\n",
    "    dis_model_name = 'gan_dis_model_epoch_{}.pth'.format(str(epoch).zfill(4))\n",
    "    gen_model_name = 'gan_gen_model_epoch_{}.pth'.format(str(epoch).zfill(4))\n",
    "\n",
    "    # save current model to models directory\n",
    "    torch.save(discriminator.state_dict(), os.path.join(models_directory, dis_model_name))\n",
    "    torch.save(generator.state_dict(), os.path.join(models_directory, gen_model_name))\n",
    "\n",
    "# open pickle file output stream\n",
    "with open('fake_images.pkl', 'wb') as f:\n",
    "\n",
    "    # save generated samples with pickle\n",
    "    pkl.dump(fake_images, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWDbSTLC6_Bk"
   },
   "source": [
    "## 6. Generative Adversarial Network Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEsuj-OmdH8L"
   },
   "source": [
    "As we do not have a test set, the evaluation of a **Generative Adversarial Network (GAN)** model does not resemble that of a typical classifier. First, we base our evaluation on the progression of the losses of our two adversarial models. Then, and more interestingly, we examine the images generated by the `Generator` from the fixed noise we created. In this regard, you could say that the training and testing of the network happen simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugCIUkWk_d1a"
   },
   "source": [
    "### 6.1 Training Loss Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gvtARRXfnIx"
   },
   "source": [
    "Let's visualize and inspect the loss per training iteration (mini-batch). We'll start with the `Discriminator`'s loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXEFraxaJPHn"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "# convert losses to numpy arrays\n",
    "discriminator_batch_losses = np.array(discriminator_batch_losses)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot losses of the Discriminator network\n",
    "plt.plot(discriminator_batch_losses, label='discriminator-loss (green)', c='tab:green')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[Training mini-batch $mb_i$]\", fontsize=14)\n",
    "ax.set_ylabel(\"[Classification Error of Discriminator $D$, $L^{BCE}$]\", fontsize=14)\n",
    "\n",
    "# add plot legends\n",
    "plt.legend()\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Iterations $mb_i$ vs. Discriminator Loss $L^{BCE}$', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BARkCDXk25q9"
   },
   "source": [
    "Now, let's visualize the `Generator`'s loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ussXKBNoJpqp"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "\n",
    "# convert losses to numpy arrays\n",
    "generator_batch_losses = np.array(generator_batch_losses)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot losses of the Generator network\n",
    "plt.plot(generator_batch_losses, label='generator-loss (orange)', c='tab:orange')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[Training mini-batch $mb_i$]\", fontsize=14)\n",
    "ax.set_ylabel(\"[Classification Error of Generator $G$, $L^{BCE}$]\", fontsize=14)\n",
    "\n",
    "# add plot legends\n",
    "plt.legend()\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Iterations $mb_i$ vs. Generator Loss $L^{BCE}$', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JOPwoYEa2-uu"
   },
   "source": [
    "What our batch losses seem to indicate is that although they are very fluctuating, the `Discriminator` starts off with a low loss which progressively increases, while the `Generator`'s loss decreases throughout the training. Let's plot the mean epoch losses of both models to get a clearer overview:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N1hHpN8lRM_W"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig, ax = plt.subplots(figsize=(20,8))\n",
    "\n",
    "# convert losses to numpy arrays\n",
    "discriminator_epoch_losses = np.array(discriminator_epoch_losses)\n",
    "generator_epoch_losses = np.array(generator_epoch_losses)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot losses of the Discriminator and Generator network\n",
    "plt.plot(discriminator_epoch_losses, label='discriminator-loss (green)', c = 'tab:green')\n",
    "plt.plot(generator_epoch_losses, label='generator-loss (orange)', c = 'tab:orange')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[Training mini-batch $mb_i$]\", fontsize=14)\n",
    "ax.set_ylabel(\"[Classification Error $L^{BCE}$]\", fontsize=14)\n",
    "\n",
    "# add plot legends\n",
    "plt.legend()\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Iterations $mb_i$ vs. Generator and Discriminator Loss $L^{BCE}$', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHW3_NuYLNXB"
   },
   "source": [
    "Okay, fantastic. The training error converges nicely for both networks. The `Discriminator` starts off strong during the first few epochs with a very low loss, while the `Generator` has a very high loss. It is very apparent that the `Generator` has no idea what to do at that point. Then, the trends change as the `Generator` gets better at knowing what fools the `Discriminator` — i.e., it gets better at faking images.\n",
    "\n",
    "We see that the loss of the `Generator` is consistently a bit lower than that of the `Discriminator` after a few epochs. You could then hypothesize that the `Generator` is often able to fool the `Discriminator`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e8UaJkh_kSy"
   },
   "source": [
    "### 6.2 Generated Images Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YZjhXwtM5tc"
   },
   "source": [
    "Let's now inspect the images that were generated during the training of our **Generative Adversarial Network (GAN)** model. To do so, we start by defining a function that we will use to display the generated samples. These samples were created using fixed noise, which helps us see the evolution of the `Generator`'s progress on a fixed distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgShBbAe8JMM"
   },
   "outputs": [],
   "source": [
    "# create function to view the image samples\n",
    "def view_samples(epoch, samples):\n",
    "\n",
    "    # initialize plot\n",
    "    fig, axes = plt.subplots(figsize=(10,7), nrows=1, ncols=4, sharey=True, sharex=True)\n",
    "    \n",
    "    # adjust padding between subplots\n",
    "    fig.tight_layout(pad=5.0)\n",
    "\n",
    "    # iterate over fake images at each epoch (we change epochs each 938 mini-batch)\n",
    "    # remember, we save 4 images together at each iteration\n",
    "    for i, (ax, img) in enumerate(zip(axes.flatten(), fake_images[epoch*938])):\n",
    "        \n",
    "        # create title for each subplot\n",
    "        ax.set_title(f'Epoch {epoch+1}, Sample {i+1}')\n",
    "\n",
    "        # detach image\n",
    "        img = img.detach()\n",
    "\n",
    "        # disable axes\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "\n",
    "        # show 28 by 28 grayscale image\n",
    "        im = ax.imshow(img.reshape((28,28)).cpu(), cmap='Greys_r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xO1Mi3b8Y18p"
   },
   "source": [
    "We now \"unpickle\" the samples we saved using the `pickle` library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMQ6SL1USx8D"
   },
   "outputs": [],
   "source": [
    "# open pickle file input stream\n",
    "with open('fake_images.pkl', 'rb') as f:\n",
    "\n",
    "    # load generated samples with pickle\n",
    "    samples = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NER-TTJkZBlA"
   },
   "source": [
    "We will call our function at each epoch to inspect the progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "84l6KA-6ScMw"
   },
   "outputs": [],
   "source": [
    "# iterate over epochs\n",
    "for i in range(num_epochs):\n",
    "  \n",
    "    # call function to view the 4 samples\n",
    "    view_samples(i, samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttewR7QJZKna"
   },
   "source": [
    "Cool, right? The samples generated by the `Generator` start off very poorly. They are totally random in the first epoch, and quite bad in the second—although we can witness clear progress. They then progressively improve, to the point where we can clearly see them representing clothes similar to those in the **FashionMNIST** dataset. The quality stabilizes over time. This rapid improvement and stabilization perfectly correspond to the progression of the `Generator`'s loss.\n",
    "\n",
    "Interestingly, we sometimes see the same sample switching fashion class between epochs. As you can see, the model - whose parameters are updated at each iteration - does not care whether it outputs a shoe or a shirt; it simply aims at minimizing its loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzdKOroEswp1"
   },
   "source": [
    "## 8. Lab Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u1X2r7Fhs1iE"
   },
   "source": [
    "In this lab, you successfully accomplished the following key learnings:\n",
    "\n",
    "> 1. **Understanding the GAN Architecture:** Mastered the fundamental concepts and architectural design of Generative Adversarial Networks (GANs), enhancing your comprehension of deep learning models tailored for generating synthetic data.\n",
    "> 2. **Model Implementation and Training:** Developed practical skills in implementing and training a GAN model using PyTorch, applying it to the Fashion-MNIST dataset to generate realistic fashion images.\n",
    "> 3. **Evaluating Model Performance:** Gained expertise in evaluating the performance of GAN models through metrics such as loss functions for both the generator and discriminator, and visually assessing the quality of generated images.\n",
    "> 4. **Visualization and Interpretation of Generated Data:** Learned to visualize and interpret the generated fashion images, providing deeper insights into the model's ability to capture the underlying distribution of the training data.\n",
    "\n",
    "This lab provided insights into designing, implementing, training, and evaluating GANs for generating synthetic data. It equipped you with tools and techniques for effective model building, evaluation, and application. These skills are invaluable for succeeding in deep learning."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "b-9fLcw87kxS",
    "GpEYm4uu55Vq",
    "w9BF1tAz5-Wx",
    "ltj04nCn6jyX",
    "XMnnAQsi60Qc",
    "EWDbSTLC6_Bk",
    "KyFaT0xw7DM4",
    "IzdKOroEswp1",
    "BS7zlqBTxv8a"
   ],
   "name": "colab_09.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
