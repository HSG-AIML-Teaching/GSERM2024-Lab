{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px; height: auto\" src=\"./banner.png\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "dTMzud2SGElb"
   },
   "source": [
    "<img align='right' style='max-width: 200px; height: auto' src='hsg_logo.png'>\n",
    "\n",
    "## Lab 08 - Generative Pretrained Transformer (GPT) Networks\n",
    "\n",
    "GSERM Summer School 2024, Deep Learning: Fundamentals and Applications, University of St. Gallen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "TvGE7TaBGElm"
   },
   "source": [
    "The lab environment is based on Jupyter Notebooks (https://jupyter.org), which provide an interactive platform for performing a variety of statistical evaluations and data analyses. In this lab, we will learn how to apply a deep learning technique referred to as **Generative Pre-trained Transformer (GPT)** models. Unlike standard feedforward neural networks, GPT models leverage the transformer architecture to capture long-range dependencies in sequential data, making them highly effective for natural language processing tasks.\n",
    "\n",
    "GPT models, introduced by *OpenAI*, have revolutionized the field of NLP by achieving state-of-the-art results in text generation, translation, and summarization. The key innovation in GPT models is the use of the transformer architecture, specifically focusing on the decoder part. This architecture utilizes multi-head self-attention mechanisms and feedforward neural networks to process and generate text. In this lab, we will use the `PyTorch` library to implement and train a **GPT model**. The model will be trained on the **Tiny Shakespeare dataset**, which consists of **40,000 lines of text from various plays by Shakespeare**. Once the model is trained, we will evaluate its performance by generating new text based on given prompts and visualizing the attention mechanisms that the model uses to generate this text.\n",
    "\n",
    "The figure below illustrates a high-level view of the machine learning process we aim to establish in this lab:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px\" src=\"./prediction.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our CANVAS (StudyNet) forum (https://learning.unisg.ch), or send us an email (using the course email)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKqWDNc6GElq"
   },
   "source": [
    "## 1. Lab Objectives"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "di086PLSGElq"
   },
   "source": [
    "After today's lab, you should be able to:\n",
    "\n",
    "> 1. **Understand the Generative Pre-trained Transformer (GPT) Design:** Learn the fundamental concepts and architectural design of GPT models.\n",
    "> 2. **Implement and Train a GPT Model:** Gain hands-on experience with PyTorch to implement, train, and evaluate GPT models.\n",
    "> 3. **Apply GPT Models for Text Generation:** Use GPT models to generate contextually relevant text based on given prompts.\n",
    "> 4. **Evaluate and Interpret Model Performance:** Evaluate the GPT model's performance using relevant metrics and interpret the generated text results.\n",
    "> 5. **Visualize and Interpret Attention Mechanisms:** Visualize the attention mechanisms to gain insights into how the model attends to the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start let's watch a motivational video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# \"Powered by TensorFlow: helping paleographers transcribe medieval text using machine learning\"\n",
    "# YouTubeVideo('v-FgOACRgfs', width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "buoFpQgxGEls"
   },
   "source": [
    "## 2. Setup of the Jupyter Notebook Environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4FJzrbhsGEls"
   },
   "source": [
    "Similar to the previous labs, we need to import several Python libraries that facilitate data analysis and visualization. We will primarily use `PyTorch`, `NumPy`, `Matplotlib`, and a few utility libraries throughout this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard python libraries\n",
    "import io\n",
    "import os\n",
    "import urllib\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `Python` machine learning and deep learning libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the PyTorch deep learning library\n",
    "import torch, torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `Matplotlib` and `Seaborn` data visualization libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib and seaborn data visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CO8h9EPwGElt"
   },
   "source": [
    "Enable inline plotting with `Matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QFfHAY9EGElu"
   },
   "source": [
    "Ignore potential library warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-58e-iazJ8Aq"
   },
   "source": [
    "Create a structure of notebook sub-directories inside of the current **working directory** to store the data and the trained neural network models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LtB6DCWjJ-gD"
   },
   "outputs": [],
   "source": [
    "# create the data sub-directory\n",
    "data_directory = './data_shakespeare'\n",
    "if not os.path.exists(data_directory): os.makedirs(data_directory)\n",
    "\n",
    "# create the models sub-directory\n",
    "models_directory = './models_shakespeare'\n",
    "if not os.path.exists(models_directory): os.makedirs(models_directory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcYgp4Gl9_r6"
   },
   "source": [
    "Set a random `seed` value to obtain reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdbqEjHb9_r7"
   },
   "outputs": [],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value); # set pytorch seed CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpKQNDr09_r7"
   },
   "source": [
    "Google Colab provides free GPUs for running notebooks. However, if you execute this notebook as is, it will use your device's CPU. To run the lab on a GPU, go to `Runtime` > `Change runtime type` and set the Runtime type to `GPU` in the drop-down menu. Running this lab on a CPU is fine, but you will find that GPU computing is faster. *CUDA* indicates that the lab is being run on a GPU.\n",
    "\n",
    "Enable GPU computing by setting the device flag and initializing a CUDA seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IAFg7INc9_r7"
   },
   "outputs": [],
   "source": [
    "# set cpu or gpu enabled device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu').type\n",
    "\n",
    "# init deterministic GPU seed\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "# log type of device enabled\n",
    "print('[LOG] notebook with {} computation enabled'.format(str(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-7Ve4-_9_r7"
   },
   "source": [
    "Let's determine if we have access to a GPU provided by environments such as `Google Colab`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCpTB9x59_r8"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AH4V_q_sGElx"
   },
   "source": [
    "## 3. Dataset Download and Assessment\n",
    "\n",
    "In this section of the lab notebook, we will download and access the Tiny Shakespeare dataset. This dataset is instrumental for training our GPT model, allowing us to **generate text** in the **distinctive style of Shakespearean English**. \n",
    "\n",
    "### 3.1 Dataset Download\n",
    "\n",
    "In this section of the lab notebook, we will download and access the **Tiny Shakespeare** dataset. This dataset consists of **40,000 lines from a variety of Shakespeare's plays**. We will use this dataset to train our GPT model to generate text in the style of Shakespeare. As featured in Andrej Karpathy's blog post *The Unreasonable Effectiveness of Recurrent Neural Networks*: http://karpathy.github.io/2015/05/21/rnn-effectiveness/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 400px\" src=\"./shakespeare.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by downloading the dataset and inspecting its contents. The dataset is available as a plain text file from a public URL. We will read the content of the file and decode it to a string format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the tiny_shakespeare dataset\n",
    "url = 'https://raw.githubusercontent.com/HSG-AIML-Teaching/GSERM2024-Lab/master/lab_08/01_data/tiny_shakespeare_dataset.txt'\n",
    "text = urlopen(url).read().decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Dataset Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the first 400 characters of the dataset to get an initial look at the content. This will help us understand the structure of the text data we will be working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:400])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe that the dataset provides a rich set of language patterns and vocabulary, making it ideal for training language models like the **Generative Pretrained Transformer (GPT)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will first split the tiny Shakespeare dataset into training and evaluation sets. Following, we will create a dataset from the text by converting it into a set of sequences of `n` characters each. These sequences will be used to train our GPT model. Each sequence will help the model learn to predict the next character based on the previous `n` characters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Conduct Train-Test Split"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now split the dataset into training and evaluation sets. We will use **90% of the text for training** our GPT model and the remaining **10% for evaluating** its performance. This split will help us to both train the model and assess its ability to generalize to unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the fraction of the training set to 90% of the original dataset\n",
    "total_raw_chars = len(text)\n",
    "train_text = text[:int(total_raw_chars * 0.9)]  # 90% of the text for training\n",
    "eval_text = text[int(total_raw_chars * 0.9):]  # 10% of the text for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's inspect the number of training and evaluation characters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the length of the training and evaluation sets\n",
    "print(f'Training set: {len(train_text)} characters')\n",
    "print(f'Evaluation set: {len(eval_text)} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Transform Dataset into Sequences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "AWDJBNr8GElx"
   },
   "source": [
    "Next, we will transform our text dataset into a format that can be processed by our GPT model. Specifically, we will map each character in the text to a unique integer and vice versa. This step is crucial for converting textual data into numerical data with which the model can work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we **identify all unique characters** in the text. We also add a **special character '#'** that we will use to signify the end of a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of unique characters in the text and add '#' as an end-of-sentence indicator\n",
    "chars = ['#'] + sorted(list(set(text)))\n",
    "print(f'Unique characters: {chars}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we understand how the data looks and how to prepare it for training. Since neural networks cannot directly work with characters, we need to convert each character in the dataset to a unique identifier. To achieve this, we will create two mapping tables:\n",
    "\n",
    "- **`char_to_int`**: Maps each character to a unique integer.\n",
    "- **`int_to_char`**: Maps each integer back to its corresponding character.\n",
    "\n",
    "These mappings will allow us to convert the text data into a numerical format that can be processed by our GPT model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align='center' style='max-width: 800px' src='character_mapping.gif'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the mapping tables accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mappings from characters to integers and from integers to characters\n",
    "\n",
    "# character to integer\n",
    "char_to_int = {c: i for i, c in enumerate(chars)}\n",
    "print(f'Character to integer mapping: {char_to_int}')\n",
    "\n",
    "# integer to character\n",
    "int_to_char = {i: c for i, c in enumerate(chars)}\n",
    "print(f'Integer to character mapping: {int_to_char}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Third, we calculate the vocabulary size, which is the total number of unique characters including our special end-of-sentence character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the vocabulary size\n",
    "vocab_size = len(chars)  # How many unique characters are in the text?\n",
    "print(f'Vocabulary size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we specify the training sequence length, which determines how many characters long each training sample should be. This parameter is crucial because the model will learn to predict the next character based on the previous `seq_length` characters. For our GPT model, we will set the `seq_length` to 100, meaning each training sample will consist of 100 characters from the text of the **Tiny Shakespeare** dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how long is the sequence that we want to retrieve from the dataset?\n",
    "seq_length = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build our **ShakespeareDataset** class. This class will be responsible for providing the data samples and their corresponding targets to the model during training. The **ShakespeareDataset** class is very similar to the one used in the LSTM lab session, but here it is tailored to work with sequences of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the ShakespeareDataset class\n",
    "class ShakespeareDataset(Dataset):\n",
    "\n",
    "    # init dataset class\n",
    "    def __init__(self, text_list, seq_length):\n",
    "        \n",
    "        # text_list: text to be used for training or evaluation\n",
    "        # seq_length: length of the sequence we want to retrieve from the dataset\n",
    "        self.text = text_list\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    # define dataset length\n",
    "    def __len__(self):\n",
    "        \n",
    "        # return the total number of sequences that the dataset includes\n",
    "        return len(self.text) - self.seq_length\n",
    "\n",
    "    # define dataset getitem\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        # retrieve a random substring and its corresponding target\n",
    "        sequence = self.text[index:index + self.seq_length + 1]\n",
    "        target = self.text[index + 1:index + self.seq_length + 1] + '#'\n",
    "        \n",
    "        # convert characters of sequence and target to integers\n",
    "        sequence_tensor = torch.tensor([char_to_int[c] for c in sequence], dtype=torch.long)\n",
    "        target_tensor = torch.tensor([char_to_int[c] for c in target], dtype=torch.long)\n",
    "\n",
    "        # return sequence and target tensor\n",
    "        return sequence_tensor, target_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input sequence is a substring of a specified length, starting at a given index within the text. The target sequence, which is the same length as the input sequence, begins one character after the input sequence. This one-character offset is essential because it trains the model to predict the next character in the sequence. For example, if the input sequence is `To be, or not to be`, the corresponding target sequence would be `o be, or not to be#`, with the `#` character indicating the end of the sequence. Both the input sequence and the target sequence are converted from characters to their respective integer representations using the `char_to_int` mapping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align='center' style='max-width: 800px' src='sequence_structure.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "UWdK5_y2GEly"
   },
   "source": [
    "We will now create the **training** and **evaluation datasets** using the `ShakespeareDataset` class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ShakespeareDataset(text_list=train_text, seq_length=seq_length)\n",
    "eval_dataset = ShakespeareDataset(text_list=eval_text, seq_length=seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works by getting the 42nd **training** sequence and its corresponding targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.__getitem__(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And by getting the 42nd **evaluation** sequence and its corresponding targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset.__getitem__(42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement the GPT architecture for the text generation model. Furthermore, we will specify the loss function, learning rate, and optimization technique to be used during the training of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Implementation of the Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will implement the architecture of the GPT neural network used to generate text based on the given input sequences. The neural network, which we name **'GPTModel'**, consists of **transformer blocks**. The core components include embedding layers, multi-head self-attention mechanisms, and feedforward neural networks, which together form the transformer blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-height: 400px\" src=\"gpt_architecture.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(left) GPT 1 architecture and training objectives as proposed by Radford et al, 2018. (right) Input transformations for fine-tuning on different tasks. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.1 Self-Attention Mechanism"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In **Recurrent Neural Network (RNN)** models (e.g., the Long-Sort Term-Memory (LSTM)), the assumption is that the current time step depends on the most recent time steps. However, this can be limiting when dealing with long sequences or sequences with complex dependencies between elements.\n",
    "\n",
    "In contrast, a **Transformer Network**  learns dependencies from the data using the **self-attention mechanism**, which has proven successful in handling various kinds of data. the self-attention mechanism allows the model to weigh different parts of the input sequence based on their importance for the current prediction. This helps the network better capture long-range dependencies and contextual information.\n",
    "\n",
    "To compute **self-attention**, the input sequence is first mapped to **query (q)**, **key (k)**, and **value (v)** vectors. These vectors are then used to compute a weight (score) for each input element based on how well it matches the query vector. The weighted sum of the value vectors is then used as the output of the self-attention layer. This enables the network to attend to different parts of the input sequence depending on the current query vector.\n",
    "\n",
    "We divide each element into three parts: query, key, and value to organize information.\n",
    "\n",
    "- **Query:** What kind of information do we want to retrieve?\n",
    "- **Key:** What information is available in the data?\n",
    "- **Value:** What relevant information meets our requirements?\n",
    "\n",
    "First, we must calculate the attention score to determine the key and value relevance. There are various methods to calculate this; in this lab session, we use a technique named **Scaled Dot Product** as described in the following an introduced in Vaswani et al., 2017. The scaling factor prevents the dot product from getting too large, which can cause the softmax function to approach either 0 or 1, making the gradients too small or too large.\n",
    "\n",
    "$$attn(q,k) = \\frac{q^T \\cdot k}{\\sqrt{|k|}}$$\n",
    "\n",
    "Then, we use the attention to weight the value vectors and derive the output of the self-attention mechanism:\n",
    "\n",
    "$$output = softmax(attn(q, k)) \\cdot v$$\n",
    "\n",
    "This process of **self-attention** is a powerful tool in modern neural networks, enabling them to process and understand complex data sequences efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Illustrative Example: \"All is well\"\n",
    "\n",
    "Let's consider the famous Shakespeare phrase **\"All is well\"** and see how self-attention can be applied to it. First, we need to tokenize and embed the phrase. Assume each word is embedded into a vector of dimension 3 for simplicity. We'll represent our phrase as `[\"All\", \"is\", \"well\"]`. Next, we create query, key, and value vectors. For simplicity, let's use the same vectors for query, key, and value. Here are some example vectors (randomly chosen for illustration):\n",
    "\n",
    "| Word | Vector (q, k, v)         |\n",
    "|------|--------------------------|\n",
    "| All  | [1, 0, 1]                |\n",
    "| is   | [0, 1, 0]                |\n",
    "| well | [1, 1, 0]                |\n",
    "\n",
    "Let's compute the **attention** scores for the word \"All\" in the phrase **\"All is well\"** using the **Scaled Dot Product**:\n",
    "\n",
    "1. For **\"All\"** (query) and **\"All\"** (key):\n",
    "\n",
    "$$ attn(q=All, k=All) = \\frac{[1, 0, 1]^{T} \\cdot [1, 0, 1]}{\\sqrt{3}} = \\frac{2}{\\sqrt{3}} $$\n",
    "\n",
    "2. For **\"All\"** (query) and **\"is\"** (key):\n",
    "\n",
    "$$ attn(q=All, k=is) = \\frac{[1, 0, 1]^{T} \\cdot [0, 1, 0]}{\\sqrt{3}} = \\frac{0}{\\sqrt{3}} = 0 $$\n",
    "\n",
    "3. For **\"All\"** (query) and **\"well\"** (key):\n",
    "\n",
    "$$ attn(q=All, k=well) = \\frac{[1, 0, 1]^{T} \\cdot [1, 1, 0]}{\\sqrt{3}} = \\frac{1}{\\sqrt{3}} $$\n",
    "\n",
    "To convert these individual attention scores into probabilities, we apply the **softmax function**. For simplicity, let’s assume the softmax results are:\n",
    "\n",
    "$$ softmax(scores) = [0.5, 0.2, 0.3] $$\n",
    "\n",
    "Finally, we use obtained softmax probabilities to **weight the individual word vectors**. Thereby, we multiply each value vector by its corresponding softmax score and sum them to get the final attention output for the word \"All\".\n",
    "\n",
    "$$ output(All) = 0.5 \\cdot [1, 0, 1] + 0.2 \\cdot [0, 1, 0] + 0.3 \\cdot [1, 1, 0] $$\n",
    "\n",
    "The final attention output for \"All\" is:\n",
    "\n",
    "$$ output(All) = [0.8, 0.5, 0.5] $$\n",
    "\n",
    "This illustrates how the attention mechanism processes each word in the phrase \"All is well\" by calculating scores, applying the softmax, and weighting the value vectors to produce the final self-attention output. **The resulting vector for \"All\" now incorporates information from the entire phrase, reflecting the contextual importance of each word relative to \"All\".** In our example the attention output `[0.8, 0.5, 0.5]` indicates that \"All\" attends mostly to itself and also captures some contextual information from \"is\" and \"well\", with higher weight given to its own vector components.\n",
    "\n",
    "Next, we will use the self-attention mechanism to implement a self-attention module to be used later in our implementation of the **Generate Pretrained Transformer (GPT)** model. The self-attention mechanism allows the model to weigh different parts of the input sequence based on their relevance to the current task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the self-attention module\n",
    "class SelfAttention(nn.Module):\n",
    "    \n",
    "    # define the class constructor\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        \n",
    "        # call the super class constructor\n",
    "        super(SelfAttention, self).__init__()\n",
    "        \n",
    "        # embed_dim: the dimension of the input vectors\n",
    "        # num_heads: the number of heads in the self-attention mechanism\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        \n",
    "        # define the key projector to get key vectors\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # define the value projector to get value vectors\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # define the query projector to get query vectors\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "    \n",
    "    # define the forward pass\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # determine the current batch size \n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # determine the current sequence length\n",
    "        x_len = x.size(1)\n",
    "        \n",
    "        # project the input x to key, value, and query vectors\n",
    "        k_t = self.key(x).reshape(batch_size, x_len, self.num_heads, -1).permute(0, 2, 3, 1)\n",
    "        v = self.value(x).reshape(batch_size, x_len, self.num_heads, -1).transpose(1, 2)\n",
    "        q = self.query(x).reshape(batch_size, x_len, self.num_heads, -1).transpose(1, 2)\n",
    "\n",
    "        # k_t.shape == (batch_size, num_heads, embed_dim/num_heads, len)\n",
    "        # v.shape == (batch_size, num_heads, len, embed_dim/num_heads)\n",
    "        # q.shape == (batch_size, num_heads, len, embed_dim/num_heads)\n",
    "        \n",
    "        # calculate the attention scores\n",
    "        attn = torch.matmul(q, k_t) / math.sqrt(q.size(-1))\n",
    "        \n",
    "        # use the mask to prevent the model from attending to the future tokens\n",
    "        mask = torch.tril(torch.ones(x_len, x_len)).unsqueeze(0).unsqueeze(0).to(x.device)\n",
    "        \n",
    "        # apply attention masking\n",
    "        attn = attn.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # apply the softmax function to get the attention weights\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        \n",
    "        # calculate the weighted sum of the value vectors\n",
    "        y = torch.matmul(attn, v)\n",
    "\n",
    "        # transpose the output to match the original shape\n",
    "        y = y.transpose(1, 2)\n",
    "\n",
    "        # reshape the output to match the original shape\n",
    "        y = y.reshape(batch_size, x_len, -1)\n",
    "\n",
    "        # return output and attention scores\n",
    "        return y, attn  # note: we return the attention scores for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Self-Attention Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention masks are a crucial component of the GPT model and ensure the model does **not have access to future tokens** in the sequence during training. An attention mask is a tensor that specifies which positions in the input sequence should be attended to. In the context of GPT, it ensures that **each position in the sequence can only attend to the positions that came before it**. Without an attention mask, the model would be able to “cheat” by looking at future tokens when generating a sequence, leading to unrealistic and incorrect language modeling. The attention mask prevents this by masking out future tokens during training and inference.\n",
    "\n",
    "The attention mask is typically a **lower triangular matrix** filled with ones and zeros. Ones indicate positions that can be attended to, while zeros indicate masked positions. For example, in a sequence of length 6, the attention mask would look like this:\n",
    "\n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 0 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 1 & 0 & 0 \\\\\n",
    "1 & 1 & 1 & 1 & 1 & 0 \\\\\n",
    "1 & 1 & 1 & 1 & 1 & 1 \\\\\n",
    "\\end{bmatrix}\n",
    "\n",
    "In this matrix, the first token can only attend to itself, the second token can attend to itself and the first token, and so on. Imagine you’re writing the sentence **“The king hath slain the dragon”** one word at a time. Now, let’s illustrate a concrete calculation of the attention mechanism for the different words: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-height: 340px\" src=\"attention_mask.png\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Multi-Head Self-Attention"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The motivation for **multi-head self-attention** is to apply different attention layers/filters to the same input. Each layer/filter can **attend to different aspects of the input data in parallel**. This strategy allows the model to capture a richer set of relationships and dependencies.\n",
    "\n",
    "Multi-head self-attention enables the model to look at the input sequence from multiple perspectives, which improves its ability to understand and generate complex sequences. By having multiple heads, the model can learn various features of the input data simultaneously, enhancing its performance and robustness.\n",
    "\n",
    "Let's now use the previously implemented **self-attention mechanism** to build the **multi-head attention mechanism**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the multihead attention\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    # define the class constructor\n",
    "    def __init__(self, embed_dim, num_heads, dropout):\n",
    "        \n",
    "        # call the super class constructor\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        \n",
    "        # embed_dim: the dimension of the input vectors\n",
    "        # num_heads: the number of heads in the multi-head attention\n",
    "        \n",
    "        # define the self-attention mechanism\n",
    "        self.self_attention = SelfAttention(embed_dim, num_heads)\n",
    "        \n",
    "        # project the concatenated multi-head attention output to the original dimension\n",
    "        self.proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        # define dropout layer\n",
    "        self.proj_dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    # define the forward pass\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # apply self-attention mechanism\n",
    "        y, attn = self.self_attention(x)\n",
    "        \n",
    "        # apply projection to the output\n",
    "        y = self.proj(y)\n",
    "\n",
    "        # apply dropout to the output\n",
    "        y = self.proj_dropout(y)\n",
    "\n",
    "        # return output and attention scores\n",
    "        return y, attn  # note: we return the attention scores for visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.4 Feed Forward Layers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "nRtQMVIoGEl6"
   },
   "source": [
    "The **Feed Forward Layers** are a crucial component of the **Transformer block**. It consists of several nonlinear layers that process the input data. After applying self-attention, the output is passed through a feed-forward neural network to introduce more complex, nonlinear transformations. This helps the model learn patterns in the data and improves its ability to make accurate predictions. The feed-forward layer is applied independently to each position, and it consists of **two linear layers** with a **non-linear ReLU activation** in between."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the feed-forward module\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    # define the class constructor\n",
    "    def __init__(self, embed_dim, num_heads, dropout):\n",
    "        \n",
    "        # call the super class constructor\n",
    "        super(FeedForward, self).__init__()\n",
    "        \n",
    "        # define the feed-forward network as a sequential model\n",
    "        self.net = nn.Sequential(\n",
    "            \n",
    "            # first linear layer with output dimension 4 times the embedding dimension\n",
    "            nn.Linear(embed_dim, num_heads * embed_dim),\n",
    "            \n",
    "            # apply ReLU activation\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # second linear layer with output dimension equal to the embedding dimension\n",
    "            nn.Linear(num_heads * embed_dim, embed_dim),\n",
    "            \n",
    "            # apply dropout for regularization\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # pass the input through the feed-forward network\n",
    "        y = self.net(x)\n",
    "\n",
    "        # return the output\n",
    "        return y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qyrOdPPkGEl7"
   },
   "source": [
    "### 5.1.5 Transformer Block"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement a **Transformer Block**, which is the fundamental component of the **GPT model**. The transformer block consists of several key elements (introduced before) that work together to process the input data efficiently.\n",
    "\n",
    "> 1. **Multi-head Self-Attention:** This allows the model to attend to different parts of the input capturing various relationships in the data.\n",
    "> 2. **Feedforward Neural Network:** This layer introduces non-linearity and helps the model learn complex patterns evident in the data.\n",
    "> 3. **Layer Normalization:** Applied before the self-attention and feedforward layers to stabilize and speed up the training process.\n",
    "\n",
    "Please note, Layer normalization ensures that the inputs to each layer maintain a stable distribution, which helps in faster and more reliable training of the model. By normalizing the inputs, the model can avoid issues related to gradient vanishing or explosion, leading to more efficient learning.\n",
    "\n",
    "Let's now see how we can implement a single **Transformer Block** in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the transformer block\n",
    "class TransformerBlock(nn.Module):\n",
    "\n",
    "    # define the class constructor\n",
    "    def __init__(self, embed_dim, num_heads, dropout):\n",
    "        \n",
    "        # embed_dim: embedding dimension\n",
    "        # num_heads: the number of heads in the multi-head attention\n",
    "        \n",
    "        # call the super class constructor\n",
    "        super(TransformerBlock, self).__init__()\n",
    "\n",
    "        # define the multi-head attention mechanism\n",
    "        self.sa = MultiheadAttention(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        # define the feed-forward network\n",
    "        self.ffwd = FeedForward(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        # define the first layer normalization\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # define the second layer normalization\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # apply the multi-head attention\n",
    "        attn_output, attn = self.sa(x)\n",
    "        \n",
    "        # apply the first layer normalization (residual connection)\n",
    "        x = self.ln1(x + attn_output)\n",
    "        \n",
    "        # apply the feed-forward network\n",
    "        ffwd_output = self.ffwd(x)\n",
    "\n",
    "        # apply the second layer normalization (residual connection)\n",
    "        x = self.ln2(x + ffwd_output)\n",
    "\n",
    "        # return output and attention scores\n",
    "        return x, attn  # note: we return the attention scores for visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This **Transformer Block** forms the foundation for building deeper and more complex models such as the **Generative Pretrained Transformer (GPT) model**. It enables the GPT model to generate coherent and contextually relevant text given a predefined input (or prompt)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.5 Positional Embedding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Generative Pretrained Transformer (GPT)** architecture does not have a built-in notion of order or sequence since it operates on sets of inputs. However, understanding the order of words in a sentence is crucial for capturing the meaning. For example, consider these two sentences:\n",
    "\n",
    "1. \"The king hath slain the dragon.\"\n",
    "2. \"The dragon hath slain the king.\"\n",
    "\n",
    "The position of the words drastically changes the meaning of the sentence. Positional embeddings are used to inject information about the position of each word in the sequence into the model. This helps the GPT model understand the order and relationships between words, enabling it to capture the context more accurately.\n",
    "\n",
    "To illustrate this, let's build on our earlier example with the phrase **\"The king hath slain the dragon\"** and see how positional embeddings are applied. Assume our phrase \"The king hath slain the dragon\" is tokenized, and each word is embedded into a vector of dimension 3. Now, we introduce positional embeddings to encode the position of each word in the sequence. Let's assume the positional embeddings for the first five positions are as outlined below. We add these positional embeddings to the original word embeddings to obtain the final input embeddings for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-height: 250px\" src=\"positional_embeddings.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These final embeddings, which combine the original and positional word embeddings, are then fed into the GPT model. This allows the model to incorporate the positional information and better understand the sequence's structure and meaning."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.6 Generative Pretrained Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will combine all the components we have discussed (self-attention, multi-head self-attention, feedforward layers, layer normalization, and positional embeddings) to build the **Generative Pretrained Transformer (GPT)** model. This comprehensive model will be capable of generating coherent and contextually relevant text based on the input sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the GPT language model\n",
    "class GPTNet(nn.Module):\n",
    "\n",
    "    # define the class constructor\n",
    "    def __init__(self, embed_dim, num_heads, n_layer, pos_len, dropout=0.2):\n",
    "        \n",
    "        # call the super class constructor\n",
    "        super(GPTNet, self).__init__()\n",
    "\n",
    "        # store the maximum length of the position embedding\n",
    "        self.pos_len = pos_len\n",
    "        \n",
    "        # create an embedding table for tokens\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, embed_dim)\n",
    "        \n",
    "        # create an embedding table for positions\n",
    "        self.position_embedding_table = nn.Embedding(pos_len, embed_dim)\n",
    "        \n",
    "        # stack multiple transformer blocks\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, dropout=dropout) for _ in range(n_layer)]\n",
    "        )\n",
    "        \n",
    "        # create a final layer normalization\n",
    "        self.ln_f = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # create a final linear layer to get the logits\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "        # initialize the weights of the model\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    # method to initialize weights\n",
    "    def _init_weights(self, module):\n",
    "        \n",
    "        # check if the module is a linear layer\n",
    "        if isinstance(module, nn.Linear):\n",
    "            \n",
    "            # initialize weights for linear layers with normal distribution\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            \n",
    "            # initialize biases to zeros if they exist\n",
    "            if module.bias is not None:\n",
    "\n",
    "                # init the biases with zeros\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        \n",
    "        # check if the module is an embedding layer\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            \n",
    "            # initialize weights for embedding layers with normal distribution\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    # define the forward pass\n",
    "    def forward(self, input_seq):\n",
    "        \n",
    "        # get the batch size (B) and sequence length (T)\n",
    "        B, T = input_seq.shape\n",
    "\n",
    "        # map the tokens to their embeddings\n",
    "        tok_emb = self.token_embedding_table(input_seq) # shape: (B, T, embed_dim)\n",
    "        \n",
    "        # create a range of positions for the input sequence\n",
    "        positions = torch.arange(T, device=input_seq.device)\n",
    "        \n",
    "        # map the positions to their embeddings\n",
    "        pos_emb = self.position_embedding_table(positions).unsqueeze(0).repeat(B, 1, 1) # shape: (B, T, embed_dim)\n",
    "        \n",
    "        # combine the token and position embeddings\n",
    "        x = tok_emb + pos_emb # shape: (B, T, embed_dim)\n",
    "\n",
    "        # initialize list to save attention scores for visualization\n",
    "        attn_list = []\n",
    "        \n",
    "        # iterate over each transformer block\n",
    "        for block in self.blocks:\n",
    "            \n",
    "            # pass through each transformer block and get attention scores\n",
    "            x, attn = block(x) # shape: (B, T, embed_dim)\n",
    "\n",
    "            # collect attention scores\n",
    "            attn_list.append(attn)\n",
    "        \n",
    "        # apply the final layer normalization\n",
    "        x = self.ln_f(x) # shape: (B, T, embed_dim)\n",
    "\n",
    "        # get the logits (predictions for the next token in the sequence)\n",
    "        logits = self.lm_head(x) # shape: (B, T, vocab_size)\n",
    "            \n",
    "        # return logits, and attention scores\n",
    "        return logits, attn_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented our first GPT network, we are ready to instantiate a network model to be trained. Therefore, we need to define several important hyperparameters that will shape the architecture and performance of our model. These hyperparameters include the embedding dimension, the number of attention heads, the number of layers, and the positional length. \n",
    "\n",
    "- **Embedding Dimension (`embed_dim`)**: This parameter defines the size of the embedding vectors for each token in the input sequence. A larger embedding dimension allows the model to capture more complex relationships between tokens. In our case, we set `embed_dim` to 384.\n",
    "\n",
    "- **Number of Attention Heads (`num_heads`)**: The multi-head attention mechanism allows the model to focus on different parts of the input sequence simultaneously. We set the number of attention heads to 6, which enables the model to capture multiple aspects of the context.\n",
    "\n",
    "- **Number of Layers (`n_layer`)**: This parameter defines the number of transformer layers in the GPT model. Each layer consists of attention and feed-forward sub-layers, allowing the model to learn hierarchical representations. We use 6 layers in our model.\n",
    "\n",
    "- **Positional Length (`pos_len`)**: The positional encoding allows the model to take the order of the tokens into account. We set `pos_len` to the sequence length plus one to account for the special token (`<#>`).\n",
    "\n",
    "Let's define these hyperparameters in our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define embedding dimensionality\n",
    "embed_dim = 384\n",
    "\n",
    "# define number of attention heads\n",
    "num_heads = 4\n",
    "\n",
    "# define number of attention layers\n",
    "n_layer = 4\n",
    "\n",
    "# define length of position encodings\n",
    "pos_len = seq_length + 1 # dont forget the <#> token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these hyperparameter specifications we will now instantiate a `GPTNet` model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPTNet(embed_dim=embed_dim, num_heads=num_heads, n_layer=n_layer, pos_len=pos_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's push the initialized `GPTNet` model to the enabled computing `device`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's double-check if our model was deployed to the GPU, if available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is initialized, we can visualize the model structure and review the implemented network architecture before training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the initialized architectures\n",
    "print('[LOG] GPTNet architecture:\\n\\n{}\\n'.format(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like intended? Brilliant! Finally, let's have a look into the number of model parameters that we aim to train in the next steps of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the number of model parameters\n",
    "num_params = 0\n",
    "\n",
    "# iterate over the distinct parameters\n",
    "for param in model.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    num_params += param.numel()\n",
    "    \n",
    "# print the number of model paramters\n",
    "print('[LOG] Number of to be trained GPTNet model parameters: {}.'.format(num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, our \"simple\" CIFAR10Net model already encompasses an impressive number **7,188,162 model parameters** to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready to train the network now that we have successfully implemented and defined the transformer model. However, we must define an appropriate loss function before starting the training. Remember, we aim to train our model to learn a set of model parameters $\\theta$ that minimize the prediction error of the true character $c^{i}$ of a given Shakespeare character sequence $x^{i}$ and its predicted character $\\hat{c}^{i} = f_\\theta(x^{i})$ as faithfully as possible.\n",
    "\n",
    "We use the **'Cross Entropy Loss'** in this lab. During training, the cross-entropy loss will penalize models that result in a high prediction error between the predicted character labels $\\hat{c}^{i}$ and their respective true character label $c^{i}$. Let's instantiate the cross-entropy loss via the following PyTorch command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization criterion / loss function\n",
    "cross_entropy_loss = nn.CrossEntropyLoss()\n",
    "\n",
    "# push the initialized cross_entropy_loss computation to the enabled computing device\n",
    "cross_entropy_loss = cross_entropy_loss.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the loss magnitude of a certain mini-batch, PyTorch automatically computes the gradients. Furthermore, based on the gradient, the library also helps us optimize and update the network parameters $\\theta$.\n",
    "\n",
    "We will use the **Adam optimization** and set the `learning rate` to 0.00001 (a larger learning rate may cause overfitting). With each mini-batch step, the optimizer will update the model parameters $\\theta$ according to the degree of prediction error (the cross-entropy loss).\n",
    "\n",
    "Let's define the learning rate and optimization strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define learning rate and optimization strategy\n",
    "learning_rate = 0.00001\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully implemented and defined the transformer model blocks, let's take some time to review the `GPTNet` model definition as well as the `loss`. Please read the above code and comments carefully and don't hesitate to ask any questions you might have."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Neural Network Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train our GPT model (as implemented in the section above) using the preprocessed Shakespeare dataset. Specifically, we will examine the distinct training steps and how to monitor the training progress."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Preparing the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have pre-processed the dataset, implemented the GPT model, and defined the classification error. Let's now start to train a corresponding model for **1,000 iterations** with a **mini-batch size of 64** Shakespeare sequences per batch. After processing each mini-batch of sequences, the parameters of the network will be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the training parameters\n",
    "train_iterations = 1000 # number of training iterations\n",
    "mini_batch_size = 64 # size of the mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's specify and instantiate a corresponding PyTorch data loader that feeds the image tensors to our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=mini_batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Running the Network Training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start training the model. The training procedure for each mini-batch is performed as follows:\n",
    "\n",
    ">1. Do a forward pass through the GPTNet network, \n",
    ">2. Compute the negative log-likelihood prediction error $\\mathcal{L}^{CE}_{\\theta}(c^{i};\\hat{c}^{i})$, \n",
    ">3. Do a backward pass through the GPTNet network, and \n",
    ">4. Update the parameters of the network $f_\\theta(\\cdot)$.\n",
    "\n",
    "We will monitor whether the **loss decreases as training progresses** to ensure learning while training our GPT model. Therefore, we obtain and evaluate the classification performance of the entire training dataset after each training epoch. Based on this evaluation, we can conclude the training progress and whether the loss is converging (indicating that the model might not improve further).\n",
    "\n",
    "The following elements of the network training code below should be given particular attention:\n",
    "\n",
    ">- `loss.backward()` computes the gradients based on the magnitude of the reconstruction loss,\n",
    ">- `optimizer.step()` updates the network parameters based on the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "train_epoch_perplexity = []\n",
    "\n",
    "# set the model in training mode\n",
    "model.train()\n",
    "\n",
    "# init collection of training iteration losses\n",
    "train_iteration_losses = []\n",
    "\n",
    "# init and wrap range of training iterations\n",
    "training_iterations = tqdm(range(0, train_iterations), position=0, leave=True)\n",
    "\n",
    "# iterate over training iterations\n",
    "for train_iter in training_iterations:\n",
    "\n",
    "    # iterate over epoch batches\n",
    "    inputs, targets = next(iter(train_dataloader))\n",
    "\n",
    "    # push mini-batch data to computation device\n",
    "    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "    # run forward pass through the network\n",
    "    logits, _ = model(inputs)\n",
    "\n",
    "    # get the shape of logits (B: batch size, T: sequence length, C: number of classes/vocabulary size)\n",
    "    B, T, C = logits.shape\n",
    "\n",
    "    # reshape logits for loss calculation, flattening the batch and sequence dimensions\n",
    "    logits = logits.view(B * T, C)\n",
    "\n",
    "    # reshape targets\n",
    "    targets = targets.view(B * T)\n",
    "\n",
    "    # determine classification loss\n",
    "    loss = cross_entropy_loss(logits, targets)\n",
    "\n",
    "    # reset graph gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # run backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update network parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    # collect mini-batch loss\n",
    "    train_iteration_losses.append(loss.data.item())\n",
    "\n",
    "    # print training progress and training loss\n",
    "    now = datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    training_iterations.set_description(\n",
    "        (\n",
    "            '[LOG {}] iteration: {}, train-loss: {}'.format(str(now), str(train_iter).zfill(6), str(round(loss.data.item(), 6)))\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # save the model checkpoint every 1000 iterations\n",
    "    if train_iter % 1000 == 0:\n",
    "\n",
    "        # set filename of actual model\n",
    "        model_name = 'gptnet_model_iteration_{}.pth'.format(str(train_iter))\n",
    "\n",
    "        # save current model to GDrive models directory\n",
    "        torch.save(model.state_dict(), os.path.join(models_directory, model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon successful training, let's visualize and inspect the training loss per iteration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot the training epochs vs. the epochs' classification error\n",
    "ax.plot(np.array(range(1, len(train_iteration_losses)+1)), train_iteration_losses, label='iteration loss (blue)')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[Training Iteration $i$]\", fontsize=10)\n",
    "ax.set_ylabel(\"[Prediction Error $\\mathcal{L}^{CE}$]\", fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Iterations $i$ vs. Prediction Error $L^{CE}$', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, fantastic. The training error converges nicely. We could definitely **train the network for more iterations** until the error converges. But let's stay with the 10,000 training iterations for now and continue with evaluating our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Neural Network Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before evaluating our model, let's **load the best-performing model**. Remember that we stored a snapshot of the model after each training epoch to our local model directory. We will now load the last snapshot saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init pre-trained model architecture\n",
    "best_model = GPTNet(embed_dim=embed_dim, num_heads=num_heads, n_layer=n_layer, pos_len=pos_len)\n",
    "\n",
    "# restore pre-trained model snapshot\n",
    "best_model_name = 'https://raw.githubusercontent.com/HSG-AIML-Teaching/GSERM2024-Lab/master/lab_08/03_models/gptnet_model_iteration_900000.pth'\n",
    "\n",
    "# read stored model from the remote location\n",
    "model_bytes = urllib.request.urlopen(best_model_name)\n",
    "\n",
    "# load model tensor from io.BytesIO object\n",
    "model_buffer = io.BytesIO(model_bytes.read())\n",
    "\n",
    "# load pre-trained models\n",
    "best_model.load_state_dict(torch.load(model_buffer, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the model was loaded successfully:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model in evaluation mode\n",
    "best_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Model Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use our pre-trained GPT model to generate text. The model takes a prompt as input and **generates coherent and contextually relevant text** based on the patterns it learned during training.\n",
    "\n",
    "Let's start by defining a method for text generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shakespeare_chat_gpt(model, prompt, max_new_tokens=300):\n",
    "\n",
    "    # initialize generated text with the prompt\n",
    "    generated_text = prompt\n",
    "    \n",
    "    # convert the prompt to tensor\n",
    "    prompt_idx = torch.tensor([char_to_int[c] for c in prompt], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    # iterate over number of tokens\n",
    "    for _ in range(max_new_tokens):\n",
    "        \n",
    "        # crop prompt, in case it's longer than pos_len\n",
    "        prompt_idx_cond = prompt_idx[:, -model.pos_len:]\n",
    "        \n",
    "        # get the predictions\n",
    "        logits, _ = model(prompt_idx_cond)\n",
    "        \n",
    "        # we only need the logits for the last token\n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        # convert logits to probabilities\n",
    "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "        \n",
    "        # sample from the distribution\n",
    "        predicted_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "        \n",
    "        # append new token to the original prompt\n",
    "        prompt_idx = torch.cat((prompt_idx, predicted_token), dim=1) # (B, T+1)\n",
    "        \n",
    "        # convert the token ID to string and add it to `generated_text`\n",
    "        generated_text += int_to_char[predicted_token[0].item()]\n",
    "        \n",
    "        # stop generation if a specific stop token is reached\n",
    "        if int_to_char[predicted_token[0].item()] == \"#\": \n",
    "\n",
    "            # stop generation\n",
    "            break\n",
    "\n",
    "    # return generated text\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate the capabilities of our GPT model, let’s generate text with a few different humorous Shakespearean-style prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 1:** In this example, we have a humorous dialogue between a king and his jester. The king starts with an insult, and we let the model generate the jester’s witty response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first example prompt\n",
    "prompt = \"KING:\\n Thou art a veritable fool, my jester.\\n JESTER:\\n \"\n",
    "generated_text = shakespeare_chat_gpt(model=best_model, prompt=prompt, max_new_tokens=100)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 2:** Here, we use a classic line from the witches in Shakespeare’s 'Macbeth' and let the model generate what the cauldron might say in response, adding a humorous twist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second example prompt\n",
    "prompt = \"WITCH:\\n Double, double toil and trouble.\\n CAULDRON:\\n \"\n",
    "generated_text = shakespeare_chat_gpt(model=best_model, prompt=prompt, max_new_tokens=100)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3:** This example features a conversation between a servant and a knight. The servant questions the knight’s eating habits, and the model generates the knight’s comical retort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third example prompt\n",
    "prompt = \"SERVANT:\\n Why dost thou eat so much?\\n KNIGHT:\\n \"\n",
    "generated_text = shakespeare_chat_gpt(model=best_model, prompt=prompt, max_new_tokens=100)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, the generated text examples demonstrate the model’s ability to create contextually relevant and coherent continuations based on the input prompt. Here are a few key points to note:\n",
    "\n",
    "- **Context:** The model generates text that fits the context and style of the original prompt.\n",
    "- **Coherence:** The generated text maintains a logical flow, making it appear as if it were part of an actual dialogue.\n",
    "- **Humor:** By choosing humorous prompts, we can see how the model handles creative funny continuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Attention Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will **visualize the attention mechanisms** of our trained GPT model. Attention visualization enables us understand which parts of the input sequence the model focuses on when generating each token in the output. This can provide insights into how the model processes and interprets the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.1 Text Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will generate text using our trained GPT model. We will start with an initial prompt and generate additional tokens step by step. During this process, we will also **extract and store the attention weights** for each generated token. We start with an initial prompt, **“To be “**, which the model will use as the basis for text generation. Next, we set the maximum number of new tokens to generate. In this case, we limit the model to generate up to 10 additional tokens. Following, we generate new tokens iteratively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the prompt for generation\n",
    "prompt = \"To be \"\n",
    "\n",
    "# set the maximum number of new tokens to generate\n",
    "max_new_tokens = 10\n",
    "\n",
    "# convert the prompt into tensor indices\n",
    "prompt_idx = torch.tensor([char_to_int[c] for c in prompt], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "# initialize an empty list to store attention weights\n",
    "attention_weights = []\n",
    "\n",
    "# init generated text with original prompt\n",
    "generated_text = prompt\n",
    "\n",
    "# generate tokens step by step\n",
    "for _ in range(max_new_tokens):\n",
    "    \n",
    "    # crop the prompt if it's longer than the model's position length\n",
    "    prompt_idx_cond = prompt_idx[:, -model.pos_len:]\n",
    "\n",
    "    # get the predictions (logits) and attention scores from the model\n",
    "    logits, attn_list = model(prompt_idx_cond)\n",
    "    \n",
    "    # we only need the logits for the last token\n",
    "    logits = logits[:, -1, :]\n",
    "    \n",
    "    # convert logits to probabilities\n",
    "    probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "    \n",
    "    # sample from the distribution\n",
    "    predicted_token = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "    \n",
    "    # append new token to the original prompt\n",
    "    prompt_idx = torch.cat((prompt_idx, predicted_token), dim=1) # (B, T+1)\n",
    "    \n",
    "    # convert the token ID to string and add it to `generated_text`\n",
    "    generated_text += int_to_char[predicted_token[0].item()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's review the text generated by our GPT model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.2 Attention Heatmaps Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's visualize and inspect the attention score heatmaps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid of subplots for attention heads\n",
    "fig, axes = plt.subplots(nrows=n_layer, ncols=num_heads, figsize=(30, 30))\n",
    "\n",
    "# iterate over attention layers\n",
    "for layer_idx in range(n_layer):\n",
    "\n",
    "    # determine attention layer output\n",
    "    attention_layer_out = attn_list[layer_idx]\n",
    "\n",
    "    # remove the batch dimension\n",
    "    attention_layer_out = attention_layer_out.squeeze(0)\n",
    "\n",
    "    # iterate over attention heads\n",
    "    for head_idx in range(num_heads):  # for each attention head\n",
    "\n",
    "        # determine attention layer head\n",
    "        attention = attention_layer_out[head_idx]\n",
    "\n",
    "        # convert to numpy array\n",
    "        attention = attention.detach().cpu().numpy()\n",
    "\n",
    "        # determine subplot axis\n",
    "        ax = axes[layer_idx, head_idx]\n",
    "\n",
    "        # visualize attention scores\n",
    "        ax.imshow(attention, cmap='gray', interpolation='nearest')  # Plot the attention in the current subplot\n",
    "\n",
    "        # set subplot title\n",
    "        ax.set_title(f\"Attention Layer {layer_idx + 1} - Head {head_idx + 1}\", fontsize=20)\n",
    "\n",
    "        # set subplot x and y ticks\n",
    "        ax.set_xticks(range(len(generated_text)-1), [*generated_text[:-1]], fontsize=15)\n",
    "        ax.set_yticks(range(len(generated_text)-1), [*generated_text[1:]], fontsize=15)\n",
    "\n",
    "        # set subplot x and y ticks location\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.tick_params(axis='x', which='both', bottom=False, top=True)\n",
    "\n",
    "# add a main title to the figure\n",
    "fig.suptitle('Attention Visualization Across Layers and Heads', fontsize=34, y=1.00)\n",
    "\n",
    "# adjust rect to make space for the title\n",
    "plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "# Show the figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the heatmap, the rows represent the input tokens, and the columns represent the attention scores assigned to each token by the model. Brighter colors indicate higher attention scores. By examining the heatmap, we can gain insights into the model’s focus and the relationships it has learned between different tokens in the input sequence.\n",
    "\n",
    "1. **Diagonal Dominance**: If the heatmap shows a strong diagonal pattern, it indicates that the model is primarily attending to the current token or nearby tokens, reflecting a local context focus.\n",
    "2. **Long-Range Dependencies**: Attention patterns that extend beyond the diagonal indicate that the model is capturing long-range dependencies, which is crucial for understanding complex sequences.\n",
    "\n",
    "By visualizing and interpreting the attention weights, we can better understand the inner workings of our GPT model and improve its performance and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.2.3 Model Perplexity Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perplexity is a crucial metric used to **evaluate the performance of a language model**. It measures how well the model predicts a given sequence of words. Essentially, perplexity **assesses the degree of \"surprise\" the model has when encountering the sequence**. A lower perplexity indicates a better-performing model, as it suggests that the model is less surprised by the given sequence, implying better predictions.\n",
    "\n",
    "The formula to compute perplexity for a single sequence is:\n",
    "\n",
    "$$\\text{Perplexity} = \\exp \\left( -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i \\mid w_1, w_2, \\ldots, w_{i-1}) \\right)$$\n",
    "\n",
    "Where \\( N \\) is the number of words in the sequence. To understand perplexity, consider the following example sequence of words: \"I am studying machine learning\". The probabilities assigned by the model to each word in the context of the previous words might be as follows:\n",
    "\n",
    "- $P(\\text{I}) = 0.1$\n",
    "- $P(\\text{am} \\mid \\text{I}) = 0.5$\n",
    "- $P(\\text{studying} \\mid \\text{I am}) = 0.2$\n",
    "- $P(\\text{machine} \\mid \\text{I am studying}) = 0.9$\n",
    "- $P(\\text{learning} \\mid \\text{I am studying machine}) = 0.7$\n",
    "\n",
    "For the example sequence, the perplexity would be:\n",
    "\n",
    "$\\text{Perplexity}(\\text{I am studying machine learning}) = (0.1 \\times 0.5 \\times 0.2 \\times 0.9 \\times 0.7)^{-\\frac{1}{5}} \\approx 2.76$\n",
    "\n",
    "A perplexity of 2.76 indicates that, on average, the model is about **2.76 times more \"surprised\"** than it would be if it predicted the sequence perfectly (indicating the model assigns the highest possible probability of 1.0 to each word in the sequence given the context of the preceding words). **A perplexity of 1.0 indicates that there is no “surprise” at all—the model predicted each word with absolute certainty**. In real-world scenarios, no model achieves perfect prediction, and thus, the perplexity is always greater than 1.0. Lower perplexity values indicate better performance, meaning the model’s predictions are closer to the actual occurrences in the sequence, showing that the model is less surprised by the actual sequence of words.\n",
    "\n",
    "Below is an example of how to evaluate the model perplexity using the evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the model to evaluation mode\n",
    "model.eval() \n",
    "\n",
    "# init perplexity log probabilities\n",
    "total_log_prob = 0\n",
    "\n",
    "# init perplexity number of words\n",
    "num_words = 0\n",
    "\n",
    "# init eval dataloader\n",
    "eval_dataloader = DataLoader(eval_dataset, batch_size=mini_batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "# disable gradient calculation\n",
    "with torch.no_grad():\n",
    "\n",
    "    # iterate over dataloader\n",
    "    for inputs, targets in tqdm(eval_dataloader):\n",
    "    \n",
    "        # push inputs and targets to compute device\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "    \n",
    "        # determine logits\n",
    "        logits, _ = model(inputs)\n",
    "    \n",
    "        # get the shape of logits (B: batch size, T: sequence length, C: number of classes/vocabulary size)\n",
    "        B, T, C = logits.shape\n",
    "    \n",
    "        # reshape logits for loss calculation, flattening the batch and sequence dimensions\n",
    "        logits = logits.view(B * T, C)\n",
    "    \n",
    "        # reshape targets\n",
    "        targets = targets.view(B * T)\n",
    "        \n",
    "        # apply softmax to get probabilities\n",
    "        probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "    \n",
    "        # calculate perplexity\n",
    "        probs = probs.detach().cpu().numpy()\n",
    "        sentence_log_prob = sum(math.log2(prob[t]) for prob, t in zip(probs, targets.reshape(-1)))\n",
    "        \n",
    "        # add to the total log probability and the number of words\n",
    "        total_log_prob += sentence_log_prob\n",
    "        num_words += probs.shape[0]\n",
    "    \n",
    "    # calculate the average log probability\n",
    "    average_log_prob = total_log_prob / num_words\n",
    "\n",
    "# calculate perplexity\n",
    "perplexity = 2 ** (-average_log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally print the model perplexity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'GPT Model Perplexity: {perplexity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Lab Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you successfully accomplished the following key learnings:\n",
    "\n",
    "> 1. **Introduction to GPT Models:** Gained a comprehensive understanding of Generative Pre-trained Transformer (GPT) models, focusing on their architecture and unique ability to capture long-range dependencies in sequential data.\n",
    "> 2. **Hands-on Implementation:** Developed practical skills in implementing a GPT model using the PyTorch library, applying it to the Tiny Shakespeare dataset to learn the intricacies of training deep learning models for natural language processing tasks.\n",
    "> 3. **Text Generation and Evaluation:** Learned how to generate new text based on trained models and evaluated the model's performance, enhancing your capability to work on real-world text generation projects.\n",
    "> 4. **Visualization of Attention Mechanisms:** Acquired expertise in visualizing attention mechanisms within the GPT model, providing deeper insights into how the model processes and generates text, which is crucial for understanding model interpretability.\n",
    "\n",
    "This lab provided insights into the application and implementation of GPT models, equipping you with essential tools and techniques for effective model building, evaluation, and application in natural language processing. These skills are invaluable for building and training your own GPT models."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "wKqWDNc6GElq",
    "buoFpQgxGEls",
    "mWW2BrhdGElw",
    "13Mh4YxqGElw",
    "RDrKDGtDGEl5",
    "WrGkqQg2GEl8",
    "VpQwySOdGEl9",
    "tgZNO4JSGEl-",
    "kBYdYkXuGEl_",
    "AuUZS9g8GEmA",
    "rz6ncsYOGEmB",
    "-xe4oCCtGEmF",
    "aunJFGacGEmJ",
    "vqvcTI8JGEmN",
    "maofm5RpGEmO",
    "ll8xrCaTGEmP",
    "9csMPZotGEmT",
    "ujQ27FlmGEmW",
    "VdaLFlNnGEmZ",
    "LQMSzjOwGEmZ",
    "HL_K5X11GEma",
    "4CzcYb7NGEmc",
    "P7eSD5-kGEmd",
    "8OeIVoZrGEmd",
    "J51-gOLOGEme",
    "pN6uzbvXGEmh",
    "l9MxQgFgGEmi",
    "xuiv6racGEmo",
    "z6oSIROFGEmo",
    "GDPMOAkfGEmp",
    "Mz4OWmUqGEms",
    "qPHoCpBmGEmt",
    "Uk_LC6aIGEmu"
   ],
   "name": "lab_001_notebook.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "343.011px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
