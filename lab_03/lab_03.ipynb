{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 800px\" src=\"banner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGwNwDKEt8lG"
   },
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"hsg_logo.png\">\n",
    "\n",
    "##  Lab 03 - Machine Learning: Naive Bayes Theorem\n",
    "\n",
    "GSERM Summer School 2024, Deep Learning: Fundamentals and Applications, University of St. Gallen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYpS4wEPt8lI"
   },
   "source": [
    "Our lab environment is based on Jupyter Notebooks (https://jupyter.org) and Python (https://www.python.org), allowing us to perform various statistical evaluations and data analyses. In our previous lab, you learned about various elements of Python programming, including conditions, loops, and function implementation, among others. In this third lab, we will construct our first **supervised machine learning classification 'pipelines'** using a classifier known as the **Gaussian Naive-Bayes (GNB)** classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 1000px; height: auto\" src=\"./splash.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Naive-Bayes (NB)** classifier, a *generative* classifier, is a straightforward 'probabilistic classifier'. It applies Bayes' theorem, underpinning its operation with strong (naive) independence assumptions between features. The Naive Bayes method has been extensively researched since the 1950s and continues to be an accessible baseline method for text categorization and other domains. This classification technique belongs to the **generative** type of classifiers, distinguishable from the **discriminative** type, as introduced in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Br5f8mEt8lK"
   },
   "source": [
    "As always, please feel free to ask any questions during the lab, post them in our CANVAS (StudyNet) forum (https://learning.unisg.ch), or send them to us via email (using the course email)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0Jnx-Ljt8lK"
   },
   "source": [
    "## 1. Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ybF-i5mQt8lL"
   },
   "source": [
    "After today's lab, you should be able to:\n",
    "\n",
    "1. Understand how to set up a **notebook or \"pipeline\"** that solves a simple supervised classification task.\n",
    "2. Recognize the **data elements** needed to train and evaluate a supervised machine learning classifier. \n",
    "3. Understand how a generative Gaussian **Naive-Bayes (NB)** classifier can be trained and evaluated.\n",
    "4. Know how to use Python's sklearn library to **train** and **evaluate** arbitrary classifiers.\n",
    "5. Understand how to **evaluate** and **interpret** the classification results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, let's watch a motivational video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "# OpenAI: \"Solving Rubik's Cube with a Robot Hand\"\n",
    "# YouTubeVideo('x4O8pojMF0w', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZaa0qAnt8lY"
   },
   "source": [
    "## 2. Setup of the Jupyter Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2yTCqemyt8la"
   },
   "source": [
    "First, we need to import a couple of Python libraries that allow for data analysis and data visualization. This lab will use the `Pandas`, `Numpy`, `Scikit-Learn`, `Matplotlib` and the `Seaborn` library. Let's import these libraries by executing the statements below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "o3ShseCwt8lb",
    "outputId": "1254c7ff-5876-4508-8fde-5528e4d704f3"
   },
   "outputs": [],
   "source": [
    "# import the numpy, scipy and pandas data science library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# import sklearn data and data pre-processing libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import sklearn naive.bayes classifier library\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# import sklearn classification evaluation library\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "# import matplotlib data visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFnbcu4yt8le"
   },
   "source": [
    "Let's enable inline Jupyter notebook plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uLbxWoZit8lf"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PsFqwDkYt8ln"
   },
   "source": [
    "Let's set the `Seaborn` plotting theme for all subsequent visualizations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dMH7Y9-Ht8lo"
   },
   "outputs": [],
   "source": [
    "sns.set_theme(style='darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMSfpCPvt8l4"
   },
   "source": [
    "## 3. Dataset Download and Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0gpZzk5t8l5"
   },
   "source": [
    "### 3.1 Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cilrWTyMt8l6"
   },
   "source": [
    "The **Iris Dataset** is a classic and straightforward dataset frequently used as a \"Hello World\" example in multi-class classification. This dataset consists of measurements taken from three different types of iris flowers (referred to as **Classes**), namely the `Iris Setosa`, the `Iris Versicolor` and the `Iris Virginica`, and their respective measured petal and sepal length (referred to as **Features**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlF-VYuOt8l7"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 700px; height: auto\" src=\"./iris_dataset.png\">\n",
    "\n",
    "(Source: http://www.lac.inpe.br/~rafael.santos/Docs/R/CAP394/WholeStory-Iris.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZBHv_Rbrt8l8"
   },
   "source": [
    "The dataset, in total, contains **150 samples** (50 samples per class) and their corresponding **4 different measurements** for each sample. The individual measurements include:\n",
    "\n",
    "- `Sepal length (cm)`\n",
    "- `Sepal width (cm)`\n",
    "- `Petal length (cm)`\n",
    "- `Petal width (cm)`\n",
    "\n",
    "Further details about the dataset can be found in the following publication: *Fisher, R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).*\"\n",
    "\n",
    "Let's load the dataset and conduct a preliminary data assessment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CtBrJGut8l9"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AE2PbwClt8mB"
   },
   "source": [
    "Print and inspect the names of the four features contained in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "NzLzNDo8t8mF",
    "outputId": "e336addc-0032-4f19-c65b-83a4482bc4a5"
   },
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UIvnl8Qct8mK"
   },
   "source": [
    "Let's determine and print the feature dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "tq6gZN-1t8mM",
    "outputId": "8c985d93-12bb-4b17-e45d-6f284cedb17a"
   },
   "outputs": [],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DwiIRMR_t8mW"
   },
   "source": [
    "Determine and print the class label dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "tayVqRQOt8mX",
    "outputId": "1ec43974-51bb-4117-e0e9-de84b82676bc"
   },
   "outputs": [],
   "source": [
    "iris.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RoQlbXs_t8md"
   },
   "source": [
    "Print and inspect the names of the three classes contained in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "R__ACqSct8me",
    "outputId": "f257226b-e22b-441c-db4a-50e47c9dad6c"
   },
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MwqoNt8gt8mh"
   },
   "source": [
    "Let's briefly envision how the feature information of the dataset is collected and presented in the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uCgJtdiot8mi"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 900px; height: auto\" src=\"./feature_collection.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rD3SBLxzt8mi"
   },
   "source": [
    "Now, let's examine the first five rows of feature data from the Iris Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "kju1z4Cft8mk",
    "outputId": "cf9f8028-e60b-4acf-dfd1-c6b2aaed1ddd"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(iris.data, columns=iris.feature_names).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P62AsvZ8t8mr"
   },
   "source": [
    "We should also take a look at the first five class labels from the Iris Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "oNjr0a5Dt8ms",
    "outputId": "160bca1e-1408-4904-efec-04a3a8939d97"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(iris.target, columns=[\"class\"]).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fxz--vVdt8mu"
   },
   "source": [
    "Next, we'll conduct a thorough assessment of the data. We'll plot the feature distributions from the Iris dataset, taking into account both the class membership of each datapoint and the pairwise relationships between features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWofkTgQt8mw"
   },
   "source": [
    "To create this plot, we'll use the **Seaborn** library in Python, which produces what's known as a **Pairplot**. Seaborn is a great data visualization library that builds on Matplotlib, offering a powerful interface for generating informative statistical graphics. You can find out more at [https://seaborn.pydata.org](https://seaborn.pydata.org). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "JmfO2-yit8mx",
    "outputId": "6a2392f8-a12e-4360-a5a8-acdf6bc9970d"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ugPoMiQt8m4"
   },
   "source": [
    "Observing the Pairplot, we can see that most feature measurements corresponding to the `setosa` flower class demonstrate **linear separability** when compared to measurements from other flower classes. Meanwhile, the `versicolor` and `virginica` classes show a mixed, or **non-linear separability**, across all feature distributions in the Iris Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gTWFzhhFt8m4"
   },
   "source": [
    "### 3.2 Dataset Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTBwny8Dt8m5"
   },
   "source": [
    "In order to understand and effectively evaluate the performance of any trained **supervised machine learning** model, it's good practice to split the dataset into a **training set** (a portion of the data used solely for training purposes) and an **evaluation set** (a portion of the data used exclusively for evaluation). Please remember that the **evaluation set** is kept separate from the model during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dFU5ijYat8m6"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"./train_eval_dataset.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YN25KKcvt8m6"
   },
   "source": [
    "To conduct an effective model evaluation, we will designate **30%** of the original dataset as evaluation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kPFvlzS6t8m7"
   },
   "outputs": [],
   "source": [
    "eval_fraction = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reproducibility, let's also establish a random seed for our train-test split, ensuring consistent results across different runs of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of 42 is a nod to Douglas Adams' **Hitchhiker's Guide to the Galaxy** (https://en.wikipedia.org/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy), where it is described as the *\"Answer to the Ultimate Question of Life, the Universe, and Everything\"*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FkQME8Ut8m9"
   },
   "source": [
    "We'll now randomly split the dataset into a training set and an evaluation set using sklearn's `train_test_split` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xF7m6KMSt8m9"
   },
   "outputs": [],
   "source": [
    "# 70% training and 30% evaluation\n",
    "x_train, x_eval, y_train, y_eval = train_test_split(iris.data, iris.target, test_size=eval_fraction, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T37IuZHIt8m_"
   },
   "source": [
    "Let's now examine the dimensions of our training set:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "N9i0U2uzt8nA",
    "outputId": "65cba01c-5c0e-4e75-e66d-e92cbdff8e29"
   },
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqJitVsit8nC"
   },
   "source": [
    "And, let's also evaluate the dimensions of our evaluation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "XeVTeCNat8nD",
    "outputId": "b96516ef-10af-4216-abfa-b7a3e4810631"
   },
   "outputs": [],
   "source": [
    "x_eval.shape, y_eval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9HtRmw-t8nJ"
   },
   "source": [
    "## 4. Gaussian Naive-Bayes (NB) Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ns_yibVst8nK"
   },
   "source": [
    "One prevalent (and remarkably straightforward) algorithm is the **Naive Bayes Classifier**. A natural way to address a given classification task involves the probabilistic query: \n",
    "\n",
    "<center>Given all the available information $x$, which is the most probable class $c^{*}$?</center>\n",
    "\n",
    "We aim to produce a conditional probability $P(c|x)$ for each class $c$, given distinct observations of $x$. Once we've computed this conditional probability for each class, we select the class $c^{*}$ corresponding to the highest $P(c|x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q966Q0Hst8nM"
   },
   "source": [
    "$$c^{*} = \\arg \\max_{c} P(c|x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0Q1VuqAkt8nN"
   },
   "source": [
    "That would require us to be prepared to estimate the probability distribution $P(c | x)$ for every possible value of $x = {x1, x2, ..., xn}$. Here, $P(c | x)$ denotes the **conditional probability** read as \"the probability of c given x\". Formally, the conditional probability is defined as \n",
    "\n",
    "$$P(c | \\mathbf{x}) = \\frac{P(c, \\mathbf{x})}{P(\\mathbf{x})},$$\n",
    "\n",
    "where $P(c, x)$ refers to the **joint probability** of c and x occurring simultaneously.\n",
    "\n",
    "**Excursion:** Imagine a document classification system that, depending on the occurrence of a particular set of words in a document, predicts the class of the document. For example, if the words **\"recipe\"**, **\"pumpkin\"**, **\"cuisine\"**, **\"pancakes\"**, etc. appear in the document, the classifier predicts a high probability of the document being a cookbook. \n",
    "\n",
    "Let's assume that the feature $x_{pancake}$ ` = 1` might signify that the word **\"pancakes\"** appears in a given document and $x_{pancake}$ ` = 0` would signify that it does not. If we had **30** such binary **\"word-appearance\" features**, that would mean that we need to be prepared to calculate the probability $P(c | x)$ of any of 2^30 (over 1 billion) possible values of the input vector $x = {x1, x2, ..., x30}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "auckZh8Dt8nN"
   },
   "source": [
    "$$\\mathbf{x^{1}}= \\{x_1=1, x_2=0, x_3=0, x_4=0, x_5=0, x_6=0, ..., x_{29}=0, x_{30}=0\\}$$\n",
    "$$\\mathbf{x^{2}}= \\{x_1=1, x_2=1, x_3=0, x_4=0, x_5=0, x_6=0, ..., x_{29}=0, x_{30}=0\\}$$\n",
    "$$\\mathbf{x^{3}}= \\{x_1=1, x_2=1, x_3=1, x_4=0, x_5=0, x_6=0, ..., x_{29}=0, x_{30}=0\\}$$\n",
    "$$...$$\n",
    "$$...$$\n",
    "$$\\mathbf{x^{2^{30}-1}}= \\{x_1=1, x_2=1, x_3=1, x_4=1, x_5=1, x_6=1, ..., x_{29}=0, x_{30}=1\\}$$\n",
    "$$\\mathbf{x^{2^{30}}}= \\{x_1=1, x_2=1, x_3=1, x_4=1, x_5=1, x_6=1, ..., x_{29}=1, x_{30}=1\\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KyrW7n63t8nN"
   },
   "source": [
    "Moreover, where is the learning? If we need to see every possible example to predict the corresponding label, then we're not learning a pattern but just memorizing the dataset. \n",
    "\n",
    "One solution to this challenge is the so-called **Bayes' theorem** (alternatively Bayes' law or Bayes' rule) you learned about in the lecture. A common scenario for applying the Bayes' theorem formula is when you want to know the probability of something \"unobservable\" (e.g., the class $c$ of a document) given an \"observed\" event (e.g., the distinct words $x$ contained in the document). Such a probability is usually called the **posterior probability**, mathematically denoted by $P(c|x)$.\n",
    "\n",
    "The formula from Bayes' theorem provides an elegant way of calculating such posterior probabilities $P(c|x)$ without the need to observe every single possible configuration of $\\mathbf{x} = \\{x_1, x_2, ..., x_n\\}$. Let's briefly revisit the formula for Bayes' theorem below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J8SqN96Kt8nO"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"./bayes_theorem.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WdhvpINqt8nO"
   },
   "source": [
    "In the formula of the **Bayes' theorem** above,\n",
    "\n",
    ">- $P(c|x)$ denotes the **posterior** probability of class $c$ given a set of features $x$ denoted by $x_1, x_2, ..., x_n$.\n",
    ">- $P(c)$ denotes the **prior** probability of observing class $c$.\n",
    ">- $P(x|c)$ denotes the **likelihood** which is the probability of a feature $x$ given class $c$.\n",
    ">- $P(x)$ denotes the **evidence** which is the general probability of observing feature $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wDxV-LR9t8nP"
   },
   "source": [
    "### 4.1 Calculation of the prior probabilities $P(c)$ of each class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W-1fKmoht8nP"
   },
   "source": [
    "Let's develop an intuition for Bayes' theorem by initially calculating the prior probability, $P(c)$, for each class of iris flower in the dataset. To achieve this, we need to determine the occurrence count for each class in the extracted training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "_LuZLk1zt8nQ",
    "outputId": "10367bfb-1dae-44a2-8840-8547a371765b"
   },
   "outputs": [],
   "source": [
    "# determine counts of unique class labels\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "# concatenate counts and class labels in a python dictionary\n",
    "class_counts = dict(zip(unique, counts))\n",
    "\n",
    "# print obtained dictionary\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "onQSHIPyt8nU"
   },
   "source": [
    "Let's convert the obtained counts into probabilities. Therefore, we divide the class counts by the total number of observations in the extracted training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "godgFvq6t8nV",
    "outputId": "38dae9f6-9eb8-4a8b-e86d-0367c0dc34bd"
   },
   "outputs": [],
   "source": [
    "# divide counts by the number of observations available in the training data\n",
    "prior_probabilities = counts / np.sum(counts)\n",
    "\n",
    "# print obtained probabilites\n",
    "print(prior_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9bACVm7Rt8na"
   },
   "source": [
    "Next, let's plot the obtained prior probabilities $P(c)$ accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "colab_type": "code",
    "id": "Oiq9nyvRt8na",
    "outputId": "def6cf93-677b-4f85-bbb8-c8a8ff90aee1"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations\n",
    "ax.bar(x=np.unique(iris.target), height=prior_probabilities, color='mediumseagreen', alpha=0.7)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$c_{i}$\", fontsize=14, labelpad=10)\n",
    "ax.set_ylabel(\"$P(c_{i})$\", fontsize=14, labelpad=10)\n",
    "\n",
    "# set x-axis ticks\n",
    "ax.set_xticks(np.unique(iris.target))\n",
    "\n",
    "# set y-axis range\n",
    "ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Distribution of the prior class probabilites $P(c)$', fontsize=16, pad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isM3cVjHt8ne"
   },
   "source": [
    "### 4.2 Calculation of the evidence $P(x)$ of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tyCLzVaft8nf"
   },
   "source": [
    "Let's now calculate the general probability of observing a particular observation $ùë•$, which, from a Bayes' theorem perspective, denotes the evidence $P(\\mathbf{x})$ of an observation $x=\\{x_1, x_2, ..., x_n\\}$. We assume that the first feature $x_{1}$ represents the \"sepal length\" observations of the Iris Dataset, the second feature $x_{2}$ represents \"sepal width\", $x_{3}$ represents \"petal length\", and $x_{4}$ represents \"petal width\". To calculate the evidence $P(x)$ of a particular observation, for example, $x=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}$, the Bayes' theorem in general utilizes the following two tricks:\n",
    "\n",
    "**Trick 1: \"Conditional Independence\"** \n",
    "\n",
    "Using the **\"Chain Rule of Probabilities\"**, we can express the evidence term $P( \\mathbf{x} )$ as:\n",
    "\n",
    "$$P( \\mathbf{x}) = P(\\{x_1, x_2, ..., x_n\\}) = P(x_1) \\cdot P(x_2 | x_1) \\cdot P(x_3 | x_1, x_2) \\cdot P(x_4 | x_1, x_2, x_3) \\cdot ... \\cdot P( x_n | x_1, ..., x_{n-1}) = \\prod^n_i P(x_i | x_{1:i-1})$$\n",
    "\n",
    "On its own, this expression does not provide further progress. Even for $d$ binary features, we still need to estimate approximately $2^d$ parameters. However, the naive Bayes theorem trick is to assume that the distinct features $x_1, x_2, \\dots, x_n$ are conditionally independent when observing a specific class $c$. This assumption significantly simplifies the evidence term $P(\\mathbf{x})$, resulting in:\n",
    "\n",
    "$$P( \\mathbf{x}) = P(\\{x_1, x_2, ..., x_n\\}) = P(x_1) \\cdot P(x_2) \\cdot P(x_3) \\cdot P(x_4) \\cdot ... \\cdot P( x_n ) = \\prod^n_i P(x_i)$$\n",
    "\n",
    "Estimating each evidence term $\\prod^n_i P(x_i)$ involves estimating the distribution of each feature $x_i$ independently. Consequently, the assumption of conditional independence reduces the complexity of our model in terms of the number of parameters, moving from an exponentially growing dependence on the number of features to a linear growing dependence. Therefore, it is called the \"naive\" Bayes' theorem because it makes the naive feature independence assumption, allowing us to disregard dependencies among them.\n",
    "\n",
    "**Trick 2: \"Law of Large Numbers\"** \n",
    "\n",
    "During the lecture, you learned that the distribution of evidence can be approximated by a Gaussian (Normal) probability distribution $\\mathcal{N}(\\mu, \\sigma)$. This simplification is justified by the application of the **\"Law of Large Numbers\"** or **\"Central Limit Theorem\"** (you may want to refer to further details of the theorem at:  https://en.wikipedia.org/wiki/Central_limit_theorem). In general, the probability density of a Gaussian \"Normal\" distribution is defined by the formula below and is parameterized by its **mean** $\\mu$ and corresponding **standard deviation** $\\sigma$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o0D2oVwht8ng"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"./evidence_calculation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fZNZwtLmt8ng"
   },
   "source": [
    "Using the **\"Law of Large Numbers,\"** we will approximate the probability density of the evidence, $P(x)$, as $\\approx \\mathcal{N}(x | \\mu, \\sigma)$ for each feature, $x_i$, by using a Gaussian distribution. We need to find a good estimate for the parameters $\\mu$ and $\\sigma$ that define the Gaussian (Normal) probability distribution to achieve this.\n",
    "\n",
    "But how can we achieve this in practice? Let's start by examining the true probability density of the **sepal length** feature (the first feature) in the Iris Dataset. The following line of code calculates a histogram of the actual distribution of **sepal length** values and plots it accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "UxpYEpTMt8ng",
    "outputId": "fe164bf2-3ecc-4c09-eeae-6aadab7bf8f6"
   },
   "outputs": [],
   "source": [
    "# determine a histogram of the \"sepal length\" feature value distribution\n",
    "hist_probabilities, hist_edges = np.histogram(x_train[:, 0], bins=10, range=(0,10), density=True)\n",
    "\n",
    "# print the histogram feature value probabilites\n",
    "print(hist_probabilities)\n",
    "\n",
    "# print the histogram edges\n",
    "print(hist_edges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-yKoA2H7t8nm"
   },
   "source": [
    "Let's also plot the probability density accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "Yuqovydut8nn",
    "outputId": "6e5a3ac8-7432-4477-dcc0-f8e4003fe6ab"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations\n",
    "ax.hist(x_train[:, 0], bins=10, range=(0, 10), density=True, color='mediumseagreen', alpha=0.7)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{1}$\", fontsize=14, labelpad=10)\n",
    "ax.set_ylabel(\"$P(x_{1})$\", fontsize=14, labelpad=10)\n",
    "\n",
    "ax.set_ylim([0.0, 0.5])\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Distribution of the \"Sepal Length\" feature', fontsize=16, pad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fH9Lh02tt8nq"
   },
   "source": [
    "How can we approximate the true probability density of the **sepal length** feature using a Gaussian distribution? All we need to do is calculate its mean $\\mu$ and standard deviation $\\sigma$. Let's start by calculating the mean $\\mu$ of the **sepal length** feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "H9qFq35at8nq",
    "outputId": "284e4aa1-4b4d-4a6f-8b14-095bc9ab9861"
   },
   "outputs": [],
   "source": [
    "# calculate the mean of the sepal length observations\n",
    "mean_sepal_length = np.mean(x_train[:, 0])\n",
    "\n",
    "# print the obtained mean\n",
    "print(mean_sepal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dOAWwXSJt8nu"
   },
   "source": [
    "Let's continue by calculating the standard devition $\\sigma$ of the **sepal length** feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "J4JZh0Vtt8nv",
    "outputId": "05ddc8be-7179-468a-9d11-674db8fa702d"
   },
   "outputs": [],
   "source": [
    "# calculate the standard deviation of the sepal length observations\n",
    "std_sepal_length = np.std(x_train[:, 0])\n",
    "\n",
    "# print the obtained standard deviation\n",
    "print(std_sepal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "votKuOxDt8ny"
   },
   "source": [
    "Now, we can determine the approximate Gaussian (Normal) probability density distribution, $\\mathcal{N}(\\mu, \\sigma)$, for the **sepal length** feature using the values of $\\mu$ and $\\sigma$ obtained above. To accomplish this, we will utilize the `pdf.norm` function available in the `scipy.stats` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "0-BMdABnt8n1",
    "outputId": "8b3543b0-dd49-4408-9308-94d1a40ed695"
   },
   "outputs": [],
   "source": [
    "# calculate the probability density function of the Gaussian distribution\n",
    "hist_gauss_sepal_length = norm.pdf(np.arange(0, 10, 0.1), mean_sepal_length, std_sepal_length)\n",
    "\n",
    "# print obtained probabilities\n",
    "print(hist_gauss_sepal_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hs9C5T06t8n4"
   },
   "source": [
    "Let's proceed to plot the approximate Gaussian (Normal) probability density distribution of the **sepal length** feature, denoted as $P(\\mathbf{x}) \\approx \\mathcal{N}(\\mu, \\sigma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "xai5fFIft8n5",
    "outputId": "432ab54e-c97d-4f77-aabc-0f439d8762ae"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), hist_gauss_sepal_length, color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations\n",
    "ax.hist(x_train[:, 0], bins=10, range=(0, 10), density=True, color='mediumseagreen', alpha=0.7)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_1$\", fontsize=14, labelpad=10)\n",
    "ax.set_ylabel(\"$P(x_{1})$\", fontsize=14, labelpad=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Length\" feature', fontsize=16, pad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "817uNi_8t8n8"
   },
   "source": [
    "We can now proceed to plot the approximate Gaussian (Normal) probability density distribution of the **sepal length** feature, represented as $P(\\mathbf{x}) \\approx \\mathcal{N}(\\mu, \\sigma)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "mO6QUCWSt8n9",
    "outputId": "99fc3c3a-6001-4f3b-faf9-6f1ac8ca31ef"
   },
   "outputs": [],
   "source": [
    "# determine mean and std of the \"sepal width\" feature\n",
    "mean_sepal_width = np.mean(x_train[:, 1])\n",
    "std_sepal_width = np.std(x_train[:, 1])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_sepal_width, std_sepal_width), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal width\" observations\n",
    "ax.hist(x_train[:, 1], bins=10, range=(0, 10), density=True, color='mediumseagreen', alpha=0.7)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_2$\", fontsize=14, labelpad=10)\n",
    "ax.set_ylabel(\"$P(x_{2})$\", fontsize=14, labelpad=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Width\" feature', fontsize=16, pad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x4C79lskt8oB"
   },
   "source": [
    "Approximate the Gaussian (Normal) probability density distribution as $P(\\mathbf{x}) \\approx \\mathcal{N}(\\mu, \\sigma)$ for the **petal length** feature and plot its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "LrfIjMgSt8oB",
    "outputId": "83fbddf0-9737-4e7d-c059-c8c5a494f6f8"
   },
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal length\" feature\n",
    "mean_petal_length = np.mean(x_train[:, 2])\n",
    "std_petal_length = np.std(x_train[:, 2])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_length, std_petal_length), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"petal length\" observations\n",
    "ax.hist(x_train[:, 2], bins=10, range=(0, 10), density=True, color='mediumseagreen', alpha=0.7)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_3$\", fontsize=14, labelpad=10)\n",
    "ax.set_ylabel(\"$P(x_{3})$\", fontsize=14, labelpad=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Length\" feature', fontsize=16, pad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ubh_eyqzt8oG"
   },
   "source": [
    "And finally, approximate the Gaussian (Normal) probability density distribution of the **petal width** feature as $P(\\mathbf{x}) \\approx \\mathcal{N}(\\mu, \\sigma)$, and plot its distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "V-JArHg7t8oI",
    "outputId": "0610eb64-347d-4716-e1bd-c89f4e15061f"
   },
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal width\" feature\n",
    "mean_petal_width = np.mean(x_train[:, 3])\n",
    "std_petal_width = np.std(x_train[:, 3])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_width, std_petal_width), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"petal width\" observations\n",
    "ax.hist(x_train[:, 3], bins=10, range=(0, 10), density=True, color='mediumseagreen', alpha=0.7)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_4$\", fontsize=14, labelpad=10)\n",
    "ax.set_ylabel(\"$P(x_{4})$\", fontsize=14, labelpad=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Width\" feature', fontsize=16, pad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OhxElzxqt8oM"
   },
   "source": [
    "### 4.3 Calculation of the likelihood $P(x|c)$ of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "df0GZ9F4t8oN"
   },
   "source": [
    "Let's now examine how we can calculate the **likelihood** $P(\\mathbf{x}|c)$, which represents the probability density of a feature given a specific class $c$. We will again employ the two techniques we used to calculate the **evidence** $P(x)$ probabilities. To compute the likelihood $P(x|c)$ for a particular observation, such as $x=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5 | c=\"setosa\"\\}$, we will apply the following:\n",
    "\n",
    "**Trick 1: \"Conditional Independence\"** - By employing the **\"Chain Rule of Probabilities\"**, we can express the likelihood term $P(\\mathbf{x} | c)$ as:\n",
    "\n",
    "$$P(\\mathbf{x} | c) = P(\\{x_1, x_2, ..., x_n\\} | c) = P(x_1, c) \\cdot P(x_2 | x_1, c) \\cdot P(x_3 | x_1, x_2, c) \\cdot P(x_4 | x_1, x_2, x_3, c) \\cdot ... \\cdot = \\prod^n_i P(x_i | x_{1:i-1}, c)$$\n",
    "\n",
    "We assume that the distinct features $x_1, x_2, ..., x_n$ are conditionally independent when observing a particular class $c$. Consequently, the likelihood term $P(\\mathbf{x} | c)$ simplifies to:\n",
    "\n",
    "$$P(\\mathbf{x} | c) = P(\\{x_1, x_2, ..., x_n\\} | c) = P(x_1 | c) \\cdot P(x_2 | c) \\cdot P(x_3 | c) \\cdot P(x_4 | c) \\cdot ... \\cdot P(x_n | c) = \\prod^n_i P(x_i | c)$$\n",
    "\n",
    "Estimating each evidence term $\\prod^n_i P(x_i | c)$ involves estimating the distribution of each feature $x_i$ independently.\n",
    "\n",
    "**Trick 2: \"Law of Large Numbers\"** - By utilizing this simplification, we can estimate $P(\\mathbf{x} | c)$ using a Gaussian (Normal) probability distribution $\\mathcal{N}(\\mu, \\sigma)$. The probability density of the Gaussian \"Normal\" distribution, defined by the following formula, is determined by its mean $\\mu$, standard deviation $\\sigma$, and the corresponding class condition $c$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VvDGdWgct8oO"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"./likelihood_calculation.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1jhj7TARt8oP"
   },
   "source": [
    "Using the **\"Law of Large Numbers,\"** we can approximate the probability density function $P(x | c)$ of each feature $x_i$ with a Gaussian distribution $\\mathcal{N}(x | \\mu, \\sigma, c)$. We need to obtain reliable estimates for the parameters $\\mu$ and $\\sigma$ that characterize a Gaussian (Normal) distribution to accomplish this.\n",
    "\n",
    "Now, how can we achieve this in practice? We can begin by applying class conditioning, which involves filtering the dataset for each class $c$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-qOYvwmBt8oQ"
   },
   "outputs": [],
   "source": [
    "# collect all iris setosa measurements, class label = 0\n",
    "x_train_setosa = x_train[y_train == 0]\n",
    "\n",
    "# collect all iris versicolor measurements, class label = 1\n",
    "x_train_versicolor = x_train[y_train == 1]\n",
    "\n",
    "# collect all iris virginica measurements, class label = 2\n",
    "x_train_virginica = x_train[y_train == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PlUNbkBct8ob"
   },
   "source": [
    "Let's begin by inspecting the true probability density of the **sepal length** feature (the first feature) in the iris dataset, given the class **setosa**. The following line of code determines a histogram of the true feature value distribution:\n",
    "\n",
    "Now, let's plot the probability density accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "TBK78B_Ht8oc",
    "outputId": "b8848b78-4485-40bd-e85d-8fa109d684c3"
   },
   "outputs": [],
   "source": [
    "# determine a histogram of the \"sepal length\" feature value distribution given the class \"setosa\"\n",
    "hist_setosa, bin_edges_setosa = np.histogram(x_train_setosa[:, 0], bins=10, range=(0, 10), density=True)\n",
    "\n",
    "# print the histogram feature value probabilites\n",
    "print(hist_setosa)\n",
    "\n",
    "# print the histogram edges\n",
    "print(bin_edges_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "71hyDanft8oj"
   },
   "source": [
    "Now, let's plot the probability density accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "vh3aL0zft8oj",
    "outputId": "2516f82b-4ae0-466c-cf4a-1d19c4c0318d"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 0], bins=10, range=(0, 10), density=True, color='mediumseagreen', alpha=0.7)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{1}$\", fontsize=14, labelpad=10)\n",
    "ax.set_ylabel(\"$P(x_{1}|c=setosa)$\", fontsize=14, labelpad=10)\n",
    "ax.set_ylim([0.0, 1.5])\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Distribution of the \"Sepal Length\" feature given class \"Setosa\"', fontsize=16, pad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXBXHssEt8ol"
   },
   "source": [
    "Next, we can again approximate the Gaussian (Normal) probability density distribution $\\mathcal{N}(\\mu, \\sigma, c)$ of the **sepal length** feature given the class **setosa** using the $\\mu$ and $\\sigma$ values obtained above, along with the `pdf.norm` function from the `scipy.stats` package.\n",
    "\n",
    "Moving on, let's calculate the mean $\\mu$ of the **sepal length** feature for the **setosa** class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "hOw03Lt7t8ol",
    "outputId": "2c6b5073-6554-4adb-ca9d-26464980652f"
   },
   "outputs": [],
   "source": [
    "# calculate the mean of the sepal length observations given class \"setosa\"\n",
    "mean_sepal_length_setosa = np.mean(x_train_setosa[:, 0])\n",
    "\n",
    "# print the obtained mean\n",
    "print(mean_sepal_length_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FEtJIBt7t8on"
   },
   "source": [
    "Similarly, let's calculate the standard deviation $\\sigma$ of the **sepal length** feature for the **setosa** class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "J0IQ6olCt8oq",
    "outputId": "399db51e-32db-43e2-9ee3-3b93aaf201d0"
   },
   "outputs": [],
   "source": [
    "# calculate the standard deviation of the sepal length observations given class \"setosa\"\n",
    "std_sepal_length_setosa = np.std(x_train_setosa[:, 0])\n",
    "\n",
    "# print the obtained standard deviation\n",
    "print(std_sepal_length_setosa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 435
    },
    "colab_type": "code",
    "id": "rN4Vs2I6t8or",
    "outputId": "4144f767-2da3-4d35-cb47-eb1355ec8881"
   },
   "outputs": [],
   "source": [
    "# calculate the probability density function of the Gaussian distribution\n",
    "hist_gauss_sepal_length_setosa = norm.pdf(np.arange(0, 10, 0.1), mean_sepal_length_setosa, std_sepal_length_setosa)\n",
    "\n",
    "# print obtained probabilities\n",
    "print(hist_gauss_sepal_length_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "URVaONMet8ou"
   },
   "source": [
    "Now, let's plot the approximate Gaussian (Normal) probability density distribution $P(\\mathbf{x} | c) \\approx \\mathcal{N}(\\mu, \\sigma, c)$ of the **sepal length** feature for the **setosa** class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "BWk9d7Xit8ow",
    "outputId": "9371b1e1-7928-43b0-8359-bbd89b48f04f"
   },
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), hist_gauss_sepal_length_setosa, color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 0], bins=10, range=(0, 10), density=True, color='mediumseagreen', alpha=0.7)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{1}$\", fontsize=14, labelpad=10)\n",
    "ax.set_ylabel(\"$P(x_{1}|c=setosa)$\", fontsize=14, labelpad=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Length\" feature given class \"Setosa\"', fontsize=16, pad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f8r3rTe2t8oy"
   },
   "source": [
    "Likewise, let's approximate the Gaussian (Normal) probability density distribution $P(\\mathbf{x} | c) \\approx \\mathcal{N}(\\mu, \\sigma, c)$ of the **sepal width** feature for the **setosa** class and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "kGFCoXIDt8oz",
    "outputId": "c50e81c7-f357-4b86-fe87-2b0312b8aa0e"
   },
   "outputs": [],
   "source": [
    "# determine mean and std of the \"sepal width\" feature given class setosa\n",
    "mean_sepal_width_setosa = np.mean(x_train_setosa[:, 1])\n",
    "std_sepal_width_setosa = np.std(x_train_setosa[:, 1])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_sepal_width_setosa, std_sepal_width_setosa), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 1], bins=10, range=(0, 10), density=True, color='mediumseagreen', alpha=0.7)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{2}$\", fontsize=14, labelpad=10)\n",
    "ax.set_ylabel(\"$P(x_{2}|c=setosa)$\", fontsize=14, labelpad=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Sepal Width\" feature given class \"Setosa\"', fontsize=16, pad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7tBsXdGit8o1"
   },
   "source": [
    "Also, approximate the Gaussian (Normal) probability density distribution $P(\\mathbf{x} | c) \\approx \\mathcal{N}(\\mu, \\sigma, c)$ of the **petal length** feature for the **setosa** class and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "c8Jp6pJyt8o1",
    "outputId": "78e470ea-cc41-455c-ed83-d23097c9988d"
   },
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal length\" feature given class setosa\n",
    "mean_petal_length_setosa = np.mean(x_train_setosa[:, 2])\n",
    "std_petal_length_setosa = np.std(x_train_setosa[:, 2])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_length_setosa, std_petal_length_setosa), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 2], bins=10, range=(0, 10), density=True, color='mediumseagreen', alpha=0.7)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{3}$\", fontsize=14, labelpad=10)\n",
    "ax.set_ylabel(\"$P(x_{3}|c=setosa)$\", fontsize=14, labelpad=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Length\" feature given class \"Setosa\"', fontsize=16, pad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oq-1aIsUt8o3"
   },
   "source": [
    "Lastly, approximate the Gaussian (Normal) probability density distribution $P(\\mathbf{x} | c) \\approx \\mathcal{N}(\\mu, \\sigma, c)$ of the **petal width** feature for the **setosa** class and plot its distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "colab_type": "code",
    "id": "WNQBndxEt8o3",
    "outputId": "42040b9c-0953-4eef-d169-dddc6ba9140e"
   },
   "outputs": [],
   "source": [
    "# determine mean and std of the \"petal width\" feature given class setosa\n",
    "mean_petal_width_setosa = np.mean(x_train_setosa[:, 3])\n",
    "std_petal_width_setosa = np.std(x_train_setosa[:, 3])\n",
    "\n",
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot fitted \"gaussian\" or normal distribution\n",
    "ax.plot(np.arange(0, 10, 0.1), norm.pdf(np.arange(0, 10, 0.1), mean_petal_width_setosa, std_petal_width_setosa), color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot histogram of \"sepal length\" observations given the class \"setosa\"\n",
    "ax.hist(x_train_setosa[:, 3], bins=10, range=(0, 10), density=True, color='mediumseagreen', alpha=0.7)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# add axis range and legends\n",
    "ax.set_xlabel(\"$x_{4}$\", fontsize=14, labelpad=10)\n",
    "ax.set_ylabel(\"$P(x_{4}|c=setosa)$\", fontsize=14, labelpad=10)\n",
    "\n",
    "# add plot title\n",
    "ax.set_title('Gaussian Approximation of the \"Petal Width\" feature given class \"Setosa\"', fontsize=16, pad=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's compute the means and standard deviations of the **'versicolor'** class distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean and std of the sepal length feature given class 'versicolor'\n",
    "mean_sepal_length_versicolor = np.mean(x_train_versicolor[:, 0])\n",
    "std_sepal_length_versicolor = np.std(x_train_versicolor[:, 0])\n",
    "\n",
    "# calculate the mean and std of the sepal width feature given class 'versicolor'\n",
    "mean_sepal_width_versicolor = np.mean(x_train_versicolor[:, 1])\n",
    "std_sepal_width_versicolor = np.std(x_train_versicolor[:, 1])\n",
    "\n",
    "# calculate the mean and std of the petal length width feature given class 'versicolor'\n",
    "mean_petal_length_versicolor = np.mean(x_train_versicolor[:, 2])\n",
    "std_petal_length_versicolor = np.std(x_train_versicolor[:, 2])\n",
    "\n",
    "# calculate the mean and std of the petal width feature given class 'versicolor'\n",
    "mean_petal_width_versicolor = np.mean(x_train_versicolor[:, 3])\n",
    "std_petal_width_versicolor = np.std(x_train_versicolor[:, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, compute the means and standard deviations of the **'virginica'** class distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean and std of the sepal length feature given class 'virginica'\n",
    "mean_sepal_length_virginica = np.mean(x_train_virginica[:, 0])\n",
    "std_sepal_length_virginica = np.std(x_train_virginica[:, 0])\n",
    "\n",
    "# calculate the mean and std of the sepal width feature given class 'virginica'\n",
    "mean_sepal_width_virginica = np.mean(x_train_virginica[:, 1])\n",
    "std_sepal_width_virginica = np.std(x_train_virginica[:, 1])\n",
    "\n",
    "# calculate the mean and std of the petal length width feature given class 'virginica'\n",
    "mean_petal_length_virginica = np.mean(x_train_virginica[:, 2])\n",
    "std_petal_length_virginica = np.std(x_train_virginica[:, 2])\n",
    "\n",
    "# calculate the mean and std of the petal width feature given class 'virginica'\n",
    "mean_petal_width_virginica = np.mean(x_train_virginica[:, 3])\n",
    "std_petal_width_virginica = np.std(x_train_virginica[:, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qu7zW41rt8o_"
   },
   "source": [
    "### 4.4 Calculation of the posterior probability $P(c|x)$ of unknown iris flower observations $x^{s}$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LbfZXyolt8o_"
   },
   "source": [
    "Now, we have determined all the distinct elements of Bayes' theorem: $P(c)$, $P(x)$, and $P(x|c)$. We can use these to determine the posterior probability $P(c=setosa|x)$ of a previously unseen observation $x$ belonging to the **setosa** class. Let's apply this to two unseen **iris flower** observations and see if they correspond to the **setosa** class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q7Q3j13Jt8pA"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"./iris_sample_1.png\">\n",
    "\n",
    "(Source: https://de.wikipedia.org/wiki/Schwertlilien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OQVe6u3Nt8pB"
   },
   "source": [
    "The first **iris flower** observation $x^{s1}$ exhibits the following feature values: $x^{s1} = \\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uo6DTk2Et8pC"
   },
   "outputs": [],
   "source": [
    "# init features of first iris flower observation \n",
    "sepal_length = 5.8 \n",
    "sepal_width  = 3.5\n",
    "petal_length = 1.5\n",
    "petal_width  = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FjGgwq6Jt8pE"
   },
   "source": [
    "Let's develop an intuition of the distinct class distributions, including the current observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "FQPyxQNZt8pH",
    "outputId": "bd4855c0-9664-40d5-b7b5-391eaa78e749"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# add observation to the iris dataset\n",
    "iris_plot = pd.concat([iris_plot, pd.DataFrame([[sepal_length, sepal_width, petal_length, petal_width, \"observation 1\"]], columns=iris_plot.columns, index=[150])])\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bikfmTDIt8pM"
   },
   "source": [
    "Next, we'll determine the posterior probability $P(c=setosa|x^{s1})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "VDPPyce5t8pN",
    "outputId": "96fcd2f2-50bb-4abf-8c6e-170610fc956b"
   },
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='setosa')\n",
    "prior = prior_probabilities[0]\n",
    "\n",
    "# determine the likelihood probability P(x|c='setosa')\n",
    "likelihood_setosa = norm.pdf(sepal_length, mean_sepal_length_setosa, std_sepal_length_setosa) * norm.pdf(sepal_width, mean_sepal_width_setosa, std_sepal_width_setosa) * norm.pdf(petal_length, mean_petal_length_setosa, std_petal_length_setosa) * norm.pdf(petal_width, mean_petal_width_setosa, std_petal_width_setosa)\n",
    "\n",
    "# determine the likelihood probability P(x|c='versicolor')\n",
    "likelihood_versicolor = norm.pdf(sepal_length, mean_sepal_length_versicolor, std_sepal_length_versicolor) * norm.pdf(sepal_width, mean_sepal_width_versicolor, std_sepal_width_versicolor) * norm.pdf(petal_length, mean_petal_length_versicolor, std_petal_length_versicolor) * norm.pdf(petal_width, mean_petal_width_versicolor, std_petal_width_versicolor)\n",
    "\n",
    "# determine the likelihood probability P(x|c='virginica')\n",
    "likelihood_virginica = norm.pdf(sepal_length, mean_sepal_length_virginica, std_sepal_length_virginica) * norm.pdf(sepal_width, mean_sepal_width_virginica, std_sepal_width_virginica) * norm.pdf(petal_length, mean_petal_length_virginica, std_petal_length_virginica) * norm.pdf(petal_width, mean_petal_width_virginica, std_petal_width_virginica)\n",
    "\n",
    "# determine the evidence probability P(x)\n",
    "evidence = likelihood_setosa * prior_probabilities[0] + likelihood_versicolor * prior_probabilities[1] + likelihood_virginica * prior_probabilities[2]\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_setosa = (prior * likelihood_setosa) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oovIgl5yt8pR"
   },
   "source": [
    "Based on our observation, the posterior probability $P(c=setosa|x^{s1})$ indicates a high likelihood of belonging to the setosa class. To provide a comparison, let's also calculate the posterior probability $P(c=versicolor|x^{s1})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "GAqY-tFkt8pT",
    "outputId": "cebdc405-1a61-40a2-dad1-9bc3349572a7"
   },
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='versicolor')\n",
    "prior = prior_probabilities[1]\n",
    "\n",
    "# determine the likelihood probability P(x|c='versicolor')\n",
    "likelihood_versicolor = norm.pdf(sepal_length, mean_sepal_length_versicolor, std_sepal_length_versicolor) * norm.pdf(sepal_width, mean_sepal_width_versicolor, std_sepal_width_versicolor) * norm.pdf(petal_length, mean_petal_length_versicolor, std_petal_length_versicolor) * norm.pdf(petal_width, mean_petal_width_versicolor, std_petal_width_versicolor)\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_versicolor = (prior * likelihood_versicolor) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_versicolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hCdb5B8vt8pW"
   },
   "source": [
    "And the posterior probability $P(c=virginica|x^{s1})`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "NkYN1Ax3t8pX",
    "outputId": "c8912584-e289-48e4-9078-a002417fdd7e"
   },
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='virginica')\n",
    "prior = prior_probabilities[2]\n",
    "\n",
    "# determine the likelihood probability P(x|c='virginica')\n",
    "likelihood_virginica = norm.pdf(sepal_length, mean_sepal_length_virginica, std_sepal_length_virginica) * norm.pdf(sepal_width, mean_sepal_width_virginica, std_sepal_width_virginica) * norm.pdf(petal_length, mean_petal_length_virginica, std_petal_length_virginica) * norm.pdf(petal_width, mean_petal_width_virginica, std_petal_width_virginica)\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_virginica = (prior * likelihood_virginica) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YRY5tGXut8pb"
   },
   "source": [
    "Considering the obtained posterior probabilities $P(c|x)$ for the distinct iris flower classes $c = \\{setosa, versicolor, virginica\\}$ given the unknown observation $x^{s1}=\\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}$:\n",
    "\n",
    "$$P(c=setosa|x^{s1}=\\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}) = \\mathbf{0.99}$$\n",
    "$$P(c=versicolor|x^{s1}=\\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}) = \\mathbf{4.69e^{-14}}$$\n",
    "$$P(c=virginica|x^{s1}=\\{x_{1}=5.8, x_{2}=3.5, x_{3}=1.5, x_{4}=0.25\\}) = \\mathbf{2.20e^{-21}}$$\n",
    "\n",
    "we can now apply our initial classification criteria denoted by $\\arg \\max_{c} P(c|x)$ to safely determine the most likely class for the observation $c^{*} = setosa$.\n",
    "\n",
    "Now, let's analyze a second **iris flower** observation and determine its most likely class $c^{*}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3fjYMMiPt8pc"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"./iris_sample_2.png\">\n",
    "\n",
    "\n",
    "(Source: https://de.wikipedia.org/wiki/Schwertlilien)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YKat5TXkt8pc"
   },
   "source": [
    "The second **iris flower** observation $x^{s2}$ exhibits the following feature values: $x^{s2} = \\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SiqXvhkJt8pd"
   },
   "outputs": [],
   "source": [
    "# init a second random feature observation \n",
    "sepal_length = 7.8\n",
    "sepal_width  = 2.3\n",
    "petal_length = 6.4\n",
    "petal_width  = 2.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbrVL1-2t8pj"
   },
   "source": [
    "Let's again develop an intuition of the distinct class distributions, including the current observation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "AyFkHCKKt8pk",
    "outputId": "dafbdd85-bd5e-45c0-9b19-a4cd8aeb4a1e"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset('iris')\n",
    "\n",
    "# add observations to the iris dataset\n",
    "iris_plot = pd.concat([iris_plot, pd.DataFrame([[sepal_length, sepal_width, petal_length, petal_width, 'observation 2']], columns=iris_plot.columns, index=[150])])\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L-S8RLkst8pn"
   },
   "source": [
    "Next, we'll determine the posterior probability $P(c=setosa|x^{s2})$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "1jr_BxeZt8pn",
    "outputId": "78aa08a0-191e-42db-cd68-4d760f01a113"
   },
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='setosa')\n",
    "prior = prior_probabilities[0] \n",
    "\n",
    "# determine the likelihood probability P(x|c='setosa')\n",
    "likelihood_setosa = norm.pdf(sepal_length, mean_sepal_length_setosa, std_sepal_length_setosa) * norm.pdf(sepal_width, mean_sepal_width_setosa, std_sepal_width_setosa) * norm.pdf(petal_length, mean_petal_length_setosa, std_petal_length_setosa) * norm.pdf(petal_width, mean_petal_width_setosa, std_petal_width_setosa)\n",
    "\n",
    "# determine the likelihood probability P(x|c='setosa')\n",
    "likelihood_setosa = norm.pdf(sepal_length, mean_sepal_length_setosa, std_sepal_length_setosa) * norm.pdf(sepal_width, mean_sepal_width_setosa, std_sepal_width_setosa) * norm.pdf(petal_length, mean_petal_length_setosa, std_petal_length_setosa) * norm.pdf(petal_width, mean_petal_width_setosa, std_petal_width_setosa)\n",
    "\n",
    "# determine the likelihood probability P(x|c='versicolor')\n",
    "likelihood_versicolor = norm.pdf(sepal_length, mean_sepal_length_versicolor, std_sepal_length_versicolor) * norm.pdf(sepal_width, mean_sepal_width_versicolor, std_sepal_width_versicolor) * norm.pdf(petal_length, mean_petal_length_versicolor, std_petal_length_versicolor) * norm.pdf(petal_width, mean_petal_width_versicolor, std_petal_width_versicolor)\n",
    "\n",
    "# determine the likelihood probability P(x|c='virginica')\n",
    "likelihood_virginica = norm.pdf(sepal_length, mean_sepal_length_virginica, std_sepal_length_virginica) * norm.pdf(sepal_width, mean_sepal_width_virginica, std_sepal_width_virginica) * norm.pdf(petal_length, mean_petal_length_virginica, std_petal_length_virginica) * norm.pdf(petal_width, mean_petal_width_virginica, std_petal_width_virginica)\n",
    "\n",
    "# determine the evidence probability P(x)\n",
    "evidence = likelihood_setosa * prior_probabilities[0] + likelihood_versicolor * prior_probabilities[1] + likelihood_virginica * prior_probabilities[2]\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_setosa = (prior * likelihood_setosa) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_setosa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wevGXIlYt8pu"
   },
   "source": [
    "Based on our observation, the posterior probability $P(c=setosa|x^{s2})$ is shallow, indicating an improbable association with the setosa class. For comparison, let's calculate the posterior probability $P(c=versicolor|x^{s2})`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "AOcWllFpt8pu",
    "outputId": "d45e5bc1-7319-44e0-b1df-a7a0a793b6a2"
   },
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='versicolor')\n",
    "prior = prior_probabilities[1]\n",
    "\n",
    "# determine the likelihood probability P(x|c='versicolor')\n",
    "likelihood_versicolor = norm.pdf(sepal_length, mean_sepal_length_versicolor, std_sepal_length_versicolor) * norm.pdf(sepal_width, mean_sepal_width_versicolor, std_sepal_width_versicolor) * norm.pdf(petal_length, mean_petal_length_versicolor, std_petal_length_versicolor) * norm.pdf(petal_width, mean_petal_width_versicolor, std_petal_width_versicolor)\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_versicolor = (prior * likelihood_versicolor) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_versicolor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1cPjox7Nt8py"
   },
   "source": [
    "And the posterior probability $P(c=virginica|x^{s2})`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "12-0tBpDt8pz",
    "outputId": "ac1a16b8-ed6c-440c-9cdd-c4b141e3a847"
   },
   "outputs": [],
   "source": [
    "# calculate the distinct elements of the Bayes theorem formula\n",
    "\n",
    "# init the prior probability P(c='virginica')\n",
    "prior = prior_probabilities[2]\n",
    "\n",
    "# determine the likelihood probability P(x|c='virginica')\n",
    "likelihood_virginica = norm.pdf(sepal_length, mean_sepal_length_virginica, std_sepal_length_virginica) * norm.pdf(sepal_width, mean_sepal_width_virginica, std_sepal_width_virginica) * norm.pdf(petal_length, mean_petal_length_virginica, std_petal_length_virginica) * norm.pdf(petal_width, mean_petal_width_virginica, std_petal_width_virginica)\n",
    "\n",
    "# determine the posterior probability\n",
    "posterior_virginica = (prior * likelihood_virginica) / evidence\n",
    "\n",
    "# print the obtained posterior probability\n",
    "print(posterior_virginica)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cI6MpHGAt8p1"
   },
   "source": [
    "Considering the obtained posterior probabilities $P(c|x)$ for the distinct iris flower classes $c = \\{setosa, versicolor, virginica\\}$ given the unknown observation $x^{s2}=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}$:\n",
    "\n",
    "$$P(c=setosa|x^{s2}=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}) = \\mathbf{1.24e^{-268}}$$\n",
    "$$P(c=versicolor|x^{s2}=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}) = \\mathbf{1.12e^{-12}}$$\n",
    "$$P(c=virginica|x^{s2}=\\{x_{1}=7.8, x_{2}=2.3, x_{3}=6.4, x_{4}=2.5\\}) = \\mathbf{0.99}$$\n",
    "\n",
    "we can now apply our initial classification criteria denoted by $\\arg \\max_{c} P(c|x)$ to safely determine the most likely class for the observation $c^{*} = virginica$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vYPrkS8Qt8p1"
   },
   "source": [
    "## 5. Gaussian Naive-Bayes Classification using the Sklearn library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_wdS9rZvt8p1"
   },
   "source": [
    "Luckily, a Python library called **Scikit-Learn** (https://scikit-learn.org) provides various machine learning algorithms, which can be easily accessed using the Python programming language. It includes supervised classification algorithms like the Gaussian Naive-Bayes classifier that we can readily utilize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v-475VY9t8p1"
   },
   "source": [
    "To begin, let's use the \"Scikit-Learn\" library and instantiate the Gaussian Naive-Bayes classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3CKOrUEit8p2"
   },
   "outputs": [],
   "source": [
    "# init the Gaussian Naive Bayes classifier\n",
    "gnb = GaussianNB(priors=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Reference: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "omozNYbKt8p4"
   },
   "source": [
    "We will then train or fit the classifier using the features and labels from the training dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "kPaxZNUzt8p4",
    "outputId": "777808f2-2756-4401-ae2f-587e54bd27f8"
   },
   "outputs": [],
   "source": [
    "# train the Gaussian Naive Bayes classifier\n",
    "gnb.fit(x_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HTPCVjM1t8p7"
   },
   "source": [
    "Once the model is trained, we can employ it to predict the classes of distinct observations within the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8hZtWyDt8p9"
   },
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(x_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZisFYfjt8p_"
   },
   "source": [
    "Let's examine the class labels **predicted** by the Gaussian Naive-Bayes classifier for the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "_aWJ9PGkt8qA",
    "outputId": "3349989a-3ad3-45c2-8b83-00904ba55b9d"
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GldhBv1gt8qD"
   },
   "source": [
    "As well as the **true** class labels present in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "3hUU5UDkt8qD",
    "outputId": "db035ece-27a7-444b-ab41-ab02a2409fb7"
   },
   "outputs": [],
   "source": [
    "y_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SXcfnX7ut8qH"
   },
   "source": [
    "Next, we need to determine the **prediction accuracy** of the trained model on the evaluation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "JUjNPSw4t8qH",
    "outputId": "f9a61e9f-e236-4ec2-eb2e-fc990b198c73"
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: \", metrics.accuracy_score(y_eval, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QAR7qFaht8qJ"
   },
   "source": [
    "Additionally, we should calculate the number of **misclassified** data samples within the evaluation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "7s8UEK8Vt8qJ",
    "outputId": "9d12ebc3-17ba-4ac9-ab8a-5d17ed2c0960"
   },
   "outputs": [],
   "source": [
    "print(\"Number of mislabeled points out of a total {} points: {}\".format(x_eval.shape[0], np.sum(y_eval != y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T_xAgsV6t8qL"
   },
   "source": [
    "In machine learning, particularly within statistical classification, the **confusion matrix** serves as a critical tool for visualizing the performance of a classification algorithm. This matrix is structured as a table, where each row corresponds to the predictions made by the classifier for each class, and each column denotes the instances of the actual class as observed in the data. This layout simplifies the visualization of the classifier's accuracy across different classes and highlights the types of errors made, thereby providing invaluable insights for improving model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sv_p7Z_3t8qL"
   },
   "source": [
    "<img align=\"center\" style=\"max-width: 450px; height: auto\" src=\"./confusion_matrix.png\">\n",
    "\n",
    "(Source: https://en.wikipedia.org/wiki/Confusion_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9jRIduF8t8qM"
   },
   "source": [
    "Let's determine the **confusion** matrix based on the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tjLxhnrOt8qO"
   },
   "outputs": [],
   "source": [
    "# determine the prediction confusion matrix\n",
    "mat = confusion_matrix(y_eval, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wAtUqq_vt8qR"
   },
   "source": [
    "Next, let's visualize the **confusion matrix** of the individual predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "colab_type": "code",
    "id": "K-_WFNpVt8qS",
    "outputId": "50407636-f3b7-4b26-caaf-72698b1efb63"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(5, 5))\n",
    "\n",
    "# plot confusion matrix heatmap\n",
    "sns.heatmap(mat.T, \n",
    "            annot=True, \n",
    "            cbar=False, \n",
    "            cmap='Greens', \n",
    "            linewidths=.5, \n",
    "            annot_kws={\"size\": 16}, \n",
    "            xticklabels=iris.target_names,\n",
    "            yticklabels=iris.target_names)\n",
    "\n",
    "# add plot axis labels\n",
    "plt.xlabel('[true label]', fontsize=16)\n",
    "plt.ylabel('[predicted label]', fontsize=16)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Iris Classes Confusion Matrix', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XZtOsS5ft8qY"
   },
   "source": [
    "Finally, let's utilize the learned model to apply it to our unknown observations, represented as $x^{s1}$, in order to determine their corresponding class predictions, denoted as $c^{*}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "E3kPaqtKt8qY",
    "outputId": "d3343c51-20f2-4871-a108-ab6e646d7c8f"
   },
   "outputs": [],
   "source": [
    "# determine class label prediction of the first unknown observation\n",
    "class_prediction_sample_1 = gnb.predict([[5.8, 3.5, 1.5, 0.25]])\n",
    "\n",
    "# convert predicted class label to class name\n",
    "print(iris.target_names[class_prediction_sample_1[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's likewise obtain the prediction for our second unknown example, represented as $x^{s2}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "RI0efIyst8qb",
    "outputId": "2e08aa8e-9868-4831-f494-d129c65d4dc4"
   },
   "outputs": [],
   "source": [
    "# determine class label prediction of the second unknown observation\n",
    "class_prediction_sample_2 = gnb.predict([[7.8, 2.3, 6.4, 2.50]])\n",
    "\n",
    "# convert predicted class label to class name\n",
    "print(iris.target_names[class_prediction_sample_2[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! For both unknown examples $x^{s1}$ and $x^{s2}$ we obtained exactly the same class predictions as in section 4. of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n94u0rxat8su"
   },
   "source": [
    "## 6. Lab Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DCOEZj-it8sv"
   },
   "source": [
    "In this lab, you successfully accomplished the following key learnings:\n",
    "\n",
    "> 1. **Understanding Gaussian Naive Bayes:** Mastered the fundamental concepts and mathematical underpinnings of the Gaussian Naive Bayes classifier, enhancing your comprehension of generative models in machine learning.\n",
    "> 2. **Model Training and Prediction:** Developed practical skills in training a Gaussian Naive Bayes model using Scikit-learn and applying it to predict class labels, which is fundamental for real-world machine learning applications.\n",
    "> 3. **Evaluating Classifier Performance:** Gained expertise in assessing the performance of classification models using metrics such as accuracy and through the construction and analysis of confusion matrices.\n",
    "\n",
    "This lab provided insights into Gaussian Naive Bayes classification, equipping you with essential tools and techniques for effective model building, evaluation, and application. These skills are invaluable for navigating and succeeding in complex data-driven tasks in machine learning."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eGwNwDKEt8lG",
    "D0Jnx-Ljt8lK",
    "CZaa0qAnt8lY",
    "mMSfpCPvt8l4",
    "n94u0rxat8su"
   ],
   "name": "lab_03.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "317px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
