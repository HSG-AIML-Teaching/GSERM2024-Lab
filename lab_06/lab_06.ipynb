{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 1000px\" src=\"banner.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lab 06 - Autoencoder Neural Networks (AENNs)\n",
    "\n",
    "GSERM Summer School 2024, Deep Learning: Fundamentals and Applications, University of St. Gallen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lab environment is based on Jupyter Notebooks (https://jupyter.org), which provide an interactive platform for performing a variety of statistical evaluations and data analyses. In this lab, we will learn how to enhance vanilla **Artificial Neural Networks (ANNs)** using `PyTorch` to detect anomalies in financial accounting records. To achieve this, we will explore a special type of deep neural network known as **Autoencoder Neural Networks (AENNs)**. AENNs leverage the ability to learn efficient data representations, allowing them to identify unusual patterns that may indicate anomalies.\n",
    "\n",
    "The history of AENNs is rich and exhibits pivotal contributions from researchers like *Geoffrey Hinton* and *Ruslan Salakhutdinov*, who developed foundational techniques for deep learning and representation learning. AENNs have since become a cornerstone in the field of unsupervised learning, significantly advancing the capabilities of anomaly detection and data compression.\n",
    "\n",
    "In this lab, we will apply AENNs to detect anomalies in financial accounting records. AENNs learn to **encode** input data into a low-dimensional representation and **decode** the original data from this encoded representation. The decoded data, or **reconstruction**, should closely match the original **input data**, so entries that can only be reconstructed with errors likely exhibit unusual characteristics. \n",
    "\n",
    "The figure below illustrates a high-level view of the deep learning process we aim to establish in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px\" src=\"./process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `PyTorch` library, we will implement and train the AENN to learn the features of historical **bookings** or **journal entries**. After training, we will apply the model to detect anomalies based on reconstruction errors and use the learned **representations** to interpret the results meaningfully. The figure below provides an overview of the deep learning process and the AENN network architecture we will implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As always, pls. don't hesitate to ask all your questions either during the lab, post them in our CANVAS (StudyNet) forum (https://learning.unisg.ch), or send us an email (using the course email)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After today's lab, you should be able to:\n",
    "\n",
    "> 1. **Understand Autoencoder Neural Network (AENN) Design:** Learn the fundamental concepts and architectural design of AENNs.\n",
    "> 2. **Implement and Train an AENN Model:** Gain hands-on experience with PyTorch to implement, train, and evaluate AENN models.\n",
    "> 3. **Apply AENN Models for Anomaly Detection:** Use AENNs to detect anomalies in synthetic financial datasets, specifically in accounting journal entries.\n",
    "> 4. **Evaluate and Interpret Model Performance:** Evaluate the AENN model's performance using reconstruction error metrics and interpret the results.\n",
    "> 5. **Visualize and Interpret Learned Representations:** Visualize and interpret the learned representations to gain deeper insights into the data's semantics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup of the Jupyter Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous labs, we need to import several Python libraries that facilitate data analysis and visualization. We will primarily use `PyTorch`, `NumPy`, `Scikit-learn`, `Matplotlib`, `Seaborn`, and a few utility libraries throughout this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python data science and utility libraries\n",
    "import os, sys, itertools, urllib, io, warnings\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import pandas_datareader as dr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `Python` machine learning and deep learning libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import `Matplotlib` and `Seaborn` data visualization libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.rcParams['figure.dpi']= 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn off possible warnings, e.g., due to future changes in the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the warning filter flag to ignore warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable inline plotting with `Matplotlib`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create subdirectories within the current working directory for (1) saving the original data, (2) the analysis results, and (3) the trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the data sub-directory\n",
    "data_directory = './01_data'\n",
    "if not os.path.exists(data_directory): os.makedirs(data_directory)\n",
    "    \n",
    "# create the results sub-directory\n",
    "results_directory = './02_results'\n",
    "if not os.path.exists(results_directory): os.makedirs(results_directory)\n",
    "\n",
    "# create the models sub-directory\n",
    "models_directory = './03_models'\n",
    "if not os.path.exists(models_directory): os.makedirs(models_directory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set a random `seed` value to obtain reproducible results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value); # set pytorch seed cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Colab provides free GPUs for running notebooks. However, if you execute this notebook as is, it will use your device's CPU. To run the lab on a GPU, go to `Runtime` > `Change runtime type` and set the Runtime type to `GPU` in the drop-down menu. Running this lab on a CPU is fine, but you will find that GPU computing is faster. *CUDA* indicates that the lab is being run on a GPU.\n",
    "\n",
    "Enable GPU computing by setting the device flag and initializing a CUDA seed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cpu or gpu enabled device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu').type\n",
    "\n",
    "# init deterministic GPU seed\n",
    "torch.mps.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed(seed_value)\n",
    "\n",
    "# log type of device enabled\n",
    "print('[LOG] notebook with {} computation enabled'.format(str(device)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine if we have access to a GPU provided by e.g. `Google's Colab` environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine the available `Python` and `PyTorch` versions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current Python version\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] The Python version: {}'.format(now, sys.version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print current PyTorch version\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] The PyTorch version: {}'.format(now, torch.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Acquisition and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays, companies are accelerating the digitization of business processes, which also impacts **Enterprise Resource Planning (ERP)** systems. These systems gather large amounts of granular data, particularly an organization's journal entries, recorded within the general ledger and subsidiary ledgers.\n",
    "\n",
    "**Figure 1** presents a hierarchical view of an ERP system that captures journal entries in database tables. In the context of audit reviews, the data captured in these systems can contain valuable clues pointing to potential fraudulent actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 600px; height: auto\" src=\"./accounting.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 1:** Hierarchical view of an Enterprise Resource Planning (ERP) system capturing business transactions at various abstraction levels in database tables, i.e., at the level of (1) business processes, (2) accounting, and (3) databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we will descriptively analyze the dataset used in the lab. Then, we will preprocess the data to create a baseline for training a neural network. The lab dataset is based on a modified subset of the **\"Synthetic Financial Dataset For Fraud Detection\"** by Lopez-Rojas. The original dataset was published on Kaggle for data science competitions and can be accessed [here](https://www.kaggle.com/ntnu-testimon/paysim1).\n",
    "\n",
    "First, we will load the dataset into our analysis environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset into the notebook\n",
    "url = 'https://raw.githubusercontent.com/HSG-AIML-Teaching/GSERM2024-Lab/main/lab_06/01_data/fraud_dataset.csv'\n",
    "ori_dataset = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will verify the dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the datasets dimensionalities\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] transactional dataset of {} rows and {} columns retreived.'.format(now, ori_dataset.shape[0], ori_dataset.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we will save a backup copy of the loaded dataset with the current timestamp:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine current timestamp \n",
    "timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# define dataset filename \n",
    "filename = timestamp + \" - original_fraud_dataset.xlsx\"\n",
    "\n",
    "# save dataset extract to the data directory\n",
    "ori_dataset.head(100).to_excel(os.path.join(data_directory, filename)) # just saving the first 100 rows as a sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Initial Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains **categorical** and **numerical attributes** corresponding to the tables BKPF (accounting document headers) and BSEG (accounting document segments) within an SAP FICO module. The list below provides an overview of the individual attributes, along with a brief description of their respective semantics:\n",
    "\n",
    ">- `BELNR`: the accounting document number\n",
    ">- `BUKRS`: the company code\n",
    ">- `BSCHL`: the posting key\n",
    ">- `HKONT`: the posted general ledger account\n",
    ">- `PRCTR`: the posted profit center\n",
    ">- `WAERS`: the currency key\n",
    ">- `KTOSL`: the key of the general ledger account\n",
    ">- `DMBTR`: the amount in the local currency\n",
    ">- `WRBTR`: the amount in the document currency\n",
    "\n",
    "Let's also examine the first 10 rows of the dataset in detail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect top rows of dataset\n",
    "ori_dataset.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While reviewing the data, you might have noticed the attribute labelled `Label`. This attribute contains the ground truth information for each individual journal entry. It describes the 'true nature' of each transaction, i.e., whether it is a **regular** transaction (marked as `regular`) or an **anomaly** (marked as `global` and `local`).\n",
    "\n",
    "In our approach, we will use the label information solely to validate the results of our trained models. However, please note that such a field often isn't available in real-life scenarios. Now, let's examine the distribution of regular transactions versus anomalous transactions in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of anomalies vs. regular transactions\n",
    "ori_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis reveals that similar to the real world, we are dealing with an **unbalanced** dataset. The dataset contains only a tiny proportion of **100 (0.109%)** anomalous transactions. Among the 100 anomalies are **70 (0.076%)** `global` anomalies and **30 (0.033%)** `local` anomalies.\n",
    "\n",
    "In the next step, we will remove the label attribute from the training dataset and store it in a separate variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the \"ground-truth\" label information for the following steps of the class\n",
    "label = ori_dataset.pop('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Preprocessing of Categorical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon examining the data, it's evident that the majority of attributes have categorical (discrete) values, such as the journal entry date, the main ledger account, the journal entry type, and the currency. Let's take a closer look at the distribution of the categorical attributes *posting key* `BSCHL` and *general ledger account* `HKONT`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to plot posting key and general ledger account side by side\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_figwidth(18)\n",
    "\n",
    "# Plot the distribution of the posting key attribute\n",
    "sns.countplot(x=ori_dataset['BSCHL'], ax=ax[0], color='lightgreen')\n",
    "\n",
    "# Set axis labels and title\n",
    "ax[0].set_xticklabels(ax[0].get_xticklabels(), rotation=90, fontsize=12)\n",
    "ax[0].set_xlabel('Attribute Value', fontsize=16)\n",
    "ax[0].set_ylabel('Attribute Value Count', fontsize=16)\n",
    "ax[0].set_title('Posting Key - Attribute Value Distribution', fontsize=18)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[0].tick_params(axis='both', which='minor', labelsize=12)\n",
    "\n",
    "# Plot the distribution of the general ledger attribute\n",
    "sns.countplot(x=ori_dataset['HKONT'], ax=ax[1], color='darkgreen')\n",
    "\n",
    "# Set axis labels and title\n",
    "ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90, fontsize=12)\n",
    "ax[1].set_xlabel('Attribute Value', fontsize=16)\n",
    "ax[1].set_ylabel('Attribute Value Count', fontsize=16)\n",
    "ax[1].set_title('General Ledger - Attribute Value Distribution', fontsize=18)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[1].tick_params(axis='both', which='minor', labelsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, neural networks are designed to process numerical data. One way to meet this requirement is to apply a method known as **One-Hot Encoding**. This method allows a numerical representation of categorical attribute values. With **One-Hot Encoding**, an additional binary column is created in the data for each categorical attribute value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the example in **Figure 2** below. The categorical attribute **'Receiver'** in the original data contains the names **'Sally'**, **'John'**, and **'Emma'**. We encode the attribute as a 'one-hot' attribute by creating an additional binary column for each categorical value in the \"Receiver\" column. For example, we encode each transaction with the value **'Sally'** in the **'Receiver'** column with the value 1.0 in the 'Sally' column. If a transaction has a different value in the **'Receiver'** column, we encode the **'Sally'** column with the value 0.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 500px; height: auto\" src=\"./encoding.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 2:** Example of **One-Hot Encoding** of different \"Receiver\" attribute values into specific binary one-hot columns. Each observed attribute value in the dataset results in its own column. The column value **1.0** indicates the occurrence of the attribute value in the corresponding journal entry, while the column value **0.0** indicates that the attribute value does not occur in the corresponding journal entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method, the six categorical attributes of the dataset can be converted into numerical attributes. The `Pandas` library offers the appropriate functionality, which we will apply as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select categorical attributes to be \"one-hot\" encoded\n",
    "categorical_attr_names = ['BUKRS', 'KTOSL', 'PRCTR', 'BSCHL', 'HKONT', 'WAERS']\n",
    "\n",
    "# encode categorical attributes into a binary one-hot encoded representation \n",
    "ori_dataset_cat_processed = pd.get_dummies(ori_dataset[categorical_attr_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afterward, we will verify the **One-Hot Encoding** using the first 10 journal entries of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect encoded sample transactions\n",
    "ori_dataset_cat_processed.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Preprocessing of Numerical Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will analyze the distributions of the two numerical attributes of the dataset: (1) *Amount in local currency* `DMBTR` and (2) *Amount in document currency* `WRBTR`. The values of both amount attributes exhibit a **skewed** and **steep distribution**. Therefore, we will first scale the values logarithmically. Subsequently, we will min-max normalize the scaled values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the 'DMBTR' and 'WRBTR' attribute\n",
    "numeric_attr_names = ['DMBTR', 'WRBTR']\n",
    "\n",
    "# add a small epsilon to eliminate zero values from data for log scaling\n",
    "numeric_attr = ori_dataset[numeric_attr_names] + 1e-7\n",
    "\n",
    "# log scale the 'DMBTR' and 'WRBTR' attribute values\n",
    "numeric_attr = numeric_attr.apply(np.log)\n",
    "\n",
    "# normalize all numeric attributes to the range [0,1]\n",
    "ori_dataset_num_processed = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will visualize the distributions of the scaled and normalized values for both amount attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init subplots\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "fig.set_figwidth(18)\n",
    "\n",
    "# plot distribution of the local amount attribute\n",
    "sns.histplot(ori_dataset_num_processed['DMBTR'], kde=True, ax=ax[0], color='lightgreen')\n",
    "\n",
    "# set axis labels and title\n",
    "ax[0].set_xlabel('Attribute Value', fontsize=16)\n",
    "ax[0].set_ylabel('Attribute Value Count', fontsize=16)\n",
    "ax[0].set_title('Amount in Local Currency - Attribute Value Distribution', fontsize=16)\n",
    "ax[0].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[0].tick_params(axis='both', which='minor', labelsize=12)\n",
    "\n",
    "# plot distribution of the document amount attribute\n",
    "sns.histplot(ori_dataset_num_processed['WRBTR'], kde=True, ax=ax[1], color='darkgreen')\n",
    "\n",
    "# set axis labels and title\n",
    "ax[1].set_xlabel('Attribute Value', fontsize=16)\n",
    "ax[1].set_ylabel('Attribute Value Count', fontsize=16)\n",
    "ax[1].set_title('Amount in Document Currency - Attribute Value Distribution', fontsize=16)\n",
    "ax[1].tick_params(axis='both', which='major', labelsize=14)\n",
    "ax[1].tick_params(axis='both', which='minor', labelsize=12)\n",
    "\n",
    "# adjust layout\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Merging Categorical and Numerical Transaction Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we merge the preprocessed numerical and categorical attributes into a **single dataset**. The merged dataset will serve as the foundation for the subsequent training of the Autoencoder Neural Network (AENN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge categorical and numeric subsets\n",
    "ori_subset_transformed = pd.concat([ori_dataset_cat_processed, ori_dataset_num_processed], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's take a final look at the dimensionality of the merged dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect final dimensions of pre-processed transactional data\n",
    "ori_subset_transformed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the preprocessing steps, we have a dataset consisting of a total of **91,147 records (rows)** and **618 attributes (columns)**. We should keep the number of columns in mind, as it will determine the dimensionality of the input and output layers of our AENN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Autoencoder Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will familiarize ourselves with the underlying concept and structure of a deep **Autoencoder Neural Network (AENN)**. We will implement the individual components and specific network architecture of AENNs using the `PyTorch` open-source library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Autoencoder Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Autoencoder Neural Networks*, also referred to as *Replicator Neural Networks*, are an unsupervised learning variant of classic feed-forward networks. This particular architecture was originally developed by Geoffrey Hinton and Ruslan Salakhutdinov. AENNs typically consist of a **symmetrical** network architecture and a central hidden layer, referred to as the **latent** or **bottleneck** layer. This layer has a lower dimensionality than the input and output layers of the network. The learning objective of the AENN is to reconstruct the original input data as accurately as possible at the output layer. **Figure 3** shows a schematic representation of an Autoencoder Neural Network:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 800px; height: auto\" src=\"./autoencoder.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Figure 3:** Schematic representation of an **Autoencoder Neural Network**, which consists of two non-linear mappings or feed-forward networks. The two interconnected networks are referred to as the **Encoder** $f_\\theta: \\mathbb{R}^{dx} \\mapsto \\mathbb{R}^{dz}$ and **Decoder** $g_\\theta: \\mathbb{R}^{dz} \\mapsto \\mathbb{R}^{dx}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, AENNs can be interpreted as 'lossy' **compression algorithms**. They are 'lossy' in the sense that the reconstructed outputs may have errors compared to the original input data. The goal of the network is to minimize these reconstruction errors, thereby learning a compressed representation of the input data. In the context of anomaly detection, AENNs can identify patterns in the data and recognize deviations from these patterns, which may indicate anomalies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AENNs consist of three main components:\n",
    "\n",
    "> 1. an Encoder $f_\\theta$,\n",
    "> 2. a Decoder $g_\\theta$,\n",
    "> 3. an error function $\\mathcal{L_{\\theta}}$.\n",
    "\n",
    "The encoder and decoder each consist of a classic feedforward network with learnable parameters $\\theta$. The **encoder** $f_\\theta(\\cdot)$ maps an input vector (e.g., a journal entry) $x^i$ onto a compressed (i.e., low-dimensional) representation $z^i$ in the so-called latent space $Z$. The low-dimensional representation $z^i$ is then mapped by the **decoder** $g_\\theta(\\cdot)$ onto an output vector $\\hat{x}^i$ in the original input space (e.g., the reconstructed journal entry). Formally, the two networks can also be represented as:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>$f_\\theta(x^i) = s(Wx^i + b)$ &emsp; $f_\\theta: \\mathbb{R}^{dx} \\mapsto \\mathbb{R}^{dz}$,</center>\n",
    "<center>$g_\\theta(z^i) = s′(W′z^i + d)$ &emsp; $g_\\theta: \\mathbb{R}^{dz} \\mapsto \\mathbb{R}^{dx}$,</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the two functions have the learnable model parameters $\\theta = \\{W, b, W', d\\}$. The parameters $W \\in \\mathbb{R}^{d_x \\times d_z}$ and $W' \\in \\mathbb{R}^{d_z \\times d_y}$ denote the weight matrices, while $b \\in \\mathbb{R}^{d_x}$ and $d \\in \\mathbb{R}^{d_z}$ represent the bias vectors of the networks. The symbols $s$ and $s′$ denote the respective non-linear activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Autoencoder Neural Network Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step, we will implement the Encoder Network in `PyTorch`. The encoder should consist of a total of **nine layers** of fully-connected neurons, with the following number of neurons per layer: 618-256-128-64-32-16-8-4-3. This means the first layer comprises 618 neurons (determined by the dimensionality of the input data), the second layer 256 neurons, and the other layers 128, 64, 32, 16, 8, 4, and 3 neurons, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"middle\" style=\"max-width: 900px; height: auto\" src=\"./neurons.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following three elements of the encoder implementation deserve special attention:\n",
    "\n",
    ">- `self.encoder_Lx`: defines the linear transformation of the respective layer applied to the input: $Wx + b$.\n",
    ">- `nn.init.xavier_uniform`: initializes weight parameters based on a uniform Xavier distribution.\n",
    ">- `self.encoder_Rx`: defines the non-linear transformation of the respective layer applied to the input: $\\\\sigma(\\\\cdot)$.\n",
    "\n",
    "We use **Leaky ReLUs** to avoid saturating neurons and to accelerate training convergence. The application of Leaky ReLUs allows the calculation of gradients even within the negative range of an activation function (see figure above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we instantiate a model of the encoder network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the encoder network\n",
    "class encoder(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(encoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 618, out 512\n",
    "        self.encoder_L1 = nn.Linear(in_features=ori_subset_transformed.shape[1], out_features=512, bias=True, device=device) # add linearity \n",
    "        nn.init.xavier_uniform_(self.encoder_L1.weight) # init weights\n",
    "        self.encoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity \n",
    "\n",
    "        # specify layer 2 - in 512, out 256\n",
    "        self.encoder_L2 = nn.Linear(512, 256, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.encoder_L2.weight)\n",
    "        self.encoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 3 - in 256, out 128\n",
    "        self.encoder_L3 = nn.Linear(256, 128, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.encoder_L3.weight)\n",
    "        self.encoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 4 - in 128, out 64\n",
    "        self.encoder_L4 = nn.Linear(128, 64, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.encoder_L4.weight)\n",
    "        self.encoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 5 - in 64, out 32\n",
    "        self.encoder_L5 = nn.Linear(64, 32, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.encoder_L5.weight)\n",
    "        self.encoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 6 - in 32, out 16\n",
    "        self.encoder_L6 = nn.Linear(32, 16, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.encoder_L6.weight)\n",
    "        self.encoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 7 - in 16, out 8\n",
    "        self.encoder_L7 = nn.Linear(16, 8, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.encoder_L7.weight)\n",
    "        self.encoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 8 - in 8, out 4\n",
    "        self.encoder_L8 = nn.Linear(8, 4, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.encoder_L8.weight)\n",
    "        self.encoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 9 - in 4, out 3\n",
    "        self.encoder_L9 = nn.Linear(4, 3, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.encoder_L9.weight)\n",
    "        self.encoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "    # define forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.encoder_R1(self.encoder_L1(x))\n",
    "        x = self.encoder_R2(self.encoder_L2(x))\n",
    "        x = self.encoder_R3(self.encoder_L3(x))\n",
    "        x = self.encoder_R4(self.encoder_L4(x))\n",
    "        x = self.encoder_R5(self.encoder_L5(x))\n",
    "        x = self.encoder_R6(self.encoder_L6(x))\n",
    "        x = self.encoder_R7(self.encoder_L7(x))\n",
    "        x = self.encoder_R8(self.encoder_L8(x))\n",
    "        x = self.encoder_R9(self.encoder_L9(x))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will instantiate a model of the Encoder network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intstantiate the encoder network model\n",
    "encoder_train = encoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will transfer the Encoder model to the `CPU` or an available `GPU`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push model to compute device\n",
    "encoder_train = encoder_train.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If available, we will check whether the model has been successfully transferred to the `GPU`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can visualize the model structure and review the network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the initialized architectures\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] encoder architecture:\\n\\n{}\\n'.format(now, encoder_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will implement the Decoder Network in `PyTorch`. The decoder should consist of a total of **nine layers** of fully-connected neurons, with the following number of neurons per layer: 3-4-8-16-32-64-128-256-618. This means the first layer comprises 3 neurons (determined by the dimensionality of the latent space), the second layer 4 neurons, and the other layers 8, 16, 32, 64, 128, 256, and 618 neurons, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementation of the decoder network\n",
    "class decoder(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        # call super class constructor\n",
    "        super(decoder, self).__init__()\n",
    "\n",
    "        # specify layer 1 - in 3, out 4\n",
    "        self.decoder_L1 = nn.Linear(in_features=3, out_features=4, bias=True) # add linearity \n",
    "        nn.init.xavier_uniform_(self.decoder_L1.weight)  # init weights\n",
    "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity\n",
    "\n",
    "        # specify layer 2 - in 4, out 8\n",
    "        self.decoder_L2 = nn.Linear(4, 8, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.decoder_L2.weight)\n",
    "        self.decoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 3 - in 8, out 16\n",
    "        self.decoder_L3 = nn.Linear(8, 16, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.decoder_L3.weight)\n",
    "        self.decoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 4 - in 16, out 32\n",
    "        self.decoder_L4 = nn.Linear(16, 32, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.decoder_L4.weight)\n",
    "        self.decoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 5 - in 32, out 64\n",
    "        self.decoder_L5 = nn.Linear(32, 64, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.decoder_L5.weight)\n",
    "        self.decoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 6 - in 64, out 128\n",
    "        self.decoder_L6 = nn.Linear(64, 128, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.decoder_L6.weight)\n",
    "        self.decoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "        \n",
    "        # specify layer 7 - in 128, out 256\n",
    "        self.decoder_L7 = nn.Linear(128, 256, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.decoder_L7.weight)\n",
    "        self.decoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 8 - in 256, out 512\n",
    "        self.decoder_L8 = nn.Linear(256, 512, bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.decoder_L8.weight)\n",
    "        self.decoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "        # specify layer 9 - in 512, out 618\n",
    "        self.decoder_L9 = nn.Linear(in_features=512, out_features=ori_subset_transformed.shape[1], bias=True, device=device)\n",
    "        nn.init.xavier_uniform_(self.decoder_L9.weight)\n",
    "        self.decoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
    "\n",
    "    # define forward pass\n",
    "    def forward(self, x):\n",
    "\n",
    "        # define forward pass through the network\n",
    "        x = self.decoder_R1(self.decoder_L1(x))\n",
    "        x = self.decoder_R2(self.decoder_L2(x))\n",
    "        x = self.decoder_R3(self.decoder_L3(x))\n",
    "        x = self.decoder_R4(self.decoder_L4(x))\n",
    "        x = self.decoder_R5(self.decoder_L5(x))\n",
    "        x = self.decoder_R6(self.decoder_L6(x))\n",
    "        x = self.decoder_R7(self.decoder_L7(x))\n",
    "        x = self.decoder_R8(self.decoder_L8(x))\n",
    "        x = self.decoder_R9(self.decoder_L9(x)) # don't apply dropout to the AE output\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will instantiate the Decoder model for `CPU` or `GPU` training and ensure that the model has been successfully initialized. To do this, we will visualize the network architecture by executing the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# intstantiate the decoder network model\n",
    "decoder_train = decoder()\n",
    "    \n",
    "# print the initialized architectures\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] decoder architecture:\\n\\n{}\\n'.format(now, decoder_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will transfer the Decoder model to the `CPU` or an available `GPU`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push model to compute device\n",
    "decoder_train = decoder_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the number of encoder model parameters\n",
    "encoder_num_params = 0\n",
    "\n",
    "# iterate over the distinct encoder parameters\n",
    "for param in encoder_train.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    encoder_num_params += param.numel()\n",
    "\n",
    "# init the number of decoder model parameters\n",
    "decoder_num_params = 0\n",
    "    \n",
    "# iterate over the distinct decoder parameters\n",
    "for param in decoder_train.parameters():\n",
    "\n",
    "    # collect number of parameters\n",
    "    decoder_num_params += param.numel()\n",
    "    \n",
    "# print the number of model paramters\n",
    "now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "print('[LOG {}] number of to be trained AENN model parameters: {}.'.format(now, encoder_num_params + decoder_num_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our AENN model comprises a considerable total of **985,021** parameters to be trained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Autoencoder Neural Network Parameter Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully instantiating the AENN model, we now want to train it. Before starting the training, we need to define a suitable error function. As a reminder, our goal is to train the model so that it learns a set of encoder and decoder parameters $θ$ that maximizes the similarity between a given journal entry $x^{i}$ and its reconstruction $\\hat{x}^{i} = g_θ(f_θ(x^{i}))$.\n",
    "\n",
    "Formally, our training objective is to learn parameters $θ^*$ such that $\\arg\\min_{\\theta} |X - g_\\theta(f_\\theta(X))|$. To achieve this optimization goal, we must continuously minimize the error function or **reconstruction error** $\\mathcal{L_{\\theta}}$ as training progresses. A suitable error function for this purpose is the **Binary Cross-Entropy (BCE)** reconstruction error, which is formally defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $\\mathcal{L^{BCE}_{\\theta}}(x^{i};\\hat{x}^{i}) = \\frac{1}{n}\\sum_{i=1}^{n}\\sum_{j=1}^{k} x^{i}_{j} ln(\\hat{x}^{i}_{j}) + (1-x^{i}_{j}) ln(1-\\hat{x}^{i}_{j})$, </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $x^{i}$, $i=1,...,n$ denotes the set of journal entries, $\\hat{x}^{i}$ the respective reconstructions, and $j=1,...,k$ indexes the various journal entry attributes. In the following, we will instantiate the corresponding BCE error function from the `PyTorch` library:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the optimization criterion / loss function\n",
    "loss_function = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will transfer the computation of the error function to the `CPU` or an available `GPU` if applicable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the optimization criterion / loss function to compute device \n",
    "loss_function = loss_function.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the error magnitude of a mini-batch of journal entries, the `PyTorch` library automatically calculates the gradients. Subsequently, the AENN parameters $θ$ are optimized based on the determined gradients. To achieve this, we only need to define the desired optimization method in `PyTorch`. In the following notebook cell, we use the **Adam Optimization** method for optimizing the model parameters $θ$. Additionally, we define a learning rate $l = 0.0001$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the learning rate\n",
    "learning_rate = 1e-4\n",
    "\n",
    "#set the paramete optimization strategy of both networks\n",
    "encoder_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = torch.optim.Adam(decoder_train.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully implementing and instantiating the three building blocks of the AENN model, let's take the time to review the definitions of the **Encoder** and **Decoder** models as well as the **BCE Reconstruction Error** and discuss any questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Autoencoder Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we aim to train an AENN model using the encoded transaction data. We will also take a detailed look at the individual training hyperparameters, training steps, and the training progress over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Training Hyperparameter Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by training an AENN model for **20 training epochs** with **128 journal entries** per mini-batch. This configuration of hyperparameters means that the dataset is fed to the AENN a total of five times in mini-batches of 128 entries each. As a result, each training epoch will have **713 updates** (91,247 entries modulo 128 entries per mini-batch) of the AENN model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify training parameters\n",
    "num_epochs = 20 # number of training epochs\n",
    "mini_batch_size = 128 # size of the mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training phase, the AENN model should continuously receive mini-batches of the entire population of journal entries. For this purpose, we use the `DataLoader` functionality of the `PyTorch` library. These are iterators that continuously provide the entries in the form of mini-batches. In the following cell, we instantiate a `PyTorch DataLoader` for the journal entry data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pre-processed transactional data to PyTorch tensor\n",
    "torch_dataset = torch.from_numpy(ori_subset_transformed.values.astype(np.float32))\n",
    "\n",
    "# push pre-processed transactional data to compute device\n",
    "torch_dataset = torch_dataset.to(device)\n",
    "\n",
    "# init training dataloader\n",
    "train_dataloader = dataloader.DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: By setting the parameter `shuffle`, the distinct mini-batches contain random journal entries of the datasets provided in a different order for each epoch.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the hyperparameters, we can start training the model. For each supplied mini-batch, the following steps are performed during the training process:\n",
    "\n",
    ">1. Perform the forward pass through the encoder and decoder network.\n",
    ">2. Calculate the BCE reconstruction error $\\mathcal{L^{BCE}_{\\theta}}(x^{i};\\hat{x}^{i})$.\n",
    ">3. Perform the backward pass through the encoder network.\n",
    ">4. Update the encoder $f_\\theta(\\cdot)$ and decoder $g_\\theta(\\cdot)$ parameters.\n",
    "\n",
    "To ensure learning during training, we monitor the BCE reconstruction error of the AENN model as the training progresses. Observing this makes it possible to infer the model's learning progress. Additionally, we can determine if and when the reconstruction error converges.\n",
    "\n",
    "During the model optimization, we want to pay special attention to the following `PyTorch` instructions:\n",
    "\n",
    ">- `reconstruction_loss.backward()`: Calculates the gradients based on the reconstruction error.\n",
    ">- `encoder_optimizer.step()` and `decoder_optimizer.step()`: Update the parameters based on the gradients.\n",
    "\n",
    "After each completed training epoch, we also want to save a **model checkpoint**. Checkpoints contain a snapshot of the model parameters. It is good practice to save such checkpoints at regular intervals during training. The training can be resumed from the last checkpoint if it is interrupted. To save a model checkpoint, we use the following `PyTorch` instruction:\n",
    "\n",
    ">- `torch.save()`: Saves the checkpoint of the current model parameter values to the local file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# set the model in training mode (apply dropout when needed)\n",
    "encoder_train.train()\n",
    "decoder_train.train()\n",
    "\n",
    "# init the best loss by setting it to infinity\n",
    "best_loss = np.inf\n",
    "\n",
    "# train autoencoder model\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # init collection of epoch losses\n",
    "    train_mini_batch_losses = []\n",
    "    \n",
    "    # init mini batch counter\n",
    "    mini_batch_count = 0\n",
    "        \n",
    "    # iterate over all mini-batches\n",
    "    for mini_batch_data in train_dataloader:\n",
    "\n",
    "        # increase mini batch counter\n",
    "        mini_batch_count += 1\n",
    "\n",
    "        # push mini batch data to compute device\n",
    "        mini_batch_data = mini_batch_data.to(device)\n",
    "\n",
    "        # =================== (1) forward pass ===================================\n",
    "\n",
    "        # run forward pass\n",
    "        z_representation = encoder_train(mini_batch_data) # encode mini-batch data\n",
    "        mini_batch_reconstruction = decoder_train(z_representation) # decode mini-batch data\n",
    "        \n",
    "        # =================== (2) compute reconstruction loss ====================\n",
    "\n",
    "        # determine reconstruction loss\n",
    "        reconstruction_loss = loss_function(mini_batch_reconstruction, mini_batch_data)\n",
    "        \n",
    "        # =================== (3) backward pass ==================================\n",
    "\n",
    "        # reset graph gradients\n",
    "        decoder_optimizer.zero_grad()\n",
    "        encoder_optimizer.zero_grad()\n",
    "\n",
    "        # run backward pass\n",
    "        reconstruction_loss.backward()\n",
    "        \n",
    "        # =================== (4) update model parameters ========================\n",
    "\n",
    "        # update network parameters\n",
    "        decoder_optimizer.step()\n",
    "        encoder_optimizer.step()\n",
    "\n",
    "        # =================== monitor training progress ==========================\n",
    "\n",
    "        # print training progress each 1.000 mini-batches\n",
    "        if mini_batch_count % 1000 == 0:\n",
    "            \n",
    "            # print mini batch reconstuction results\n",
    "            now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "            print('[LOG {}] epoch: [{}/{}], batch: {}, batch-train-loss: {}'.format(str(now), str(epoch+1), str(num_epochs), str(mini_batch_count), str(np.round(reconstruction_loss.item(), 8))))\n",
    "            \n",
    "        # collect mini-batch loss\n",
    "        train_mini_batch_losses.extend([reconstruction_loss.item()])\n",
    "\n",
    "    # =================== evaluate model performance =============================\n",
    "    \n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "                                 \n",
    "    # print training epoch results\n",
    "    now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "    print('[LOG {}] epoch: [{}/{}], epoch-train-loss: {}'.format(str(now), str(epoch+1), str(num_epochs), str(np.round(train_epoch_loss, 8))))\n",
    "\n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "    \n",
    "    # =================== save model snapshot to disk ============================\n",
    "    \n",
    "    # case: new best model trained\n",
    "    if train_epoch_loss < best_loss:\n",
    "    \n",
    "        # save trained encoder model file to disk\n",
    "        encoder_model_name = \"ep_{}_encoder_model.pth\".format((epoch+1))\n",
    "        torch.save(encoder_train.state_dict(), os.path.join(models_directory, encoder_model_name))\n",
    "\n",
    "        # save trained decoder model file to disk\n",
    "        decoder_model_name = \"ep_{}_decoder_model.pth\".format((epoch+1))\n",
    "        torch.save(decoder_train.state_dict(), os.path.join(models_directory, decoder_model_name))\n",
    "        \n",
    "        # update best loss\n",
    "        best_loss = train_epoch_loss\n",
    "\n",
    "        # print epoch loss\n",
    "        now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "        print('[LOG {}] epoch: [{}/{}], new best epoch-train-loss: {} found'.format(str(now), str(epoch+1), str(num_epochs), str(np.round(train_epoch_loss, 8))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will visualize the reconstruction error for each training epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fig.set_figwidth(18)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot the training epochs vs. the epochs' prediction error\n",
    "ax.plot(np.array(range(1, len(train_epoch_losses)+1)), train_epoch_losses, label='epoch loss (blue)')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[Training Epoch $e_i$]\", fontsize=14)\n",
    "ax.set_ylabel(\"[Reconstruction Error $\\mathcal{L}^{BCE}$]\", fontsize=14)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Epochs $e_i$ vs. Reconstruction Error $L^{BCE}$', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be observed that the reconstruction error of the AENN model begins to decrease continuously after five epochs. This implies that the model is gradually succeeding in reconstructing the journal entries contained within the dataset. However, the visualization also shows that the model could be trained for several more epochs until the reconstruction error no longer decreases or converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Autoencoder Neural Network Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will evaluate the learned AENN model's ability to detect anomalies in journal entry data. We will use pre-trained AENN models for this purpose. The evaluation includes both **local** and **global** anomalies in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Loading a Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We typically load the AENN model with the **lowest** reconstruction error or use another pre-trained model for the evaluation. During training, we saved a checkpoint of the model parameters for each epoch in the local model directory. We will now load a model checkpoint that has already been trained for **30 training epochs**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore pre-trained model checkpoint\n",
    "encoder_model_name = 'https://raw.githubusercontent.com/HSG-AIML-Teaching/GSERM2024-Lab/main/lab_06/03_models/ep_30_encoder_model_small.pth'\n",
    "decoder_model_name = 'https://raw.githubusercontent.com/HSG-AIML-Teaching/GSERM2024-Lab/main/lab_06/03_models/ep_30_decoder_model_small.pth'\n",
    "\n",
    "# read stored model from the remote location\n",
    "encoder_bytes = urllib.request.urlopen(encoder_model_name)\n",
    "decoder_bytes = urllib.request.urlopen(decoder_model_name)\n",
    "\n",
    "# load tensor from io.BytesIO object\n",
    "encoder_buffer = io.BytesIO(encoder_bytes.read())\n",
    "decoder_buffer = io.BytesIO(decoder_bytes.read())\n",
    "\n",
    "# init evaluation encoder and decoder model\n",
    "encoder_eval = encoder()\n",
    "decoder_eval = decoder()\n",
    "\n",
    "# load trained models\n",
    "encoder_eval.load_state_dict(torch.load(encoder_buffer, map_location=torch.device('cpu')))\n",
    "decoder_eval.load_state_dict(torch.load(decoder_buffer, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After successfully loading the model checkpoint, we transfer the model for evaluation purposes to the `CPU` (Note: This allows us to calculate the reconstruction errors of all journal entries without any limitations due to the `GPU` memory):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set networks in evaluation mode (don't apply dropout)\n",
    "encoder_eval.eval()\n",
    "decoder_eval.eval()\n",
    "\n",
    "# push encoder and decoder model to compute device\n",
    "encoder_eval = encoder_eval.to('cpu')\n",
    "decoder_eval = decoder_eval.to('cpu')\n",
    "\n",
    "# push the dataset to the CPU \n",
    "torch_dataset = torch_dataset.to('cpu')\n",
    "\n",
    "# push the loss function to the CPU\n",
    "loss_function = loss_function.to('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will calculate the dataset's **individual** BCE reconstruction errors for each journal entry $x_{i}$. First, the reconstruction $\\hat{x}_{i}$ of each entry is determined. Second, the BCE reconstruction error of the reconstructed entry $\\hat{x}{i}$ is calculated by comparing it to the original journal entries $x_{i}$ from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconstruct encoded transactional data\n",
    "reconstruction = decoder_eval(encoder_eval(torch_dataset))\n",
    "\n",
    "# init binary cross entropy errors\n",
    "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
    "\n",
    "# iterate over all detailed reconstructions\n",
    "for i in range(0, reconstruction.size()[0]):\n",
    "\n",
    "    # determine reconstruction loss - individual transactions\n",
    "    reconstruction_loss_transaction[i] = loss_function(reconstruction[i], torch_dataset[i]).item()\n",
    "\n",
    "    if(i % 10000 == 0):\n",
    "\n",
    "        ### print conversion summary\n",
    "        now = dt.datetime.utcnow().strftime(\"%Y.%m.%d-%H:%M:%S\")\n",
    "        print('[LOG {}] collected individual reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After calculating the individual reconstruction errors, we will visualize the magnitude of each error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# set plot size\n",
    "fig.set_figwidth(16)\n",
    "fig.set_figheight(8)\n",
    "\n",
    "# assign unique id to transactions\n",
    "plot_data = np.column_stack((np.arange(len(reconstruction_loss_transaction)), reconstruction_loss_transaction))\n",
    "\n",
    "# obtain regular transactions as well as global and local anomalies\n",
    "regular_data = plot_data[label == 'regular']\n",
    "global_outliers = plot_data[label == 'global']\n",
    "local_outliers = plot_data[label == 'local']\n",
    "\n",
    "# plot reconstruction error scatter plot\n",
    "ax.scatter(regular_data[:, 0], regular_data[:, 1], c='C0', alpha=0.4, marker=\"o\", s=30, label='regular') # plot regular transactions\n",
    "ax.scatter(global_outliers[:, 0], global_outliers[:, 1], c='C1', marker=\"^\", s=80, label='global') # plot global outliers\n",
    "ax.scatter(local_outliers[:, 0], local_outliers[:, 1], c='C2', marker=\"*\", s=80, label='local') # plot local outliers\n",
    "\n",
    "# add plot legend of transaction classes\n",
    "ax.legend(loc='best')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[Journal Entry ID $x_i$]\", fontsize=14)\n",
    "ax.set_ylabel(\"[Reconstruction Error $\\mathcal{L}^{BCE}$]\", fontsize=14)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Journal Entry ID $x_i$ vs. Reconstruction Error $L^{BCE}$', fontsize=16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The visualization shows that our AENN model can reconstruct most regular journal entries with low error. Simultaneously, both the **global anomalies** (green) and **local anomalies** (red) exhibit a comparatively high reconstruction error. Based on this analysis, we can conclude that it is possible to differentiate **anomalies** (green and red) from regular journal entries (blue) within the dataset using reconstruction errors.\n",
    "\n",
    "To further investigate this observation, we will now filter out journal entries that have a **reconstruction error >= 0.12** from the dataset. We assume (as demonstrated above) that these journal entries correspond to the **global anomalies** within the total population of journal entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append labels to original dataset\n",
    "ori_dataset['label'] = label\n",
    "\n",
    "# extract transactions exhibiting a reconstruction error >= 0.12\n",
    "autoencoder_global_anomalies = ori_dataset[reconstruction_loss_transaction >= 0.12]\n",
    "\n",
    "# inspect transactions exhibiting a reconstruction error >= 0.12\n",
    "autoencoder_global_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now extract the journal entries into an Excel table to make them available to the audit team. First, we will generate a timestamp of the data extract for the audit trail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will extract the filtered **global anomalies** as an Excel file for further substantive testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the filename of the excel spreadsheet\n",
    "filename = str(timestamp) + \" - ACA_001_autoencoder_global_anomalies.xlsx\"\n",
    "\n",
    "# specify the target data directory of the excel spreadsheet\n",
    "data_directory = os.path.join(results_directory, filename)\n",
    "\n",
    "# extract the filtered transactions to excel\n",
    "autoencoder_global_anomalies.to_excel(data_directory, header=True, index=False, sheet_name='Global_Anomalies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's closely examine the journal entries with a **reconstruction error >= 0.04 but <= 0.12**. We assume (as illustrated above) that these journal entries correspond to the **local anomalies** within the population of journal entries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract transactions exhibiting a reconstruction error < 0.12 and >= 0.04\n",
    "autoencoder_local_anomalies = ori_dataset[(reconstruction_loss_transaction >= 0.04) & (reconstruction_loss_transaction < 0.12)]\n",
    "\n",
    "# inspect transactions exhibiting a reconstruction error < 0.12 and >= 0.04\n",
    "autoencoder_local_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will again extract the filtered journal entries into an Excel table to make them available to the audit team. First, we will generate a timestamp of the data extract for the audit trail:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = dt.datetime.utcnow().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following that, we will extract the filtered **local anomalies** as an Excel file for further substantive testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the filename of the excel spreadsheet\n",
    "filename = str(timestamp) + \" - ACA_002_autoencoder_local_anomalies.xlsx\"\n",
    "\n",
    "# specify the target data directory of the excel spreadsheet\n",
    "data_directory = os.path.join(results_directory, filename)\n",
    "\n",
    "# extract the filtered transactions to excel\n",
    "autoencoder_local_anomalies.to_excel(data_directory, header=True, index=False, sheet_name='Local_Anomalies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation of Journal Entry Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real-world audit context, it's generally beneficial to examine the **learned representations** of the journal entries by the AENN model in addition to the reconstruction error. This examination provides insights into the **structural semantics** of the entries and their corresponding attributes. Additionally, this analysis allows us to contextualize any identified anomalies within the population of journal entries.\n",
    "\n",
    "To obtain the representation of the journal entry, we perform a forward pass through the encoder of the AENN model for each journal entry. For this, we load an encoder network **model checkpoint** trained for 80 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# restore pretrained model checkpoint\n",
    "encoder_model_name = 'https://raw.githubusercontent.com/HSG-AIML-Teaching/GSERM2024-Lab/main/lab_06/03_models/ep_80_encoder_model_small.pth'\n",
    "\n",
    "# read stored model from the remote location\n",
    "encoder_bytes = urllib.request.urlopen(encoder_model_name)\n",
    "\n",
    "# load tensor from io.BytesIO object\n",
    "encoder_buffer = io.BytesIO(encoder_bytes.read())\n",
    "\n",
    "# init evaluation encoder and decoder model\n",
    "encoder_eval = encoder()\n",
    "\n",
    "# load trained models\n",
    "encoder_eval.load_state_dict(torch.load(encoder_buffer, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will perform a forward pass through the AENN model for each journal entry. Consequently, we will obtain the three-dimensional representation of each entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# push the dataset to the CPU \n",
    "torch_dataset = torch_dataset.to('cpu')\n",
    "\n",
    "# push encoder and decoder model to compute device\n",
    "encoder_eval = encoder_eval.to('cpu')\n",
    "\n",
    "# run forward path through encoder to obtain journal entry representations\n",
    "entry_representations = encoder_eval(torch_dataset)\n",
    "\n",
    "# convert the representations to a pandas dataframe\n",
    "entry_representation = pd.DataFrame(entry_representations.data.cpu().numpy(), columns=['z1', 'z2', 'z3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, for validation and visualization purposes, we will label the representations with the original labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_representation['label'] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before visualizing, let's carefully examine the obtained coordinates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_representation.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the three-dimensional **latent space**, we can visualize the space and the representations using the `Matplotlib 3D` functionality. In the following cell, we will create a visualization of this space and the representations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enforce inline plotting\n",
    "%matplotlib inline\n",
    "\n",
    "# import 3d plotting and animation libraries\n",
    "from IPython.display import HTML\n",
    "from matplotlib import animation\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "\n",
    "# init the plot\n",
    "fig = plt.figure(figsize=(5, 5))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# change plot perspective\n",
    "ax.view_init(elev=30, azim=240)\n",
    "\n",
    "# set axis paramaters of subplot\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot regular transactions, just the first 1000 to gain an intuition\n",
    "regular = entry_representation[entry_representation['label'] == 'regular']\n",
    "ax.scatter(regular['z1'][0:2000], regular['z2'][0:2000], regular['z3'][0:2000], c='C0', alpha=0.4, marker=\"o\", label='regular')\n",
    "\n",
    "# plot first order anomalous transactions\n",
    "global_anomalies = entry_representation[entry_representation['label'] == 'global']\n",
    "ax.scatter(global_anomalies['z1'], global_anomalies['z2'], global_anomalies['z3'], c='C1', s=100, marker=\"^\", label='global')\n",
    "\n",
    "# plot second order anomalous transactions\n",
    "local_anomalies = entry_representation[entry_representation['label'] == 'local']\n",
    "ax.scatter(local_anomalies['z1'], local_anomalies['z2'], local_anomalies['z3'], c='C2', s=100, marker=\"*\", label='local')\n",
    "\n",
    "# set axis labels\n",
    "ax.set_xlabel('activation [$z_1$]', weight='normal', fontsize=12)\n",
    "ax.set_ylabel('activation [$z_2$]', weight='normal', fontsize=12)\n",
    "ax.set_zlabel('activation [$z_3$]', weight='normal', fontsize=12)\n",
    "\n",
    "# add plot legend of transaction classes\n",
    "ax.legend(loc='best')\n",
    "\n",
    "# set plot title\n",
    "plt.title('AEEN Model Latent Space', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Lab Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, you successfully accomplished the following key learnings:\n",
    "\n",
    "> 1. **Understanding Autoencoder Neural Network Design:** Mastered the fundamental concepts and architectural design of autoencoder neural networks (AENNs), enhancing your comprehension of deep learning models tailored for anomaly detection in accounting data.\n",
    "> 2. **Model Implementation and Training:** Developed practical skills in implementing and training an AENN model using PyTorch, applying it to synthetic financial datasets to detect anomalies in journal entries.\n",
    "> 3. **Evaluating Model Performance:** Gained expertise in evaluating the performance of AENN models through metrics such as reconstruction error and effectively utilized these metrics to identify local and global anomalies.\n",
    "> 4. **Visualization and Interpretation of Representations:** Learned to visualize and interpret the learned representations of journal entries, providing deeper insights into the structural semantics of the data and facilitating the contextualization of anomalies.\n",
    "\n",
    "This lab provided insights into designing, implementing, training, and evaluating AENNs for unsupervised anomaly detection. It equipped you with tools and techniques for effective model building, evaluation, and application. These skills are invaluable for succeeding in deep learning."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "206.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
